

\clearpage
\appendix
\section{Appendix}




% ---------------------------------------------------------------------------
\subsection{Implementing the MFP baseline}

We were required to make several key changes to the publicly released MFP codebase to create a usable MFP baseline for our contingency planning scenarios in CARLA:
\begin{enumerate}
    \item The MFP codebase only includes code for the NGSIM dataset, so we added support for training (and visualizing) models on CARLA datasets, in addition to adding support for deploying the model in the CARLA simulator. Since MFP is a model for prediction (not planning), the latter required implementing a planning procedure to turn predictions from the MFP model into control for the ego vehicle (see Appendix \ref{sec:mfpplanning}).
    \item We added rotation of model inputs based on the each vehicle's yaw as an additional preprocessing stage, such that the reference frame for prediction always starts with the ego vehicle at (0,0) and pointing in the +X direction (the existing repository only shifts the inputs to (0,0)). We observed that rotation of inputs was necessary for training on the left and right turn scenarios.
    \item We removed the attention stage of the model architecture which significantly improved model performance on our CARLA datasets (recall that the scenarios we consider in this paper only include two agents).
    \item We removed the pretraining stage during model training (where the RNN/decoder output sequence is generated in a non-interactive fashion) which also significantly improved model performance.
\end{enumerate}
A repository containing a version of the MFP codebase with the aforementioned changes can be found on the project website. 
%We found that the training procedure is quite sensitive to hyperparameters and did 

An important difference between \ours ~and MFP is that with \ours, we can train a single model on multiple scenarios, whereas with MFP, we are limited to training a single model for each individual scenario. We were unable to train MFP models on multiple scenarios that generated reasonable predictions (usable for planning): we found that the mixed-scenario models would often generate trajectories from the wrong scenario (e.g., a left turn on the right turn scenario). Additionally, we found it necessary to tune hyperparameters per-scenario, e.g., preprocessing with rotation is critical for the left and right turn scenarios, but degrades performance significantly on the overtake scenario.

\subsection{Planning with MFP}\label{sec:mfpplanning}

Similar to our behavior model described in \cref{sec:model_design}, MFP's forward pass (used during inference) generates predicted trajectories for $\bX_{\leq T}$.
Moreover, MFP generates $K$ different trajectories for each vehicle, where each trajectory $\bx_{\leq T}^{a,k}$ pertains to a different ``mode'' $k$ ($k \in 1, \dots, K$). 
According to the MFP authors, modes are meant to represent semantically different types of driving behavior such as intentions (e.g., left/right/straight) or behaviors (e.g., aggressive/conservative). 

MFP's multimodal prediction is implemented by using $K$ RNN decoders (one for each mode, with the total numbers of modes fixed during training), which model the future trajectories for all agents, conditioned on the past trajectories of all agents, the context view (the trajectories and visual context both processed by an encoder), and the mode: 
$p(\bY^a|\bX,\mathcal{I},k)$, 
paramaterized by a 2D Gaussian distribution over xy coordinates for each agent at every predicted timestep $t$, i.e., $\mathcal{N}_2(\mu,\rho)$ represented by the 5-tuple $(\mu^{a,t}_X$, $\mu^{a,t}_Y$, $\sigma^{a,t}_X$, $\sigma^{a,t}_Y$, $\rho^{a,t})$.
A given agent $a$'s history $\bX^a$ and an attention-based encoding of the history of other agents $Attn(\bX^{i \neq{} a})$ are concatenated in a vector $[\bX^a, Attn(\bX^{i \neq{} a}), \mathcal{I}]$ and passed as input to the encoding RNN, which produces a latent encoding and a Multinoulli distribution over modes per-agent, $p(K^a | \bX^a, Attn(\bX^{i \neq{} a}), \mathcal{I})$. The latent encoding is then input to each decoding RNN (one per mode) to generate $K$ predicted trajectories per agent.
All agents share the same set of (encoder and decoder) RNNs, so parameters scale linearly with modes, whereas the number of agents scales the batch/input dimension.

To summarize, the outputs of the trained MFP model represent the following distributions:

$p(\bY^a | \bX, \mathcal{I}, k)$, a distribution over future coordinates, per-agent, per-mode, generated by the decoding RNNs, and $p(K^a | \bX, \mathcal{I})$, a distribution over modes per-agent, generated by the encoding RNN. This means the model generates $K$ futures for each agent, i.e., $K \times A$ total trajectory distributions.

Note that the MFP model outputs do not include an explicit joint trajectory distribution $p(\bY | \bX, ...)$; the conditional joint future is factorized across agents, and across time (using the autoregressive decoder):
\begin{equation}\label{eq:mfpjoint}
    p(\bY^a | \bX, \mathcal{I}, k) = \prod_{t=1}^T \prod_{a=1}^A p(\by_t^a | \bY_{\leq t}, \bX, \mathcal{I}, k^a)
\end{equation}
We note that in Eq. \ref{eq:mfpjoint} (and its original formulation in \cite{tang2019mfp}), the joint is conditioned on the (ego) agent's own $k$, but not on the $k$s of other agents. However, in the reference implementation of MFP\footnote{\href{https://github.com/apple/ml-multiple-futures-prediction/commit/402f82d}{https://github.com/apple/ml-multiple-futures-prediction/commit/402f82d}} (and our own code), the joint is conditioned on all agents having the same $k$ value.

Planning using the same cost model as \ours ~requires explicit joint trajectories.
Since the MFP model outputs a distribution of $K$ trajectories for each of the $A$ agents, to generate candidate joint trajectories $\hat{\bY}$ in a two-agent setting, we can simply consider the set of all $K \times K$ possible combinations of individual ego and actor trajectories:
\begin{equation}
\begin{aligned}
    \hat{\bY}
    & = [(\mu_{\bY^{ego}}, \mu_{\bY^{actor}}) ~\text{for}~ k^{ego}, k^{actor} \in K]
    \\& = [(\mu^{ego}_X,\mu^{ego}_Y,\mu^{actor}_X,\mu^{actor}_Y) ~\text{for}~ k^{ego}, k^{actor} \in K]
\end{aligned}
\end{equation}
In our MFP experiments we used 5 modes, so we consider 25 total candidate joint trajectories. To select the best ego trajectory to use for planning, we can select the ego portion of the joint with the lowest cost (highest reward), weighted by the probability of each trajectory:
\begin{equation}
\begin{aligned}
    {\bY^{ego}}^\ast = 
    \argmin_{(\mu^{ego}_X,\mu^{ego}_Y)}~
    & (Cost(\mu^{ego}_X,\mu^{ego}_Y,\mu^{actor}_X,\mu^{actor}_Y)
    \\& \times p(K^{ego}=k^{ego} | \bX, \mathcal{I})
    \\& \times p(K^{actor}=k^{actor} | \bX, \mathcal{I}))
\end{aligned}
\end{equation}
Instead of weighting candidate joint trajectories by individual trajectory probabilities, alternatively we can filter out unrealistic joint trajectories by ignoring those with individual trajectory probabilities below a certain threshold, e.g.:
\begin{equation*}
    P(K^{ego}=k^{ego} | \bX, \mathcal{I}) < 0.01
\end{equation*}

Assuming a perfect prediction model, both of these methods will naturally create an \emph{overconfident} planner since we consider every possible combination of ego and actor trajectories. To create an \emph{underconfident} planner (assuming a perfect MFP model), we can choose the ego trajectory with the best worst-case joint trajectory, i.e., we assume an adversarial actor that chooses the actor trajectory leading to the worst performing joint, conditioned on the choice of ego trajectory:
\begin{equation}
\begin{aligned}
    {\bY^{ego}}^\ast = 
    & \argmin_{(\mu^{ego}_X,\mu^{ego}_Y)}~
     max_{(\mu^{actor}_X,\mu^{actor}_Y)}~
    \\& Cost(\mu^{ego}_X,\mu^{ego}_Y,\mu^{actor}_X,\mu^{actor}_Y)
\end{aligned}
\end{equation}

Finally, the form of the MFP model is amenable to contingent planning, in contrast to the action shooting originally described in \cite{tang2019mfp}. 

Once we have selected an optimal ego trajectory, we input the chosen ego trajectory to a PID controller to generate controls for the vehicle. In practice we found the outputs of MFP trained on the CARLA dataset to be imperfect, and observed that the underconfident planner is highly sensitive to noisy predictions and would often generate control that lead to out-of-distribution prediction. Therefore in our experiments we report results using the (weighted probability) overconfident planner.

\subsection{Implementing the model-free RL baseline} \label{app:model_free}
For our model-free reinforcement learning baseline, we use the implementation of Proximal Policy Optimization provided in the Stable Baselines repository~\citep{stable-baselines} (``PPO2''), and build on the OATomobile~\citep{filos2020can} repository to wrap each scenario in an OpenAI Gym environment.
The reward function used to train the RL agent is a linear combination of a collision penalty (to penalize risky behavior), constant timestep penalty (to encourage the agent to complete the turn as fast as possible), and ``lane-tracking'' reward (to encourage the agent follow a standard arc maneuver while turning). In practice we found the lane-tracking reward to be critical for learning realistic driving behavior.
We experiment with two different action spaces, one where the agent only controls the acceleration (steering is provided with an auto-pilot) which mirrors the RL experimental setup in \cite{tang2019mfp}, and another more realistic setting where the agent is also responsible for controlling steering and braking.
Each episode has a horizon of 200 steps but is terminated early if the agent completes the turn (i.e., reaches a target location).

We had to make several significant changes to the OATomobile codebase to enable RL training on our suite of contingency planning scenarios. Most importantly, the original codebase only allows control of a single agent in the CARLA simulator, i.e., the ego vehicle. However, our scenarios are multi-agent: the actor vehicle has to dynamically respond to the ego vehicle's state in order to implement the scenario behavior trees (e.g., Figure \ref{fig:unprotected_left_didactic}). To allow for training on a two-agent scenario, we modified the OATomobile API to take actor actions (in addition to ego actions) at each timestep, which are determined by the same behavior script described in Section \ref{sec:experiments}.

Although we were able to train an agent with PPO that manages to achieve a 0\% collision rate and 100\% success rate (using the restricted action space), we observe that training is highly unstable: safe policies were quick to diverge to degenerate solutions exhibiting highly overconfident behavior. We hypothesize that additional reward shaping and hyperparameter tuning may be able to stabilize training.

We were unable to learn a reasonable policy with RL using the larger (more realistic) action space, where the agent controls steering and braking in addition to acceleration. RL agents trained using the full action space generally exhibit erratic and unrealistic driving behavior. We hypothesize that additional reward shaping, post-processing of actions (e.g., smoothing in action space), and a pre-training stage (e.g., using expert data, or the restricted action space) may help in learning a more realistic policy.

\subsection{Visualizations of plans}

\begin{figure}[thb]
    \centering
    \includegraphics[width=.9\linewidth]{fig/contingency_combo_v2.jpg}
    \caption{\small Examples of contingent plans in the left turn and overtake task. Each row corresponds to a contingent plan -- a single robot $\bz_{\leq T}^r$ -- and yields different joint trajectories when forward-simulated by the model with an accompanying sample of $\bz_{\leq T}^h$. Blue denotes the forecasted robot trajectory; orange denotes the forecasted human trajectory.}
    \label{fig:contingent_plan_examples}
    \vspace{-10pt}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[t]{.24\linewidth}
    \includegraphics[width=\linewidth]{fig/task_screenshots_compress/left_oc.jpg}
    \end{subfigure}
    \begin{subfigure}[t]{.24\linewidth}
    \includegraphics[width=\linewidth]{fig/task_screenshots_compress/overtake_oc.jpg}
    \end{subfigure}
    \begin{subfigure}[t]{.24\linewidth}
    \includegraphics[width=\linewidth]{fig/task_screenshots_compress/left_uc.jpg}
    \end{subfigure}
    \begin{subfigure}[t]{.24\linewidth}
    \includegraphics[width=\linewidth]{fig/task_screenshots_compress/overtake_uc.jpg}
    \end{subfigure} %
        \caption{\small Examples of overconfident (left half) and underconfident (right half) planning. In the left-turn and overtaking scenarios, the overconfident planner believes the human (orange) will always yield to the robot (blue). The underconfident planner believes that the robot cannot influence the human and plans a suboptimal determinstic path.}\label{fig:oc_uc}
    \vspace{-10pt}
\end{figure}


\subsection{Representing contingencies}
%  and are knowable \emph{a priori}, i.e. without observing ${\bo}$. 
 The model in Eq.~\ref{eq:esp_model} can be equivalently represented as an \emph{autoregressive flow} $q_f$, composed of learned bijections $f=[f_0^0, \dots, f_0^A, \dots, f_T^0, \dots, f_T^A]$ and base distributions $\bar{q}(\bZ)=\bar{q}_{\leq T}^{1:A}(\bZ_{\leq T}^{1:A})$. Consider the discrete setting. Discrete flows learn both the probability mass function $\bar{q}$ and $f$, and satisfy $q_f(\bX=\bx)\!=\!\bar{q}(\bZ\!\!=\!\!f^{-1}(\bx))$ \citep{tran2019discrete}; in our multi-agent autoregressive case, $\bar{q}(\bZ)\!=\!\prod_{t=1}^T\prod_{a=1}^A\bar{q}_t^a(\bZ_t^a)$. 
This implies that by using $\bz^r$ to parameterize contingency plans, the space of contingency plans that this can represent is the set of sequences of $n_t$-th ``best responses'' under the model.  
 To see this fact, let ${}^{n_t}\bz_t^r=\argmax_{\bz^t_r \in \mathbb Z^d \setminus \{{}^1\bz_t^r, \dots, {}^{n_t-1}\bz_t^r\}} \bar{q}_t^r$. When $n_t=1$, ${}^1\bz_t^r$ is the first best response at time $t$, as judged by $\bar{q}_t^r$. When $n_t>1$, ${}^{n_t}\bz_t^r$ is the $n_t$-th best response at time $t$. Thus, at every time step $t$, a choice of $\bz_t^r$ is identified with the $n_t$-th best response ${}^n\bz_t^r$. Thus, the space of contingent plans is $\{{}^{n_{1}}\bz_{1}^r, \dots, {}^{n_{T}}\bz_{T}^r : n_{t} \in \mathbb N\}$; the best contingency plan under the model is ${}^{1,\dots,1}\bz^r_{\leq T}$. Therefore, the space of contingent plans that a discrete autoregressive flow model can represent is relatively flexible, as the contingent plans that this parameterization does not represent are just those that have values of $n_t$ that are contingent on the possible futures $\bX_{1:t-1}$ (e.g. policies that perform the first best response to partial future $X$ and the second best response to partial future $Y$).  Although our model is not discrete, these observations lead us to speculate
 that there generally will exist $\bz_{\leq T}^r$ for each scene that represent high-quality contingent plans; evidence from our experiments support this speculation. An illustration of the contingencies that a binary autoregressive flow can represent with $\bz_{\leq T}^r$ is shown in Fig.~\ref{fig:binary_contingency_demo}.

 \begin{figure*}[htb]
 \centering
 \includegraphics[width=.9\textwidth]{fig/contingencies_v6.jpg}
\caption{A discrete autoregressive flow can represent contingency plans when provided with a subset of all base distribution values; the provided values represent the decisions that can be controlled/planned in a partially-controlled system. The represented contingencies include the most-likely responses at every time-step. In this case, the discrete autoregressive flow model is $q(\bX=\bx)\!=\!\prod_{t=1}^{T=2}\prod_{i=1}^{A=2}\bar{q}_t^i(\bZ_t^i=f_t^{-1,i}(\bx_t^i, \bx_{<t}))$. Agent $i$'s action at time $t$ is a random variable $\bX_t^i$ that assumes a value  $\bx_t^i \in \{a_t^i, b_t^i\}$, with $i=r$ denoting the robot (controlled agent), and $i=h$ denoting the human (uncontrolled agent). Blue nodes represent robot actions, orange nodes represent human actions, and dark blue nodes represent the likelier of two robot actions. Thick arrows denote the likelier of two actions under the model. Colored edges denote specific contingency plans; solid colored edges denote the representable contingencies, whereas gray-dashed colored edges denote the unrepresentable contingencies. By choosing the base-space reactions ahead of time ($\mathbf{z}_t^r$), this parameterization represents reactions ordered by their probabilities ${}^1\mathbf{z}_t^r=\argmax \bar{q}_t^r$, ${}^2\mathbf{z}_t^r=\argmax \bar{q}_t^r~\mathrm{s.t.}~{{}^2\bz_t^r \neq {}^1\mathbf{z}_t^r}\implies \bar{q}_t^r({}^1\mathbf{z}_t^r) > \bar{q}_t^r({}^2\mathbf{z}_t^r)$. Thus it cannot represent contingencies with model probability ordering that depend upon future data, but this is not a significant limitation. Note again that the representable contingencies include the
n-most-likely responses at every timestep.} \label{fig:binary_contingency_demo}
 \end{figure*}

\subsection{Planning criterion derivation}
We derive the planning criterion of Eq.~\ref{eq:final_planning_criterion} as a MAP estimation of a scene-conditioned posterior of policy parameters, $p(\bz_{\leq T}^r|\mathcal G, {\bo})$.
\renewcommand{\CancelColor}{\color{gray}}
{
\begin{align}
    &\argmax_{\bz_{\leq T}^r} p(\bz_{\leq T}^r|\mathcal G, {\bo}) \nonumber \\
    &=\argmax_{\bz_{\leq T}^r} \log \, p(\mathcal G,\bz_{\leq T}^r|{\bo}) \,-\,
      {\log p(\mathcal G|{\bo})} 
      \label{eq:planning_criterion_first_step} \\
      &=\argmax_{\bz_{\leq T}^r} \!\! \log \!\! \int \!\! p(\mathcal G|\bz_{\leq T}^r,\!\bz^h\!\!\!, {\bo}\!)p(\bz_{\leq T}^r,\!\bz^h|{\bo}\!)p(\bz^h\!|{\bo}\!)\mathrm{d}\bz^h \label{eq:planning_criterion_middle_step}\\
      &=\argmax_{\bz_{\leq T}^r} \log \mathbb E_{p({\bz}^h|{\bo})} p(\mathcal G|\bz_{\leq T}^r,\bz^h, {\bo})p(\bz_{\leq T}^r,\bz^h|{\bo})  \label{eq:planning_criterion}
\end{align}
}%
where between \cref{eq:planning_criterion_first_step} and \cref{eq:planning_criterion_middle_step} we drop the term $\log p (\mathcal{G} | {\bo})$ since it is independent of $\bz_{\leq T}^r$.
%
Now, we incorporate the learned prior of expert behavior $q_\theta(\bX\!=\!\bx|{\bo})$
by defining the prior over policy parameters and human stochasticity $p(\bz_{\leq T}^r, \bz^h|{\bo})\!\doteq\! q_\theta(\bX\!=\!f(\bz^r_{\leq T}, \bz^h)|{\bo})$. We incorporate the goals $\mathcal G$ in terms of two distributions: $p(\mathcal G|\bz_{\leq T}^r,\bz^h, {\bo})\doteq p(\mathbf{g}|\bz_{\leq T}^r,\bz^h,{\bo})\delta_{\mathbb G}(\bx)$, with $p(\mathbf{g}|\bz_{\leq T}^r,\bz^h, {\bo})\doteq \mathcal N(f(\pi^r_{\bz_{\leq T}}, \bz^h)_T^r; \mathbf{g}, I)$ -- this favors $\pi^r_{\bz_{\leq T}^r}$ that tend to end near $\mathbf{g}$. Finally, we use $p(\bz^h|{\bo})\doteq \mathcal N(0, I)$, which is the model's prior over human stochasticity. In practice, we perform stochastic gradient ascent on a numerically version of the criterion in Eq.~\ref{eq:planning_criterion} that swaps the ordering of $\log$ and $\mathbb E$ (by Jensen's inequality) to arrive at a lower-bound, and approximates a violated constraint ($\log \delta_{\mathbb G}(\bx)\!=\!-\infty$) with a large negative value. This final planning criterion is used to implement \oursfull~(\ours), and is reproduced in Eq.~\ref{eq:app_final_planning_criterion}.

 \begin{align}
    &\mathcal{L}_\text{\ours}(\pi^r_{\bz_{\leq T}})
    =  \label{eq:app_final_planning_criterion}\\
    &\mathop{\E}_{\bz^h \sim \mathcal{N}(0, I)} \Big[
    \log \underbrace{q_\theta(\bar{\bx}_{\leq T}|{\bo})}_{(1)~\text{prior}}\!+\!\log  \underbrace{\mathcal N\big(\bar{\bx}^r_T; \mathbf{g}, I
    \big)}_{(2)~\text{destination}}\!+\!\log \underbrace{\delta_{\mathbb G}(\bar{\bx}_{\leq T})}_{(3)~\text{constraint}} 
    \Big].\notag
 \end{align}

\subsection{Theoretical Justification for Contingency Planning}
Here we demonstrate why value increases if our planning considers contingencies, rather than only planning a single state sequence. We additionally show the benefit of active planning, where the robot is known to affect other agents.
Consider planning just two time steps into the future as visualized in \cref{fig:models}. Both the ``No leader'' and ``Robot leader'' methods plan full state trajectories open-loop, with associated value functions $Q^{\text{NL}}$ and $Q^{\text{RL}}$. By contrast, the contingent planners (``Human leader'' and ``Co-leaders'', with values $Q^{\text{HL}}$ and $Q^{\text{CL}}$) understand that future decisions like $\bx^r_2$ depend on future information, like the value of $\bx^h_1$, which is not yet known. Below we show the value functions corresponding to each planner. We use the hat symbol \smash{$\hat{Q}$} to reflect the estimated value function that each planner optimizes, an estimate which is based on different modelling assumptions about how multi-agent interaction occurs:

\newcommand{\reward}[1]{\text{reward}(\bx^r_#1,\bx^h_#1)}
\newcommand{\objectivereturn}[0]{\text{return}(\bx^{r,h}_{1,2})}
\begin{alignat}{2}
\hat{Q}^{\text{NL}}({\bo},\bx^r_1) &\doteq \max_{\bx^r_2} \, \blue{\mathbb{E}_{\bx^h_1|{\bo}}} \, \orange{\mathbb{E}_{\bx^h_2|\bx^h_1,{\bo}}} && \Big[\objectivereturn\Big], 
\label{eq:vn} \\
%
\hat{Q}^{\text{RL}}({\bo},\bx^r_1) &\doteq \max_{\bx^r_2} \, \blue{\mathbb{E}_{\bx^h_1|{\bo}}} \, \green{\mathbb{E}_{\bx^h_2|\bx^r_1,\bx^h_1,{\bo}}} && \Big[\objectivereturn\Big], 
\label{eq:vr} \\
%
\hat{Q}^{\text{HL}}({\bo},\bx^r_1) &\doteq \blue{\mathbb{E}_{\bx^h_1|{\bo}}} \, \max_{\bx^r_2} \, \orange{\mathbb{E}_{\bx^h_2|\bx^h_1,{\bo}}} && \Big[\objectivereturn\Big], 
\label{eq:vh} \\
%
\hat{Q}^{\text{CL}}({\bo},\bx^r_1) &\doteq \blue{\mathbb{E}_{\bx^h_1|{\bo}}} \, \max_{\bx^r_2} \, \green{\mathbb{E}_{\bx^h_2|\bx^r_1,\bx^h_1,{\bo}}} && \Big[\objectivereturn\Big].
\label{eq:vc}
\end{alignat}

\paragraph{Case for \textit{contingency} planning}
For any $({\bo},\bx^r_1)$, we have $\hat{Q}^{\text{NL}} \leq \hat{Q}^{\text{HL}}$ and $\hat{Q}^{\text{RL}} \leq \hat{Q}^{\text{CL}}$ due to Jensen's inequality, noting the maximum operator is convex. 
More intuitively, by moving a max operator after the blue expectation operator, the value function can realize the ``best of all worlds'', by planning optimal contingencies for each future outcomes. By contrast, a max operator before the expectation computes evaluates the single best ``compromise'' decision $\bx^r_2$ that is common to all possible values of the preceding human state $\bx^h_1$.

\paragraph{Case for \textit{active} contingency planning}
Note $\hat{Q}$ denotes the value function that each planner aims to optimize, but is not necessarily the ``true value'' of such a planner (denoted $Q$) if $\hat{Q}$ was based on erroneous assumptions.
In \cref{eq:vn,eq:vr,eq:vh,eq:vc} the orange expectation operator is based on a incomplete behavior model that predicts $\bx^h_2$ given $(\bx^h_1,{\bo})$, even though $(\bx^h_1,{\bo})$ is not the Markov state, since it omits the robot's influence on the human. The full Markov state to predict the next human state is featured in the green expectation operator, which assumes a behavior model to predict $\bx^h_2$ given all the information: $(\bx^r_1,\bx^h_1,{\bo})$. We thus consider the green operator to reflect a behavior model that is more complete, and faithful to how reality will progress, compared to the orange operator. Thus, we have $\hat{Q}^{\text{RL}}=Q^{\text{RL}}$ and $\hat{Q}^{\text{CL}}=Q^{\text{CL}}$, but the same cannot generally be said for $\hat{Q}^{\text{NL}}$ and $\hat{Q}^{\text{HL}}$.

With respect to the random variable of the orange expectation, we can expand it:
\begin{align*}
p(\bx^h_2|\bx^h_1,{\bo}) &= \int\! p(\bx^h_2,\bx^{r'}_1\!|\bx^h_1,{\bo}) \text{d}\bx^{r'}_1 \\
&= \int\! p(\bx^h_2|\bx^{r'}_1\!,\bx^h_1,{\bo}) p(\bx^{r'}_1\!|{\bo}) \text{d}\bx^{r'}_1 \\
&=\, \mathbb{E}_{\bx^{r'}_1\!|{\bo}}\!\left[p(\bx^h_2|\bx^{r'}_1\!,\bx^h_1,{\bo})\right].
\end{align*}
Thus we can represent $\orange{\mathbb{E}_{\bx^h_2|\bx^h_1,{\bo}}} = \red{\mathbb{E}_{\bx^{r'}_1\!|{\bo}}} \yellow{\mathbb{E}_{\bx^h_2|\bx^{r'}_1\!,\bx^h_1,{\bo}}}$
where $\bx^{r'}_1$ represents the robot state that was implicitly integrated-out at training time, instead of being explicitly conditioned on. This implicit variable $\bx^{r'}_1$ is distinct from the robot-state at \textit{test-time} $\bx^r_1$ which the planner optimizes. 
%
This way of re-framing \cref{eq:vh} frames the human-leader model as omitting the known (planned) value of $\bx^r_1$ at test-time when predicting $\bx^h_2$ during planning, and instead reverting to the prior distribution of robot states seen at training times: $p(\bx^{r'}_1\!|{\bo})$. Omitting known information reduces the value of such a planner. The ``true'' value of the human-leader planner can be computed using $Q^{\text{CL}}$ given $Q^{\text{CL}}$'s definition assumes the correct process (green operator) of how $\bx^h_2$ is generated:
\begin{align}
V^{\text{HL}}({\bo})
&= Q^{\text{CL}}\big({\bo},\,\argmax_{\bx^r_1} \hat{Q}^{\text{HL}}({\bo},\bx^r_1)\big) \\
&\leq Q^{\text{CL}}\big({\bo},\,\argmax_{\bx^r_1} Q^{\text{CL}}({\bo},\bx^r_1)\big)
= V^{\text{CL}}({\bo}),  \nonumber
\end{align}
%
where we define a corresponding $V$-function for each $Q$-function in \cref{eq:vn,eq:vr,eq:vh,eq:vc}:
\begin{align}
V({\bo}) \;&\doteq\; \max_{\bx^r_1} \, Q({\bo},\bx^r_1).
\end{align}
By analogous reasoning, $V^{\text{NL}}({\bo}) \leq V^{\text{RL}}({\bo})$, meaning \textit{active} planning is beneficial among non-contingent planners too: whenever the robot considers how its actions affect the world, and those effects have relevant repercussions in the form of objective rewards, the planner can simulate how robot actions can such repercussions to achieve a higher return.

\paragraph{Summary}
The benefit of (1) \textit{contingency} planning, and (2) \textit{active} planning discussed above, results in a partial ordering of the \textit{true} values of each planning category seen in \cref{fig:models}:
\begin{align}
V^{\text{NL}} \;\;\leq\;\; \{V^{\text{RL}},V^{\text{HL}}\} \;\;\leq\;\; V^{\text{CL}},
\end{align}
noting the value of active contingency planning is greater-or-equal to all other planning methods.

\paragraph{T-step $Q$-values}

In a more general horizon of $T$ time-steps, the various $Q$-values are below. For brevity, we do not show the conditioning on $\bo$, and treat it as implicit.

\newcommand{\fullreturn}[0]{\text{return}(\bx^{r,h}_{1:T})}

\small{
\begin{alignat}{2}
\hat{Q}^{\text{NL}}(\bx^r_1) &\doteq \max_{\bx^r_2} \max_{\bx^r_3} \cdots \max_{\bx^r_T} \, \blue{\mathbb{E}_{\bx^h_1}} \orange{\mathbb{E}_{\bx^h_2|\bx^h_1}} \cdots \mathbb{E}_{\bx^h_T|\bx^h_{1:T-1}} && \Big[\fullreturn\Big], \nonumber \\
%
\hat{Q}^{\text{RL}}(\bx^r_1) &\doteq \max_{\bx^r_2} \max_{\bx^r_3} \cdots \max_{\bx^r_T} \blue{\mathbb{E}_{\bx^h_1}} \green{\mathbb{E}_{\bx^h_2|\bx^{r,h}_1}} \cdots \mathbb{E}_{\bx^h_T|\bx^{r,h}_{1:T-1}} && \Big[\fullreturn\Big], \nonumber \\
%
\hat{Q}^{\text{HL}}(\bx^r_1) &\doteq \blue{\mathbb{E}_{\bx^h_1}} \max_{\bx^r_2} \orange{\mathbb{E}_{\bx^h_2|\bx^h_1}} \max_{\bx^r_3} \cdots \max_{\bx^r_T} \mathbb{E}_{\bx^h_T|\bx^h_{1:T-1}} && \Big[\fullreturn\Big], \nonumber \\
%
\hat{Q}^{\text{CL}}(\bx^r_1) &\doteq \blue{\mathbb{E}_{\bx^h_1}} \max_{\bx^r_2} \green{\mathbb{E}_{\bx^h_2|\bx^{r,h}_1}} \max_{\bx^r_3} \cdots \max_{\bx^r_T} \mathbb{E}_{\bx^h_T|\bx^{r,h}_{1:T-1}} && \Big[\fullreturn\Big]. \nonumber
\end{alignat}
}
