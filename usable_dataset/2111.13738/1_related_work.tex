\section{Related Work}
\vspace{0.5em}\noindent\textbf{Active Depth Imaging.}\hspace{0.1em}
Active depth methods emit a known illumination pattern into the scene and measure the returned signal to reconstruct depth. Structured light approaches rely on this illumination to improve local image contrast~\cite{zhang2018high, scharstein2003high} and simplify the stereo-matching process. Time-of-Flight (ToF) technology instead uses the travel time of the light itself to measure distances. Indirect ToF achieves this through measuring the phase differences in the returned light~\cite{lange20003d}, whereas direct ToF methods time the departure and return of pulses of light via avalanche photodiodes~\cite{cova1996avalanche} or single-photon detectors (SPADs)~\cite{mccarthy2009long}. The low spatial-resolution depth stream we use in this work is sourced from a LiDAR direct ToF sensor. While its mobile implementation comes with the caveats of low-cost SPADs~\cite{callenberg2021low} and vertical-cavity surface-emitting lasers~\cite{warren2018low}, with limited spatial resolution and susceptibility to surface reflectance, this sensor can provide robust metric depth estimates, without scale ambiguity, on visually textureless surfaces such as blank walls. We use this kilo-pixel depth data to ensure our depth solution does not stray too far from the underlying measured depth data.

%In this work, the depth data is sourced from a mobile direct ToF LiDAR sensor, which inherits the limitations of low-cost SPAD hardware~\cite{callenberg2021low} and vertical-cavity surface-emitting lasers~\cite{warren2018low} such as low spatial resolution and susceptibility to surface reflectance. We found that

\vspace{0.5em}\noindent\textbf{Multi-View Stereo.}\hspace{0.1em}
Multi-view stereo (MVS) algorithms are passive depth estimation methods that infer the 3D shape of a scene from a bundle of RGB views and, optionally, associated camera poses. COLMAP~\cite{schonberger2016structure} estimates both poses and sparse depths by matching visual features across frames. The apparent motion of each feature in image space is uniquely determined by its depth and camera pose. Thus there exists an important relationship that, for a noiseless system, \emph{any pixel movement (disparity) not caused by a change in pose must be caused by a change in depth}. While classical approaches typically formulate this as an explicit photometric cost optimization~\cite{sinha2007multi,furukawa2009accurate,galliani2015massively}, more recent learning-based approaches bend the definition of \emph{cost}. These approaches construct  cost volumes with learned visual features~\cite{yao2018mvsnet,tankovich2021hitnet,lipson2021raft}, which aid in dense matching as they incorporate non-local information into otherwise textureless regions and are more robust to variations in lighting and noise that distort raw RGB values. In our scenario with virtually no variation in lighting or appearance, and free access to reliable LiDAR-based depth estimates in textureless regions, we look towards photometric MVS to extract parallax information from our images with poses.

\vspace{0.5em}\noindent\textbf{Monocular Depth Prediction.}\hspace{0.1em}
Single-image monocular approaches~\cite{ranftl2019towards,ranftl2021vision,godard2019digging} offer \emph{visually reasonable} depth maps, where foreground and background objects are clearly separated but may not be at a correct scale, with minimal data requirements -- just a single image.  Video-based methods such as \cite{fonder2021m4depth,luo2020consistent,watson2021temporal} leverage structure-from-motion cues~\cite{ullman1979interpretation} to extract additional information on scene scale and geometry. Works such as \cite{ha2016high,im2018accurate} use video data with \emph{small} (decimeter-scale) motion, and \cite{joshi2014micro,yu20143d} explore micro-baseline (mm-scale) measurements. As the baseline decreases so does the scale of parallax motion cues, and the depth estimation problem gradually devolves from MVS to single-image prediction. Our work investigates how we can leverage monocular information readily available during a single snapshot -- high-resolution images synchronized with 3D pose and coarse depth -- to distill high-quality depth.