\section{Introduction}
The cell-phone of the 90s was a phone, the modern cell-phone is a handheld computational imaging platform~\cite{delbracio2021mobile} that is capable of acquiring high-quality images, pose, and depth. Recent years have witnessed explosive advances in passive depth imaging, from single-image methods that leverage large data priors to predict structure directly from image features~\cite{ranftl2021vision,ranftl2019towards} to efficient multi-view approaches grounded in principles of 3D geometry and epipolar projection~\cite{tankovich2021hitnet, shamsafar2021mobilestereonet}. Alongside, progress has been made in the miniaturization and cost-reduction~\cite{callenberg2021low} of active depth systems such as LiDAR and correlation time-of-flight sensors~\cite{lange2001solid}. This has culminated in their leap from industrial and automotive applications~\cite{schwarz2010mapping,dong2017lidar} to the space of mobile phones. Nestled in the intersection of high-resolution imaging and miniaturized LiDAR we find modern smartphones, such as the iPhone 12 Pro, which offer access to high frame-rate, low-resolution depth and high-quality pose estimates.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/teaser.pdf}
    \caption{We reconstruct centimeter-scale depth features for this tabletop object from nothing more than a handheld snapshot.}
    \label{fig:my_label}
    \vspace{-1em}
\end{figure}

As applications of mixed reality grow, particularly in industry~\cite{li2018critical} and healthcare~\cite{gerup2020augmented} settings, so does the demand for convenient systems to extract 3D information from the world around us. Smartphones fit this niche well, as they boast a wide array of sensors -- e.g. cameras, magnetometer, accelerometer, and the aforementioned LiDAR system -- while remaining portable and affordable, and consequently ubiquitous. Image, pose, and depth data from mobile phones can drive novel problems in view synthesis~\cite{mildenhall2020nerf, park2021nerfies}, portrait relighting~\cite{pandey2021total,sun2019single}, and video interpolation~\cite{bao2019depth} that either implicitly or explicitly rely on depth cues, as well as more typical 3D understanding tasks concerning salient object detection~\cite{zhang2021bts,fan2020rethinking}, segmentation~\cite{schwarz2018rgb}, localization~\cite{zhuang2021semantic}, and mapping~\cite{schops2019bad, mur2017orb}.  

Although 3D scene information is essential for a wide array of 3D vision applications, today's mobile phones do not offer accurate high-resolution depth \emph{from a single snapshot}. While RGB image data is available at more than 100 megapixels (e.g., Samsung ISOCELL HP1), the most successful depth sensors capture at least three orders of magnitude fewer measurements, with pulsed time-of-flight sensors~\cite{morimoto2020megapixel} and modulated correlation time-of-flight imagers~\cite{lange20003d,hansard2012time,kolb2010time} offering kilopixel resolutions. Passive approaches can offer higher spatial resolution by exploiting RGB data; however, existing methods relying on stereo~\cite{chen20173d, Chang2018, Kendall2017} depth estimation require large baselines, monocular depth methods~\cite{chen2016monocular,ranftl2021vision} suffer from scale ambiguity, and structure-from-motion methods~\cite{schonberger2016structure} require diverse poses that are not present in a single snapshot. Accurate high-resolution snapshot depth remains an open challenge. 

% For mobile phones this kind of depth information can also be particularly important as it can in ways make up for camera hardware limitations, for example by allowing refocusing post-capture (cite?)~\todo{unclear what you mean here}.  
% Modern phones contain a lot more hardware than just a camera such as the IMU, magnetometer, etc; this data can be incredibly useful for localization of the phone and overall 3D reconstruction (cite?).
For imaging tasks, \emph{align and merge} computational photography approaches have long exploited subtle motion cues \emph{during a single snapshot capture}. These take advantage of the photographer's natural hand tremor during viewfinding to capture a sequence of slightly misaligned images, which are fused into one super-resolved image~\cite{wronski2019handheld, tsai1984multiframe}. These misaligned frames can also be seen as mm-baseline stereo pairs, and works such as \cite{yu20143d, joshi2014micro} find that they contain enough parallax information to produce coarse depth estimates. Unfortunately, this micro-baseline depth is not enough to fuel mixed reality applications alone, as it lacks the ability to segment clear object borders or detect cm-scale depth features. In tandem with high-quality poses from phone-based SLAM~\cite{durrant2006simultaneous} and low-resolution LiDAR depth maps, however, we can use the high-resolution micro-baseline depth cues to guide the reconstruction of a refined high-resolution depth map. We develop a pipeline for recording LiDAR depth, image, and pose bundles at 60~Hz, with which we can conveniently record 120 frame bundles of measurements during a single snapshot event.

With this hand shake data in hand, we take a test-time optimization approach to distill a high-fidelity depth estimate from hand tremor measurements. Specifically, we learn an implicit neural representation of the scene from a bundle of measurements. Depth represented by a coordinate multilayer perceptron (MLP) allows us to query for depth at floating point coordinates, which matches our measurement model, as we effectively traverse a continuous path of camera coordinates during the movement of the photographer's hand. We can, during training, likewise conveniently incorporate parallax and LiDAR information as photometric and geometric loss terms, respectively. In this way we search for an accurate depth solution that is consistent with low-resolution LiDAR data, aggregates depth measurements across frames, and matches visual features between camera poses similar to a multi-view stereo approach. Specifically, we make the following contributions:
\begin{itemize}
    \item A smartphone app with a point-and-shoot user interface for easily capturing synchronized RGB, LiDAR depth, and pose bundles in the field.\vspace{-0.25em}
    \item An implicit depth estimation approach that aggregates this data bundle into a single high-fidelity depth map.\vspace{-0.25em}
    \item Quantitative and qualitative evaluations showing that our depth estimation method outperforms existing single and multi-frame techniques.
\end{itemize}
%
We will make the smartphone app, training code, experimental data, and trained models available at: \\ \href{https://github.com/princeton-computational-imaging/HNDR}{github.com/princeton-computational-imaging/HNDR}