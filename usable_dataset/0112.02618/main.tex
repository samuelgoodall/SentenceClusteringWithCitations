\documentclass{article}
\usepackage{iclr2022_conference,times}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{caption}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{diagbox}
\usepackage{color} 
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{lineno}
\usepackage[pdftex]{graphicx}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs
\usepackage[labelformat=simple]{subcaption}
\usepackage{wrapfig}
\newenvironment{smalleralign}[1][\small]
 {\par\nopagebreak\leavevmode\vspace*{-\baselineskip}%
  \skip0=\abovedisplayskip
  #1%
  \def\maketag@@@##1{\hbox{\m@th\normalfont\normalsize##1}}%
  \abovedisplayskip=\skip0
  \align}
 {\endalign\ignorespacesafterend}
\makeatother

\usepackage{natbib}
\usepackage{multicol, multirow}
\usepackage[ruled,vlined]{algorithm2e}
\newcommand{\hang}[1]{{\color{blue} #1}}
\newcommand{\taher}[1]{{\color{blue} [TA: #1]}} %
\newcommand{\AS}[1]{{\color{magenta} #1}} %[AS: #1]
\newcommand{\boldpi}{\boldsymbol{\pi}}
\newcommand{\mc}{\mathcal}
\newcommand{\N}{\mc{N}}
\newcommand{\G}{\mc{G}}
\newcommand{\T}{\top}
\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\orange}{\textcolor{orange}}
\newcommand{\taskpolicy}{{}} 
\newcommand{\safepolicy}{{2, {\rm safe}}}
\newcommand{\intervenepolicy}{{\rm int}}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\input{mathdef}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output} 
\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
% \newtheorem*{remark-non}{Remark}
% \newenvironment{proof}{
%   \renewcommand{\proofname}{Sketch}\proof}{\endproof}
% \newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
  \renewcommand\customgenericname{#2}%
  \renewcommand\theinnercustomgeneric{##1}%
  \innercustomgeneric
  }
  {\endinnercustomgeneric}
}
\newcustomtheorem{customtheorem}{Theorem}
% \usepackage{lineno}
% \linenumbers
% \usepackage{setspace}
% \doublespacing

\usepackage{hyperref}
\usepackage{cleveref}
\renewcommand\thesubfigure{(\alph{subfigure})}

\title{%Learning to Generate Intrinsic Rewards to \\ Optimise Multi-agent Learning 
LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning 
}
 

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\author{%
\textbf{David  Mguni$^{1}$\thanks{{{Correspondence to david.mguni@huawei.com.}}},  Taher Jafferjee$^1$, Jianhong Wang$^{2}$, Oliver Slumbers Nicolas$^{1,5}$ Perez-Nieves$^{2}$, } \\ \textbf{Feifei Tong$^1$, Li Yang$^3$, Yaodong Yang$^4$, Jiangcheng Zhu$^1$, Jun Wang$^5$} \\ 
  $^1$Huawei Technologies, $^2$Imperial College London, \\ $^3$Shanghaitech University,$^4$King's College London, $^5$University College London.}
  
\iclrfinalcopy 
\begin{document}

\maketitle

% \textbf{Items to be tuned: }
% \begin{itemize}
%     \item Parameters for $\phi$ i.e. tune NN weights (as opposed to using Xavier initialisation)
%     \item Learning rates for \{agents 1-N, generator switching policy, generator action policy\}
%     \item Size of generator action set (try singleton), magnitudes of generator  actions
%     \item Coefficients in each objective
%     \item Intervention cost magnitude
%     \item Termination probability 
%     \item Size of RND
% \end{itemize}
% \textbf{Investigations}
% \begin{itemize}
%     \item Mechanism for switching off shaping reward during training. Do so after $k$ consecutive episodes of declining reward
%     \item Try linear combo of RNDs with input taken from the following spaces $\boldsymbol{\mathcal{A}}\times\mathcal{S}$, $\boldsymbol{\mathcal{A}}$, $\mathcal{S}$
%     \item Test different base learners other than PPO
% \end{itemize}

% \textbf{Tasks} 
% \begin{itemize}
%     \item Add further (3+) SMAC experiments
%     \item Add subgoal discovery experiment (test: reward on button, coordination required on button, augment)... \textit{The issue here is that the RND signal may diminish at the optimal goal state before the button has been pressed. Some possible remedies are 1. increasing the size of the RND. 2. having an observation of whether the button has been pressed, i.e. augment the state space to have a binary component. 3. adding a time component to our shaping reward function.}
%     \item Add heatmap of where intrinsic rewards have been added
%     \item Show heatmap with an ablation study of two versions of the same policy. e.g. have a high variance policy and an low variance policy and two goal states one very high reward goal state surrounded by high penalty states and a medium reward goal state.
% \end{itemize}

% \textbf{Tasks for Journal} 
% \begin{itemize}
%     \item Add more SMAC experiments
%     \item Add exploration trap experiment (see ROSA paper Experiment 3).
%     \item Add experiments from new domain (e.g. multi-agent mujoco)
%     \item Add more baselines
%     \item Extend convergence proof to non-linear function approximators?
% \end{itemize}

% %   \section*{Ideas for improvement (1/3)}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X  
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Issue/Question} & \textbf{Suggested Resolution/Check} & \textbf{Notes} & \textbf{Owner}& \textbf{Status}  \\\hline\hline
% %   Is the shaping reward agent permutation invariant?   &Check the output of $F$ should be different for each agent & Permutation invariance is consistent with our formalisation  &TJ &
% %   \\\hline
% %   Is the RND adding something useful?   &  Check $L\to 0$ as number of state-action pair visitations tends to $\infty$. Check RND term in agent 0 objective is properly scaled. Check $L:\boldsymbol{\mathcal{A}}\times\mathcal{S}\to \mathbb{R}$ is better than $L:\mathcal{S}\to \mathbb{R}$. Try also $L:\boldsymbol{\mathcal{A}}\times\mathcal{S}\to \mathbb{R}=L_1(s,\boldsymbol{a})+L_2(s)$ & Are the state definition and the action space definition appropriate here?   &TBA &
% %     \\\hline
% %   Do we need a shaper for each learning process? &  Implement $N$ reward-shaper framework: $N$ reward shapers with shared trunk    &  Having 1 shaper for all agents may be inappropriate. Can test each agent having its own shaper. All shaper NNs share a common trunk  &TBA &
% %       \\\hline
% %   Are the rewards scaled appropriately?  & Testing checks and magnitudes & Needs to be done for intrinsic versus extrinsic rewards + $L$ term and cost for the agent 0 objective  &TJ &
% %     \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}    
% % \section*{Ideas for improvement (2/3)}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X  
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Issue/Question} & \textbf{Suggested Resolution/Check} & \textbf{Notes} & \textbf{Owner}& \textbf{Status}  \\\hline\hline
% %   Are these optimised: \textbf{Learning rates, size of switching cost, termination criteria}   & Perform tuning  & -&TBA &
% %   \\\hline
% %   Are we using a good $\phi$ function?
% %   & Check RND (as done previously), generic neural network & & TBA &
% %   \\\hline
% %   Does the RND in {\fontfamily{cmss}\selectfont Generator} objective behave as we expect? & Check input of $\mathcal{S}\times\mathcal{A}$ against input of just $\mathcal{S}$. Check performance in each case, check if the value of $L$ is diminishing.& &TBA
% %   \\\hline
% %   Are the training speeds for agent 0 and agents $1-N$ relatively ok? & Check effect on slowing down agent 0 training speed: try i) reducing learning rate ii) freezing the updates for some iterations. &&TBA &
% %   \\\hline
% %   What is the optimal weighting on terms within agent $i>0$ and agent 0 objectives  &Introduce weighting coefficients to be optimised & Check to see if there are general principles  &TJ &
% %   \\\hline
% %   Is the hypothesis that removing $F$ from agent 0 objective helps true? &   & 
% %     \\\hline
% %   What is the optimal observation set of shaper - include subset of joint actions?& Think of reasonable observation sets for the shaper & -  &TBA& 
% %     \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}    
% % \newpage
% % \section*{Ideas for improvement (3/3)}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X  
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Issue/Question} & \textbf{Suggested Resolution/Check} & \textbf{Notes} & \textbf{Owner}& \textbf{Status}  \\\hline\hline
% %  Can a mechanism that switches off shaping mechanism help?  & Idea is after some training, the shaping reward is no longer useful. Try a termination criterion that switches off shaping reward after $n$ e.g. 2 episodes of decreasing reward  &TJ &
% %         \\\hline
% %   Dealing with cont. states discrete actions? for augmented RND term and $\phi$.    & Perform tuning  & -&TBA &
% %   \\\hline
% %   What is the best exploration bonus term
% %   & for discrete we can use counting measure & & TBA &
% %   \\\hline
% %   Does the RND in {\fontfamily{cmss}\selectfont Generator} objective behave as we expect? & Check input of $\mathcal{S}\times\mathcal{A}$ against input of just $\mathcal{S}$. Check performance in each case, check if the value of $L$ is diminishing.& &TBA
% %   \\\hline
% %   Are the training speeds for agent 0 and agents $1-N$ relatively ok? & Check effect on slowing down agent 0 training speed: try i) reducing learning rate ii) freezing the updates for some iterations. &&TBA &
% %   \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}    
% % \section*{Tasks}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Task} & \textbf{Outputs} & \textbf{Notes} & \textbf{Owner} \\\hline\hline
% %   check current implementation, switch to value based method   & - & Different value based methods. Decide which is best &TJ, Jianhong \\
% %  \hline
% %  Augment implementation to MAS setting with joint action input & Updated MARL ready algo & RND $L$ term to be updated to now take joint action (test without also). Agent $0$ to have reward which takes the global reward of all agents. $F$ term is identical for all agents &TJ \\
% %  \hline
% %  Perform simple proof of concept experiment for MARL exploration in StarCraft & Performance curve vs training, compare against standard baseline (use standard baseline as plug and play) & Simple experiment can be exploration-based with suboptimal team nash equilibrium. Perhaps use fully decentralised MARL algo &TJ \\
% %   \hline
% %   Perfect code and send to friends
% %   & & &TJ, Jianhong \\ 
% %     \hline
% % Perform multi-agent Mujoco experiments & Performance curves against baselines and metrics &  & Jack
% %  \\     
% %   \hline
% %  Design simple conceptual experiments & design setup and metrics &  & .., DM, TJ
% %  \\     \hline
% %  Do literature review
% %   & Comprehensive study or relevant MARL frameworks and methods  &  & DM 
% %   \\
% %  \hline
% %   Find and run all relevant MARL baselines
% %   & List of MARL baselines \& run them on our experiments  &  & .. + 
% %   \\
% %      \hline
% %   Update proofs and writing for MARL setup
% %   &  &  & DM 
% %   \\
% % %      \hline
% % %   Extend Brian Swenson proof to SGs
% % %   &  &  &  Zhihao
% % %   \\
% % %      \hline
% % %   Do optimal dependent convergence proof
% % %   &  &  &  DM
% % %   \\
% % %      \hline
% % %   Do optimal independent consensus convergence proof
% % %   &  &  &  DM
% %   \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}
    
% % \section*{Outputs}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Task} & \textbf{Outputs} & \textbf{Notes} & \textbf{Owner} \\\hline\hline
% %   check current implementation, switch to value based method   & - & Different value based methods. Decide which is best &TJ, Jianhong \\
% %  \hline
% %  Augment implementation to MAS setting with joint action input & Updated MARL ready algo & RND $L$ term to be updated to now take joint action (test without also). Agent $0$ to have reward which takes the global reward of all agents. $F$ term is identical for all agents &TJ \\
% %  \hline
% %  Perform simple proof of concept experiment for MARL exploration in StarCraft & Performance curve vs training, compare against standard baseline (use standard baseline as plug and play) & Simple experiment can be exploration-based with suboptimal team nash equilibrium. Perhaps use fully decentralised MARL algo &TJ \\
% %   \hline
% %   Perfect code and send to friends
% %   & & &TJ, Jianhong \\ 
% %     \hline
% % Perform multi-agent Mujoco experiments & Performance curves against baselines and metrics &  & Jack
% %  \\     
% %   \hline
% %  Design simple conceptual experiments & design setup and metrics &  & .., DM, TJ
% %  \\     \hline
% %  Do literature review
% %   & Comprehensive study or relevant MARL frameworks and methods  &  & DM 
% %   \\
% %  \hline
% %   Find and run all relevant MARL baselines
% %   & List of MARL baselines \& run them on our experiments  &  & .. + 
% %   \\
% %      \hline
% %   Update proofs and writing for MARL setup
% %   &  &  & DM 
% %   \\
% % %      \hline
% % %   Extend Brian Swenson proof to SGs
% % %   &  &  &  Zhihao
% % %   \\
% % %      \hline
% % %   Do optimal dependent convergence proof
% % %   &  &  &  DM
% % %   \\
% % %      \hline
% % %   Do optimal independent consensus convergence proof
% % %   &  &  &  DM
% %   \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}

\maketitle

\begin{abstract}
%Coordinated learning is central to optimising performance in many multi-agent systems. This affects both how quickly agents find actions that increase their own rewards during exploration and their ability to coordinate to achieve optimal outcomes.
Efficient exploration is important for reinforcement learners (RL) to achieve high rewards. In multi-agent systems, \textit{coordinated}  exploration and behaviour is critical for agents to jointly achieve optimal outcomes. In this paper, we introduce a new general framework for improving
coordination and performance of multi-agent reinforcement learners (MARL). Our framework,
named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS) introduces
an adaptive learner, Generator that observes the agents and learns to construct
intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour.
Using a novel combination of reinforcement learning (RL) and switching
controls, LIGS determines the best states to learn to add intrinsic rewards which
leads to a highly efficient learning process. LIGS can subdivide complex tasks making them easier to solve and enables systems of
RL agents to quickly solve environments with sparse rewards. 
LIGS can seamlessly adopt existing multi-agent RL algorithms and our theory shows that it ensures convergence to joint policies that deliver higher system performance. We demonstrate
the superior performance of the LIGS framework in challenging tasks in Foraging and StarCraft
II.
\end{abstract}




\section{Introduction}
% [Motivatation from neuroscience?]

Cooperative multi-agent reinforcement learning (MARL) has emerged as a powerful tool to enable autonomous agents to solve various tasks such as ride-sharing \cite{zhou2020smarts} and swarm robotics \cite{huttenrauch2017guided,mguni2018decentralised}. In multi-agent systems (MAS), maximising system performance often requires agents to coordinate during exploration and learn coordinated joint actions \cite{matignon2012independent}. However, in many MAS, the reward signal provided by the environment is not sufficient to guide the agents towards coordinated behaviour \cite{matignon2012independent}. Consequently, relying on solely the individual rewards received by the agents may not lead to optimal outcomes \cite{mguni2019coordinating}.  This problem is worsened by the fact that MAS can have many stable points some of which lead to arbitrarily bad outcomes \cite{roughgarden2007introduction}.

% A key issue in MARL is exploration efficiency. 
As in single agent RL, in MARL inefficient exploration can dramatically decrease sample efficiency. In MAS, a major challenge is how to overcome sample inefficiency from poorly \textit{coordinated exploration}. Unlike single agent RL, in MARL, the collective of agents is typically required to coordinate its exploration to find their optimal joint policies\footnote{In single agent RL exploration issues can be mitigated by adjusting
exploration rates or the policy variance \cite{tijsma2016comparing}. However it has been shown that the same is not possible in MARL \cite{mahajan2019maven}.}.
% This challenging feature of MAS is further exacerbated by  representational constraints necessary for decentralisation (for example QMIX \cite{rashid2018qmix} requires a monotonic value function factorisation).
A second issue is that in many MAS settings of interest, such as video games and physical tasks, rich informative signals of the agents' \textit{joint} performance are not readily available \cite{hosu2016playing}. For example, in StarCraft Micromanagement \cite{samvelyan2019starcraft}, the sparse reward alone (win, lose) gives insufficient information to guide agents toward their optimal joint policy. Consequently, MARL requires large numbers of samples producing a great need for MARL methods that can solve such problems efficiently. 


% Among MARL methods are a class of algorithms known as  independent learners e.g. independent Q learning \cite{tan1993multi}. These algorithms ignore actions of other agents and are ill-suited to tackle MAS and often fail to coordinate or even learn 
% % due to the appearance of a \textit{nonstationary} environment during learning
% \cite{hernandez2017survey}. 
% % potentially leading to catastrophic outcomes. 
% In contrast, 
To aid coordinated learning, algorithms such as QMIX \cite{rashid2018qmix}, MAVEN \cite{mahajan2019maven} and COMA \cite{foerster2018counterfactual}, so-called centralised critic and decentralised execution (CT-DE) methods use a centralised critic whose role is to estimate the agents' expected returns. The critic makes use of all available information generated by the system, specifically the global state and the joint action. 
% Given this, the critic is limited since it cannot promote joint exploration among the agents.
To enable effective CT-DE, it is critical that the joint greedy action should be equivalent to the collection of individual greedy actions of agents, which is called the IGM (Individual-Global-Max) principle \cite{son2019qtran}.  
CT-DE methods are however, prone to convergence to suboptimal joint policies \cite{wang2020towards}. In particular, existing value factorisations, e.g. QMIX and VDN \cite{sunehag2017value}, cannot ensure an exact guarantee of IGM consistency \cite{wang2020qplex}. Moreover, CT-DE methods such as QMIX require a monotonicity condition which is violated in scenarios where multiple agents must coordinate but are penalised
if only a subset of them do so (see Exp. 2, Sec. \ref{exp:foraging}).  


To tackle these issues, in this paper we introduce a new MARL framework, LIGS that constructs intrinsic rewards online which guide MARL learners towards their optimal joint policy. LIGS involves an \textit{adaptive} intrinsic reward agent, 
% the \yaodong{i think there should be a the?}
{\fontfamily{cmss}\selectfont Generator} that selects intrinsic rewards to add according to the history of visited states and joint actions performed by the agents. {\fontfamily{cmss}\selectfont Generator} adaptively guides the agents' exploration and behaviour towards coordination and maximal joint performance. A pivotal feature of LIGS is the novel combination of RL and \textit{switching controls} \cite{bayraktar2010one,mguni2018viscosity} which enables it to determine the best set of states to learn to add intrinsic rewards while disregarding less useful states. This enables {\fontfamily{cmss}\selectfont Generator} to quickly learn how to set intrinsic rewards that guide the agents during their learning process.  
% With this, {\fontfamily{cmss}\selectfont Generator} guides the agents toward joint policies that maximise the system payoff. 
Moreover, the intrinsic rewards added by {\fontfamily{cmss}\selectfont Generator} can significantly deviate from the environment rewards. This enables LIGS to both promote complex \textit{joint exploration} patterns and decompose difficult tasks. Despite this flexibility, special features within LIGS ensure the underlying optimal policies are preserved so that the agents learn to solve the task at hand.   


% \textbf{KEY CHALLENGES} 

% \textbf{1.} \textbf{Coordinated (joint) exploration.}
% Performing required joint action at a given state. {EXPERIMENT:} Efficient joint exploration  
%     \newline\textbf{2.} \textbf{Optimising convergence points.} {EXPERIMENT:} COORDINATION
%     \newline\textbf{3.}\textbf{ Sparse rewards environments.} {EXPERIMENT:} SPARSE REWARD 
%     % \newline\textbf{4.} Adaptive Correlating device (avoidance of miscoordination)
% % \newline {EXPERIMENT:} MERGE TRAFFIC PROBLEM
%     \newline\textbf{4.} \textbf{Multi-agent subgoal discovery.} {EXPERIMENT:} SUBGOAL DISCOVERY

Overall, LIGS has several advantages:  
% By encoding the agents' joint learning agenda and history of state and joint actions into its objective, 
\newline$\bullet$ LIGS has the freedom to introduce rewards that vastly deviate from the environment rewards. With this, LIGS promotes \textit{coordinated exploration} (i.e. visiting unplayed state-joint actions) among the agents enabling them to find joint policies that maximise the system rewards and generates intrinsic rewards to aid solving sparse reward MAS.
%  \newline\textbf{iii)} Our framework is guaranteed to preserve the solution of the dec-\textbf{PO}MDP framework. This ensures that its solutions can be be sustained by decentralised executors after training and once {\fontfamily{cmss}\selectfont Generator} has been removed
\newline$\bullet$ LIGS selects which best states to add intrinsic rewards \textit{adaptively} in response to the agents' behaviour while the agents learn leading to an efficient learning process.
% \textbf{iii)} The computational complexity of our {\fontfamily{cmss}\selectfont Generator} is much lower: the centralised agent now only introduces modifications over a subset of states. It's action set is also smaller.  
\newline$\bullet$ LIGS's intrinsic rewards preserve the agents' optimal joint policy and ensure that the total \textit{environment} return is (weakly) increased.

% \yaodong{if you choose to merge the below items into a paragraph, which i think is also fine. and then you can end Introduction up with advantages of LIGS}
To enable the framework to perform successfully, we overcome several challenges: 
\textbf{i)} Firstly, constructing an intrinsic reward can change the underlying problem leading to the agents solving irrelevant tasks  \cite{mannion2017policy}. We resolve this by endowing the intrinsic reward function with special form which both allows a rich spread of intrinsic rewards while preserving the optimal policy. 
\textbf{ii)} Secondly, introducing intrinsic reward functions can \textit{worsen} the agents' performance \cite{devlin2011theoretical} and doing so \textit{while training} can lead to convergence issues.  We prove LIGS leads to better performing policies and that LIGS's learning process converges and preserves the MARL learners' convergence properties. 
% \newline$\bullet$ Finding the appropriate intrinsic rewards at each state leads to an expensive computation which can become infeasible in settings with many states. In LIGS, {\fontfamily{cmss}\selectfont Generator} uses
% % a type of control known as 
% \textit{switching controls} \cite{bayraktar2010one} to determine the best states to add an intrinsic reward which needs only that {\fontfamily{cmss}\selectfont Generator} learns the best intrinsic in a subset of states. This lowers computational complexity for {\fontfamily{cmss}\selectfont Generator}'s problem.
\textbf{iii)} Lastly, adding an agent {\fontfamily{cmss}\selectfont Generator} with its own goal leads to a Markov game (MG) with $N+1$ agents \cite{fudenberg1991tirole}. 
% Solving MGs involves finding a stable point in which each agent responds optimally to the actions of the other. 
Tractable methods for solving MGs are extremely rare with convergence only in special cases  \cite{yang2020overview}. Nevertheless, using a special set of features in LIGS's design, we prove LIGS converges to a solution in which it learns an intrinsic reward function that improves the agents' performance.
% and that LIGS preserves the convergence of the agents' MARL algorithms. 
% from which the agents can learn their optimal value function for the task. 

% \textbf{Key results}

% \textbf{1.} The solution of the game with {\fontfamily{cmss}\selectfont Generator} coincides with the solution of the dec-POMDP (DONE)\\
% \textbf{2.} Almost every stochastic potential game and for almost every initial condition, the best-response dynamics have a unique solution (extend Swenson et. al 2018)\\
% \textbf{3.} Given any convergent multi-agent RL algorithm, LIGS converges to the optimal dec-POMDP solution (adapt Wiewiora, 2003 +1 + 2+ our Theorem 1)\\ 
% \textbf{4.} Given any multi-agent RL algorithm with consensus, LIGS converges to the optimal dec-POMDP solution (adapt. K. Zhang, ICML 2018 +3).

% 
% 



% \textbf{Principal-Agent models (Mechanism Design)}


% 
% 
% A \textit{pure policy} (PS) is a map $\pi_i: \mathcal{S}\to\mathcal{A}_i $, for any $i\in\mathcal{N}$ that assigns to any state an action in $\mathcal{A}_i$. 
% 
% \subsubsection*{Partially Observable Multi-Agent Systems: Decentralised POMPDs}
% 
% \textcolor{gray}{Partially observable multi-agent scenarios are modelled by Decentralised partially observable MDPs (Dec-POMDPs) \cite{amato2013decentralized}. In a Dec-POMDP, each agent draws individual observations $z\in Z$ according to an observation function $O:\mathcal{S} \times \mathcal{A}_i \to  Z$. Each agent has an action-observation history $\tau^i={(a^i_k,z^i_k)}_{k\geq 0}$ on
% which it conditions its individual stochastic policy $\pi^i(a|z): Z \times \mathcal{A}_i\to [0,1]$. As before, each agent seeks to maximise its value
% function 
% $v_i$.}
% $v^{\pi^i,\pi^{-i}}(s_t) = \mathbb{E}_{\pi^i,\pi^{-i}}\left[\sum_{t\geq 0}\gamma^tR(s_t,\boldsymbol{a}_t)\right]$.
% 
% ...
% 
% In settings in which the reward signal is sparse, $R$ is not informative enough to provide a signal from which the $N$ agents can efficiently learn a joint optimal policy. 
%  
% Crucially, the solution space of Dec-POMPDs in general \textit{does not coincide to the solution space of dec-MDPs}.\DM{Highlight and spell this out mathematically} (see Example XX in Sec. XX).
% 
% 
% 

\section{Related Work}
% \textbf{Centralised Training Decentralised Execution methods} such as QMIX (and similar methods) \cite{rashid2018qmix} imposes representational constraints on the
% joint action-values which can lead to provably poor exploration and suboptimality. The QMIX algorithm learns factored value functions by decomposing the joint value function into factors that depend only on individual agents. This decentralisation which requires a monotonicity condition constraint restricts QMIX to suboptimal
% value approximation. QTRAN \cite{son2019qtran} formulates the MARL $\mathcal{M}$s an optimisation problem with linear constraints and relaxing it with L2 penalties for tractability.\newline
\textbf{Multi-agent exploration methods} seek to promote coordinated exploration among MARL learners. Maven et. al \cite{mahajan2019maven} propose a hybridisation of value and policy-based methods that uses mutual information to learn a diverse set of behaviours between agents. Though this approach promotes coordinated exploration, it does not encourage exploration of novel states. Other approaches to promote exploration in MARL while assuming aspects of the environment are known in advance and agents can perform perfect communication between themselves \cite{viseras2016decentralized}.  Similarly, to promote coordinated exploration in partially observable settings,   \cite{pesce2020improving} propose end-to-end learning of a communication protocol through a memory device. In general, exploration-based methods provide no performance guarantees nor do they ensure the optimal policy (of the underlying dec-MDP) is preserved. Moreover, many employ heuristics that naively reward exploration to unvisited states without consideration of the environment reward. This can lead to spurious objectives being maximised.
% \DM{More exploration stuff has been requested}
\newline
% 
% 
%  \cite{iqbal2019coordinated} Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning --- this paper has not been published so we may not need to include this in our related work
%  
% \cite{wang2019influence} Influence-based multi-agent exploration, \cite{liu2021cooperative} Cooperative Exploration for Multi-Agent Deep Reinforcement Learning, \cite{bohmer2019exploration}, 
% 
% \cite{luo2019multi} Multi-agent collaborative exploration through graph-based deep reinforcement learning,
% 
%   
% (see Experiment XX in \S \ref{XXX}). 
\textbf{Reward shaping} \cite{harutyunyan2015expressing} (RS) is a technique which aims to alleviate the problem of sparse and uninformative rewards by supplementing the agent's reward with a prefixed term $F$. 
% (which can take a variety of functional forms). 
% This augments the objective to $
% v^{\pi}(s_0)=\mathbb{E}_{\pi,P}\left[\sum_{t=0}^\infty \gamma^t\left\{R(s_t,a_t)+F\right\}\right]$ where $F$  serves to supplement the agent's reward signal.
In \cite{ng1999policy} it was established that adding a \textit{shaping reward function} of the form $F(s_{t+1},s_{t})=\gamma\phi(s_{t+1})-\phi(s_t)$ preserves the optimal policy and in some cases can aid learning. 
% and in fact lead to a policy which is irrelevant to the task.   
% 
RS has been extended to MAS \cite{devlin2011empirical,mannion2018reward,devlin2011theoretical, devlin2012dynamic, devlin2016plan,sadeghlou2014dynamic} in which it is used to promote convergence to efficient social welfare outcomes. 
% and, in noncooperative settings, convergence to stable points (Nash equilibria) that offer high payoffs
% Similar to the single agent setting, 
Poor choices of $F$ in a MAS can slow the learning process and 
% . Moreover, since MAS have a number of stable points, 
% % each of which are attraction basins for MARL algorithms.
% % This means that 
% poor choices of $F$ 
can induce convergence to poor system performance \cite{devlin2011theoretical}. In MARL, the question of which shaping function to use remains unaddressed.
% Attempts to learn $F$ present convergence issues given the concurrent learning of both the optimal policy and the reward function components.
Typically, RS algorithms rely on hand-crafted shaping reward functions that are constructed using domain knowledge, contrary to the goal of autonomous learning~\cite{devlin2011theoretical}. As we later describe LIGS, which successfully \textit{learns} an instrinsic reward function $F$, uses a similar form as PBRS however, $F$ is now augmented to include the actions of another RL agent to learn the intrinsic rewards online. In \cite{du2019liir} an approach towards learning intrinsic rewards is proposed in which a parameterised intrinsic reward is learned using a bilevel approach through a centralised critic. Loosely related are single-agent methods \cite{zheng2018learning, dilokthanakul2019feature, kulkarni2016hierarchical, pathak2017curiosity} which, in general, introduce heuristic terms to generate intrinsic rewards. 
% For example, in \cite{dilokthanakul2019feature} the intrinsic reward is defined as the squared difference between two consecutive states, and \cite{pathak2017curiosity} which uses a `curiosity’ heuristic as an intrinsic reward.

% This increases the burden on the designer and results in domain-specific shaping reward functions.
% Consequently, finding an appropriate $F$ is a significant challenge. 

Within these categories, closest to our work is the intrinsic reward approach in \cite{du2019liir}. There, the agents' policies and intrinsic rewards are learned with a bilevel approach. In contrast, LIGS performs these operations \textit{concurrently} leading to a faster, more efficient procedure.  A crucial point of distinction is that in LIGS, the intrinsic rewards are constructed by an RL agent ({\fontfamily{cmss}\selectfont Generator}) with its own reward function. Consequently, LIGS can generate complex patterns of intrinsic rewards, encourage \textit{joint exploration}. Additionally, LIGS learns intrinsic rewards only at relevant states, this confers high computational efficiency.  Lastly, unlike exploration-based methods e.g., \cite{mahajan2019maven}, LIGS ensures preservation of the agents' joint optimal policy for the task.
% 
%
\section{Preliminaries}
% We consider RS with switching controls in multi-agent team game settings. 
% 
% \textbf{DECENTRALISED-MARKOV DECISION PROCESSES (DEC-MDPS)}
% \vspace{-4.5 mm} 
A fully cooperative MAS is modelled by a decentralised-Markov decision process (dec-MDP) \cite{yang2020overview}. A dec-MDP is an augmented MDP involving a set of $N\geq 2$  agents denoted by $\mathcal{N}$ that independently decide actions to take which they do so simultaneously over many rounds. Formally, a dec-MDP is a tuple $\mathfrak{M}=\langle \mathcal{N},\mathcal{S},\left(\mathcal{A}_{i}\right)_{i\in\mathcal{N}},P,R,\gamma\rangle$ where $\mathcal{S}$ is the finite set of states, $\mathcal{A}_i$ is an action set for agent $i\in\mathcal{N}$ and $R:\mathcal{S}\times\boldsymbol{\mathcal{A}}\to\mathcal{P}(D)$ is the reward function that all agents jointly seek to maximise where $D$ is a compact subset of $\mathbb{R}$ and lastly, $P:\mathcal{S} \times \boldsymbol{\mathcal{A}} \times \mathcal{S} \rightarrow [0, 1]$ is the probability function describing the system dynamics where $\boldsymbol{\mathcal{A}}:=\times_{i=1}^N\mathcal{A}_i$.  Each agent $i\in\mathcal{N}$ uses a \textit{Markov policy}
% \footnote{A Markov policy requires as input only the current state (and not the game history or other agents' actions or strategies).}
$\pi_{i}: \mathcal{S} \times \mathcal{A}_i \rightarrow [0,1]$ to select its actions. At each time $t\in 0,1,\ldots,$ the system is in state $s_t\in\mathcal{S}$ and each agent $i\in\mathcal{N}$ takes an action $a^i_t\in\mathcal{A}_i$. The \textit{joint action}\ $\boldsymbol{a}_t=(a^1_t,\ldots, a^N_t)\in\boldsymbol{\mathcal{A}}$  produces an immediate reward $r_i\sim R(s_t,\boldsymbol{a}_t)$ for agent $i\in\mathcal{N}$ and influences the next-state transition which is chosen according to $P$.  
The goal of each agent $i$ is to maximise its expected returns measured by its value function $v^{\pi^i,\pi^{-i}}(s)=\mathbb{E}_{\pi^i,\pi^{-i}}\left[\sum_{t=0}^\infty \gamma^tR(s_t,\boldsymbol{a}_t)\right]$,
where $\Pi_i$ is a compact Markov policy space and $-i$ denotes the tuple of agents excluding agent $i$.
% We refer to this as \textbf{$\mathcal{M}$}.  

Intrinsic rewards can strongly induce more efficient learning (and can promote convergence to higher performing policies) \cite{devlin2011theoretical}. We tackle the problem of how to \textit{learn} intrinsic rewards produced by a function $F^\star$ that leads to the agents learning policies that jointly maximise the system performance (through coordinated learning). Naturally, the problem of learning $F^\star$ can be approached by optimising the input $\boldsymbol{\theta}\in\mathbb{R}^m$ of a parameteric function: $\hat{F}(s,\boldsymbol{a};\boldsymbol{\theta}) $; that is, finding $\boldsymbol{\theta^\star}\in\mathbb{R}^m$ for which $F^\star(s,\boldsymbol{a})=\hat{F}(s,\boldsymbol{a},\boldsymbol{\theta^\star})$ 
% which yields optimal behaviour among the team of agents 
given an intrinsic reward $\hat{F}$ where $s$ is the state and $\boldsymbol{a}$ is the agents' joint action.
% 
Determining this function is a significant challenge since poor choices can hinder learning and the concurrency of multiple learning processes presents potential convergence issues in a system already populated by multiple learners \cite{zinkevich2006cyclic}. Additionally, we require that the method preserves the optimal joint policy and the underlying dec-MDP. Note that using an optimisation procedure to find $\boldsymbol{\theta^\star}$ directly does not make use of information generated by intermediate  state-joint-action-reward tuples of the RL problem which can guide the optimisation.  


\section{The LIGS Framework} 

%As alluded to earlier, 
 

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=.99\linewidth]{figs/flow.pdf}
%     \caption{(a) Schematic of the dec-MDP framework. (b) Sequence of events (NB $b_t\equiv (t,\theta^c_t)$). \DM{To shift red line to $t=3$?} }
%     \label{fig:atari-mario}
% \end{figure*}


% \JW{It could be better to give a title to each paragraph below for clarity.}\DM{Done}

We now describe the details of the LIGS framework and how it learns intrinsic rewards that improve learning and team performance. We then describe the agents' objectives and their learning processes.  

To tackle the challenges described above, we introduce {\fontfamily{cmss}\selectfont Generator}  an \textit{adaptive} agent with its own objective that determines the best intrinsic rewards to give to the agents at each state.  Using observations of the joint actions played by the $N$ agents, the goal of {\fontfamily{cmss}\selectfont Generator} is to construct intrinsic rewards (which the $N$ MARL learners cannot generate themselves) to coordinate exploration and guide the agents towards learning joint policies that maximise their shared rewards. 
% Therefore the problem that {\fontfamily{cmss}\selectfont Generator} solves is distinct (but complementary) from that tackled by the agents.
% 
% 
% 
% 
% 
% 
To do this, {\fontfamily{cmss}\selectfont Generator} learns how to choose the values of an intrinsic reward function $F^{\boldsymbol{\theta}}$ at each state. 
% by choosing a particular $\boldsymbol{\theta}$.
Simultaneously, the $N$ agents perform actions to maximise their rewards using their individual policies.  The objective for each agent $i\in\{1,\ldots,N\}$ is given by: 
\begin{smalleralign}
v^{\pi^i,\pi^{-i},g}(s)=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t\left(R+F^{\boldsymbol{\theta}}\right)\Big|s_0=s\right], \nonumber
% \label{RL_controller_objective}
\end{smalleralign}
where 
% $r_i\sim R$ is the reward received by the agent from the environment and
$\boldsymbol{\theta}$ is determined by {\fontfamily{cmss}\selectfont Generator} using the policy $g:\mathcal{S}\times\Theta\to[0,1]$ and $\Theta\subset\mathbb{R}^p$ is {\fontfamily{cmss}\selectfont Generator}'s action set. The intrinsic reward function is given by $
    F^{\boldsymbol{\theta}}(\cdot)\equiv \phi(s_t,\theta^c_t)-\gamma^{-1}\phi(s_{t-1},\theta^c_{t-1})$ for any $s_t,s_{t-1}\in\mathcal{S}$
% 
% \begin{align}
% v^{f,\pi^i,\pi^{-i}}_i(s)=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t\left\{r_i+F(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1})\right\}\Big|s=s_0\right], \qquad \forall i \in\{1,\ldots N\},\nonumber
% % \label{RL_controller_objective}
% \end{align}
% \DM{F taking two actions}
where $\theta^c_t\sim g$  is the action chosen by {\fontfamily{cmss}\selectfont Generator} and $\theta^c_t\equiv 0, \forall t<0$. The function $\phi:\mathcal{S}\times\Theta\to \mathbb{R}$ is a continuous map that satisfies the condition $\phi(s,0)\equiv 0$, $\forall s\in\mathcal{S}$ (for example, $\phi$ can be  a neural network with fixed weights with input $(s,\theta^c)$ and $\Theta$ can be a set of integers $\{1,\ldots,K\}$).
Therefore, {\fontfamily{cmss}\selectfont Generator} determines the output of $F^{\boldsymbol{\theta}}$ (which it does through its choice of $\theta^c$). With this, {\fontfamily{cmss}\selectfont Generator} constructs intrinsic rewards that are tailored for the specific setting. 

 As the agent $i\in\mathcal{N}$ policy can be learned using any MARL method, LIGS freely adopts any MARL algorithm for the $N$ agents (see Sec. \ref{sec:plug_n_play} in the Supp. Material).  The transition probability $P:\mathcal{S}\times\boldsymbol{\mathcal{A}}\times\mathcal{S}\to[0,1]$ takes the state and \textit{only} the actions of the $N$ agents as inputs.
%  \footnote{The function $F$ can be easily augmented to include a time component. Note that since {\fontfamily{cmss}\selectfont Generator}
%  does not affect the transition dynamics, this does not destroy the Markov property.} 
 Note that unlike reward-shaping methods e.g. \cite{ng1999policy}, the function $\phi$ now contains an action term $\theta^c$ which is chosen by {\fontfamily{cmss}\selectfont Generator} which enables the intrinsic reward function to be learned online. The presence of the action $\theta^c$ term may spoil the policy invariance result in \cite{ng1999policy}. We however prove a policy invariance result (Prop. \ref{invariance_prop}) analogous to that in \cite{ng1999policy} which shows LIGS preserves the optimal policy of $\mathfrak{M}$.
% 
% 
% 
% 
% 
% This avoids inserting hand-designed exploration heuristics into the $N$ agents' objective as in curiosity-based methods \cite{burda2018exploration,pathak2017curiosity} and classical RS \cite{ng1999policy,zhang2019bilevel}. Additionally, as we later show, though in our setup {\fontfamily{cmss}\selectfont Generator} modifies the $N$ agents' reward, the framework preserves the optimal policy of $\mathfrak{M}$.     
%  We can encode some predefined terminating criterion to make the problem easier (see the \textbf{Options} paragraph for an explanation of the termination rule) --- for example the option can terminate whenever a `dissimilar' state is observed or after a number of steps. 
% 
% 
% 
% \JW{Start an NEw paragraph for introducing the details.}
% 
% 
% % \footnote{\yaodong{the definition on F has four inputs but later has two, maybe introduve b here?}\DM{we're just defining the map here. not sure if stating the notation here is the best place for that}}
% Lastly, the function $\hat{R}_c:\mathcal{S}\times\mathcal{A}\times\Theta\to\mathbb{R}$ is the one-step reward for {\fontfamily{cmss}\selectfont Generator}. 
% \yaodong{explain the implications here,e..g, LIGS can be easily adopted by any existing RL frmawork/algorthsm, which echos to your algo-agnostic in the abstract}\DM{agree this will help, will defer it to the end of the paragraph so as not to interrupt the formalism flow}).
% \JW{It is better to give a reason or reference, otherwise some reviewer may not be convinced.} \DM{This is true by construction.} % \textbf{The {\fontfamily{cmss}\selectfont Collect} Objective}
    % 
% Unlike CT-DE methods, we consider a framework in which the centralised learning component also has its own objective which is oriented around the agents' joint learning. 
% The function $F$ is activated by switches controlled by {\fontfamily{cmss}\selectfont Generator}.
% To encourage the controller to explore and, to supplement the intrinsic reward signal 
% \footnote{\yaodong{this needs a lot of elaborations, it seems to me that there can have many obj for LIGS, but the one we choose is not unique}\DM{yes, the objective is a design choice - lots of possibilities}}
% {\fontfamily{cmss}\selectfont Generator} activates streams of rewards to be supplied to the controller.
% In order to induce {\fontfamily{cmss}\selectfont Generator} to selectively choose when to switch on the $F$, each switch activation  incurs a fixed cost for {\fontfamily{cmss}\selectfont Generator}. The cost has two effects: first it reduces the complexity of {\fontfamily{cmss}\selectfont Generator} problem since its decision space is to determine which \textit{subregions} of $\mathcal{S}$ it should activate $F$. Second, it ensures that the \textit{information-gain} from {\fontfamily{cmss}\selectfont Generator} encouraging the $N$ agents to explore a given set of state-action tuple is sufficiently high to merit activating the stream of rewards.  
% 
\newline {\fontfamily{cmss}\selectfont Generator} is an RL agent whose objective takes into account the history of states and $N$ agents' joint actions. Define by $R^{\boldsymbol{\theta}}(s,\boldsymbol{a}):=R(s,\boldsymbol{a})+F^{\boldsymbol{\theta}}$ 
% and $R:=\sum_{i\in\mathcal{N}}r_i$, 
{\fontfamily{cmss}\selectfont Generator}'s objective is:
% Define by $r^{\boldsymbol{\theta}}_i:=r_i+F^{\boldsymbol{\theta}},R^{\boldsymbol{\theta}}:=\sum_{i\in\mathcal{N}}r^{\boldsymbol{\theta}}_i$ and $\boldsymbol{R}:=\sum_{i\in\mathcal{N}}r_i$, 
% \begin{align}
% v^{\boldsymbol{\pi},g}_0(s_0)  = \mathbb{E}_{\boldsymbol{\pi}}\left[ \sum_{t=0}^\infty \gamma^t\left(\boldsymbol{R}^{\boldsymbol{\theta}} +L(s_t,\boldsymbol{a}_t)\right)\right]-\mathbb{E}_{\boldsymbol{\pi^0}}\left[\sum_{t=0}^\infty \gamma^t\boldsymbol{R}\right],
% % \label{P2_obj}
% \end{align}
\begin{smalleralign}
v^{\boldsymbol{\pi},g}_c(s)  = \mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}}(s_t,\boldsymbol{a}_t) +L(s_t,\boldsymbol{a}_t)\right)\Big| s_0=s\right], \quad \forall s\in\mathcal{S}.
% \label{P2_obj}
\end{smalleralign}
% where $\boldsymbol{\pi^0}$ is the joint policy played in the dec-MDP without {\fontfamily{cmss}\selectfont Generator}.
The objective encodes {\fontfamily{cmss}\selectfont Generator}'s agenda, namely to maximise the agents' joint expected return. Therefore, using its intrinsic rewards,  {\fontfamily{cmss}\selectfont Generator} seeks to guide the set of agents toward optimal joint trajectories (potentially away from suboptimal trajectories, c.f. Experiment 2) and enables the agents to learn faster (c.f. StarCraft experiments in Sec. \ref{Section:Experiments}). Lastly, $L:\mathcal{S}\times\boldsymbol{\mathcal{A}}\to\mathbb{R}$ rewards {\fontfamily{cmss}\selectfont Generator} when the agents jointly visit novel state-joint-action tuples and tends to $0$ as the tuples are revisited. We later prove that with this objective, {\fontfamily{cmss}\selectfont Generator}'s optimal policy (for constructing the intrinsic rewards) maximises the expected team (extrinsic) return (Prop. \ref{preservation_lemma}).    

Since {\fontfamily{cmss}\selectfont Generator} has its own (distinct) objective, the resulting setup is an MG, $\mathcal{G}=\langle \mathcal{N}\times\{c\},\mathcal{S},(\mathcal{A}_i)_{i\in\mathcal{N}},\Theta,P,R^{\boldsymbol{\theta}},R_c,\gamma\rangle$ where the new elements are $\{c\}$,  {\fontfamily{cmss}\selectfont Generator} agent, $R^{\boldsymbol{\theta}}:=R+F^{\boldsymbol{\theta}}$, the new team reward function which contains the intrinsic reward $F^{\boldsymbol{\theta}}$,  $R_c:\mathcal{S}\times\boldsymbol{\mathcal{A}}\times\Theta\to\mathbb{R}$, the one-step reward for {\fontfamily{cmss}\selectfont Generator} (we give the details of this later). The transition probability $P:\mathcal{S}\times\boldsymbol{\mathcal{A}}\times\mathcal{S}\to[0,1]$ takes the state and \textit{only} the $N$ agents joint action as inputs.
% The objective encodes {\fontfamily{cmss}\selectfont Generator} agenda, namely to induce an improved performance by the $N$ agents.  


\textbf{Switching Control Mechanism}


So far {\fontfamily{cmss}\selectfont Generator}'s problem involves learning to construct intrinsic rewards at \textit{every} state which can be computationally expensive. We now introduce an important feature to the framework with which it only learns the best intrinsic reward in a subset of states in which intrinsic rewards are most useful. This is in contrast to the problem tackled by the $N$ agents who must compute their optimal actions at all states. 
% 
% 
% 
% 
% The difference $\mathbb{E}_{\boldsymbol{\pi}}[\sum_{t=0}^\infty\gamma^t\boldsymbol{R}^{\boldsymbol{\theta}}]-\mathbb{E}_{\boldsymbol{\pi^0}}[\sum_{t=0}^\infty\gamma^t \boldsymbol{R}]$ encodes {\fontfamily{cmss}\selectfont Generator} agenda, namely to induce an improved performance by the $N$ agents where $\boldsymbol{\pi^0}$ is the agents' joint policy when there is no {\fontfamily{cmss}\selectfont Generator}.  
% 
% 
% For this there are various possibilities; a model prediction error \cite{stadie2015incentivizing}, a count-based exploration bonus \cite{strehl2008analysis} are examples. Crucially this term encourages exploration of joint actions at a given state allowing the agents to find the optimal joint actions for coordinated play. 
% 
% 
% 
% 
% 
% 
% We now discuss the ability of {\fontfamily{cmss}\selectfont Generator} to choose which states to add intrinsic rewards.
 To achieve this, we now replace {\fontfamily{cmss}\selectfont Generator}'s policy space with a form of policies known as \textit{switching controls}. These policies enable {\fontfamily{cmss}\selectfont Generator} to decide at which states to learn the value of intrinsic rewards. This enables {\fontfamily{cmss}\selectfont Generator} to learn quickly both where to add intrinsic rewards and the magnitudes that improve performance since {\fontfamily{cmss}\selectfont Generator}'s magnitude optimisations are performed only at a subset of states. Crucially, with this {\fontfamily{cmss}\selectfont Generator} can learn its policy rapidly enabling it to guide the agents toward coordination and higher performing policies while they train.  

At each state {\fontfamily{cmss}\selectfont Generator} first makes a \textit{binary decision} to decide to \textit{switch on} its $F$ for agent $i\in\mathcal{N}$ using a switch $I_t$ which takes values in $\{0,1\}$. 
% This leads to a dec-MDP in which, unlike classical dec-MDPs, {\fontfamily{cmss}\selectfont Generator} now uses \textit{switching controls} to perform its actions.  
Crucially, now {\fontfamily{cmss}\selectfont Generator} is tasked with learning how to construct the $N$ agents' intrinsic rewards \emph{only} at states that are important for guiding the agents to their joint optimal policy. Both the decision to activate the function $F$ and its magnitudes is determined by {\fontfamily{cmss}\selectfont Generator}. With this, the agent $i\in\mathcal{N}$ objective becomes: 
\begin{smalleralign}
v^{\boldsymbol{\pi},g}(s_0,I_0)=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t\left\{R+F^{\boldsymbol{\theta}}\cdot I_t\right\}\right],\; \forall (s_0,I_0)\in\mathcal{S}\times\{0,1\},
\end{smalleralign} 
where $I_{\tau_{k+1}}=1-I_{\tau_{k}}$,  which is the switch for the function $F$ which is $0$ or $1$ and $\{\tau_k\}_{k> 0}$ are times that a switch takes place
\footnote{More precisely, $\{\tau_k\}_{k\geq 0}$ are \textit{stopping times}
\cite{oksendal2003stochastic}.} so for example if the switch is first turned on at the state $s_5$ then turned off at $s_7$, then $\tau_1=5$ and $\tau_2=7$ (we will shortly describe these in more detail). 
% Therefore by switching $I_t$ between $0$ or $1$, {\fontfamily{cmss}\selectfont Generator} decides to add the additional reward.
At any state, the decision to turn on $I$ is decided by a (categorical) policy $\mathfrak{g}_c:\mathcal{S} \to \{0,1\}$ which acts according to {\fontfamily{cmss}\selectfont Generator}'s objective. In particular, first, {\fontfamily{cmss}\selectfont Generator} makes an observation of the state $s_k\in\mathcal{S}$ and the joint action $\boldsymbol{a}_k$ and using $\mathfrak{g}_c$, {\fontfamily{cmss}\selectfont Generator} decides whether or not to activate the policy $g$ to provide an intrinsic reward whose value is determined by $\theta^c_k\sim g$. With this it can be seen the sequence of times $\{\tau_k\}$ is $\tau_k=\inf\{t>\tau_{k-1}|s_t\in\mathcal{S},\mathfrak{g}_c(s_t)=1\}$ so the switching times.
$\{\tau_k\}$ \textit{are \textbf{rules} that depend on the state.} Therefore, by learning an optimal $\mathfrak{g}_c$, {\fontfamily{cmss}\selectfont Generator} learns the useful states to switch on $F$. 

As we later describe, the termination times occur according to some external (probabilistic) rule. To induce {\fontfamily{cmss}\selectfont Generator} to selectively choose when to switch on the additional rewards, each switch activation  incurs a fixed cost for {\fontfamily{cmss}\selectfont Generator}. In this case, the objective for {\fontfamily{cmss}\selectfont Generator} is: 
\begin{smalleralign}
v^{\boldsymbol{\pi},g}_c(s_0,I_0)  = \mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}}(s_t,\boldsymbol{a}_t) +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
+L(s_t,\boldsymbol{a}_t)\right)\right], \label{generator_objective}
\end{smalleralign} 
where the new term $c:\{0,1\}^2\to \mathbb{R}_{<0}$ is a strictly negative cost function which imposes a cost for each switch activation and is modulated by the Kronecker-delta function $\delta^t_{\tau_{2k-1}}$ which is $1$ whenever $t={\tau_{2k-1}}$ and $0$ otherwise. The cost has two effects: first, it reduces the computational complexity of {\fontfamily{cmss}\selectfont Generator}'s problem since {\fontfamily{cmss}\selectfont Generator} now determines \textit{subregions} of $\mathcal{S}$ it should learn the values of $F$. Second, it ensures the \textit{information-gain} from encouraging the agents to explore a given set of state-action tuples is sufficiently high to merit activating a stream of intrinsic rewards. 
% 
% 
%  
% Recall that $\theta^c_k\sim g(\cdot|s_k)$ determines the reward signal added by {\fontfamily{cmss}\selectfont Generator}. 
% 
% 
% 
% The role of $\mathfrak{g}_c$ is therefore twofold: first, it makes the decision of whether or not the reward from {\fontfamily{cmss}\selectfont Generator} is added at a given state. Secondly, it decides which policy {\fontfamily{cmss}\selectfont Generator} uses to determine $\boldsymbol{R^\theta}$ (through $F$). 
% 
% 
We set $\tau_0\equiv 0$,  $\theta_{\tau_k}\equiv 0,\forall k\in\mathbb{N}$ ($\theta_{\tau_k+1},\ldots, \theta_{\tau_{k+1}-1}$ remain non-zero), $\theta^c_k\equiv 0\;\; \forall k\leq 0$ 
% 
% We define by $\hat{R}_1(s_t,I_t,a_t,\theta^2_t,a^2_{t-1}):=R(s_t,a_t)+\hat{F}(s_t,a^2_t;s_{t-1},a^2_{t-1})I_t$. 
and denote by $I(t)\equiv I_t$.
% 
% The set $\Pi^c$ can either consist of i) pre-fixed polices $\{\pi_1,\ldots,\pi_{|M|}\}$ which, when $F$ is activated at a state, {\fontfamily{cmss}\selectfont Generator} selects from (using $\mathfrak{g}_c$) to achieve the best reward ii) a set of policies that are trained to optimise $F$.  
% 
%  
%  If $\mathfrak{g}_c(s_k)=0$ then the switch is not activated ($I_{t=k}=0$). Alayer $i\in\mathcal{N}$ receives a reward $r\sim R(s_k,\boldsymbol{a}_k)$ and the system transitions to the next state $s_{k+1}$. If $\mathfrak{g}_c(s_k)=1$ then  {\fontfamily{cmss}\selectfont Generator} takes an action $a_k^c$ sampled from its policy $g$ and agent $i$ receives a reward $R(s_k,\boldsymbol{a}_k)+F(\theta^c_k,\theta^c_{k-1})\times 1$ and the system transitions to the next state $s_{k+1}$.
% 
%  
% \begin{algorithm}[H]
% \DontPrintSemicolon
% \KwInput{Initial {\fontfamily{cmss}\selectfont Controllers'} joint policy $\boldsymbol{\pi}_0=(\pi_{0,1},\ldots,\pi_{0,N})$,  {\fontfamily{cmss}\selectfont Generator} policies $\mathfrak{g}_{0_{0}}, \pi^2_0$, RL learning algorithm $\Delta$}
% \KwOutput{Optimised {\fontfamily{cmss}\selectfont Controller} policy $\pi^*$}
%     \For{$t = 1,T$}
%     {
% 	    Given environment state $s_t$, sample $a^i_t$ from $\pi^i(s_t)$ and obtain $s_{t+1},  r^i_{t+1}$ by applying $a_t$ to environment\;
	    
% 	    Evaluate $\mathfrak{g}_c(s_{t})$ according to Prop. \ref{prop:switching_times}\;
% 	   % Sample $\mathfrak{g}_c$ \textbf{// Switch is off (which occurs according to Equation \eqref{option_termination_criterion})}\; 
% 	   \eIf{$\mathfrak{g}_c(s_{t})$ = 1}
% 	   {
% 	        {\fontfamily{cmss}\selectfont Generator} samples an action $a^2_{t+1}\sim\pi^2(\cdot|s_{t+1
% 	    })$\;
% 	    {\fontfamily{cmss}\selectfont Generator} computes $r^i_{t+1} = \hat{F}(s_t, a^2_{t},  s_{t+1}, a^2_{t+1}) $, \;
% 	    Set intrinsic reward $r = r_{t+1} + r^i_{t+1}$\;
% 	   }
% 	   {
% 	        Set $r = r_{t+1}$\;
% 	   }
%     Update $\pi, \mathfrak{g}_c, \pi^2$ using $s_t, a_t, r, s_{t+1}$ and $\Delta$ \textbf{// Learn the individual policies}
%     }
% \caption{\textbf{S}elective \textbf{C}oordinating \textbf{A}daptive  \textbf{L}earning \textbf{A}lgorithm (LIGS)}
% \label{algo:Opt_reward_shap_psuedo} 
% \end{algorithm}
% 
% \noindent\textbf{{Summary of events:}}
% 
% At a time $k\in 0,1\ldots$ 
% 
%     \textbf{1.} Each agent $i\in\mathcal{N}$ makes an observation $z^i_k\in\mathcal{Z}^i$.
    
%     \textbf{2.} Each agent $i\in\mathcal{N}$ takes an action $a^i_k$ sampled from its policy $\pi^i$.
    % 
%     \textbf{3.} {\fontfamily{cmss}\selectfont Generator} makes an observation of the state $s_k\in\mathcal{S}$ and the joint action $\boldsymbol{a}_k=(a^1_k,\ldots,a^N_k)$.
    % 
%     \textbf{4.} {\fontfamily{cmss}\selectfont Generator} decides whether or not to activate a reward modification using $\mathfrak{g}_c:\mathcal{S}\times\boldsymbol{\mathcal{A}}  \to \{0\}\times M$: 
    % 
%     \textbf{5.} If $\mathfrak{g}_c(s_k)=0$: 
    
%     \textcolor{white}{X}$\circ$ The switch is not activated ($I_{t=k}=0$). agent $i\in\mathcal{N}$ receives a reward $r\sim R(s_k,\boldsymbol{a}_k)$ and the system transitions to the next state $s_{k+1}$.
        % 
%     \textbf{6.} If $\mathfrak{g}_c(s_k)=m$ for some $m\in  M$:
    
%     \textcolor{white}{X}$\circ$  {\fontfamily{cmss}\selectfont Generator} takes an action $a_k^c$ sampled from its policy $g$. 
    % 
%     \textcolor{white}{X}$\circ$ The switch is activated ($I_{t=k}=1$), agent $i$ receives a reward $R(s_k,\boldsymbol{a}_k)+F(\theta^c_k,\theta^c_{k-1})\times 1$ and the system transitions to the next state $s_{k+1}$.
% 
% We set $\tau_0\equiv 0$ and  $a_{\tau_k}^c\equiv 0,\forall k\in\mathbb{N}$ (note the terms $a_{\tau_k+1}^c,\ldots, a_{\tau_{k+1}-1}^c$ remain non-zero) and $\theta^c_k\equiv 0\;\; \forall k\leq 0$. 
% We define by $\hat{R}(s_t,I_t,a^i_t,\theta^c_t,\theta^c_{t-1}):=R(s_t,a^i_t)+F(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1})I_t$.and occasionally suppress the index $m$ on {\fontfamily{cmss}\selectfont Generator} policy $g$ and write $f$. 
% 
% 
%  and denote by $\left( \mathcal{V},\|\|\right)$ any finite normed vector space
% 
There are various possibilities for the \textit{termination} times $\{\tau_{2k}\}$ (recall that $\{\tau_{2k+1}\}$ are the times
% _{k\geq 1}
which the intrinsic reward $F$ is \textit{switched on) using $\mathfrak{g}_c$.} One is for the terminations to occur randomly. Another is to build a construction of $\{\tau_{2k}\}$ that directly incorporates the information gain that a state visit provides --- we defer the details of this arrangement to Sec. \ref{sec:termination_times} of the Appendix.

\noindent\textbf{{Summary of Events}}

At a time $t\in 0,1,\ldots$\\
\begin{tabular}{l|m{0.9\linewidth}}
 $1.$\; & The $N$ agents makes an observation of the state $s_t\in\mathcal{S}$.\\
    
    $2.$\; & The $N$ agents perform a joint action $\boldsymbol{a}_t=(a^1_t,\ldots,a^N_t)$ sampled from $\boldsymbol{\pi}=(\pi^1,\ldots,\pi^N)$.\\
    
    $3.$\; & {\fontfamily{cmss}\selectfont Generator} makes an observation of $s_t$ and $\boldsymbol{a}_t$ and draws samples from its polices $(\mathfrak{g}_c,g)$.\\
    
    $4.$\;
& If $\mathfrak{g}_c(s_t)=0$: \\
    
    & \textcolor{white}{X}$\circ$ Each agent $i\in\mathcal{N}$ receives a reward $r_i\sim R(s_t,\boldsymbol{a}_t)$ and the system transitions to the next state $s_{t+1}$ and steps 1 - 3 are repeated.\\
        
    $5.$\; & If $\mathfrak{g}_c(s_t)=1$:
    \\
        & \textcolor{white}{X}$\circ$
    $F^{\boldsymbol{\theta}}$ is computed using $s_t$, $\boldsymbol{a}_t$ and the {\fontfamily{cmss}\selectfont Generator} action $\theta^c\sim g$. 
    \\& \textcolor{white}{X}$\circ$ Each agent $i\in\mathcal{N}$ receives a reward $r_i+F^{\boldsymbol{\theta}}$ and the system transitions to $s_{t+1}$.\\
    
    $6.$\; & At time $t+1$ if the intrinsic reward terminates then steps 1 - 3 are repeated or if the intrinsic reward has not terminated then step 5 is repeated.\\
    
\end{tabular}


\subsection{The Learning Procedure}\label{sec:learning_proc}

% In the next section we provide the convergence properties of the algorithm, for now however we give a description of the algorithm (a full code description is in the Appendix). The algorithm consists of two independent procedures: {\fontfamily{cmss}\selectfont Generator} updates its own policy that determines states to perform a switch and $\boldsymbol{R^\theta}$ while the agents $\{1,\ldots,N\}$ learn their individual policies $\{\pi_1,\ldots,\pi_N\}$.  
% % 
% We adopted proximal policy optimisation (PPO) as the learning algorithm for the agents $\{1,\ldots,N\}$, the policy of {\fontfamily{cmss}\selectfont Generator} and the switching control policy. 
% % For the adaptive exploration module of {\fontfamily{cmss}\selectfont Generator} we used RND. 
% For {\fontfamily{cmss}\selectfont Generator} $L$ term we use $L(s_{t}, \boldsymbol{a}_t):=\|\hat{f} - \mathit{f}\|_{2}^{2}$\footnote{This is similar to random network distillation (RND) \cite{burda2018exploration} however now the input is drawn from the product space of action and state spaces.} where $\hat{f}$ is a random initialised network which is the target network which is fixed and $\mathit{f}$ is the \textit{prediction function} that is consecutively updated during training. We set the function $\phi$ to be a fixed feed forward NN that maps $\mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^m\to\mathbb{R}$ where $m\in\mathbb{N}$ is a tuneable parameter. For {\fontfamily{cmss}\selectfont Generator} action set we use \textcolor{red}{XX YY ZZ} and $g$ is a fixed feed forward NN that maps $\mathbb{R}^d \mapsto \mathbb{R}^m$.

In Sec. \ref{sec:convergence}, we provide the convergence properties of the algorithm, 
% for now we give a description of the algorithm (the full code is in Sec. \ref{sec:algorithm} of the Appendix). 
and give the full code of the algorithm in Sec. \ref{sec:algorithm} of the Appendix.  The algorithm consists of the following procedures: {\fontfamily{cmss}\selectfont Generator} updates its policy that determines the values $\theta$ at each state
and the states to perform a switch
 while the agents $\{1,\ldots,N\}$ learn their individual policies $\{\pi_1,\ldots,\pi_N\}$.
% 
In our implementation, we used proximal policy optimization (PPO) \cite{schulman2017Proximal} as the learning algorithm for both {\fontfamily{cmss}\selectfont Generator}'s intervention policy $\mathfrak{g}_c$ and {\fontfamily{cmss}\selectfont Generator}'s policy $g$. For the $N$ agents we used MAPPO \cite{yu2021surprising}.
% 
For {\fontfamily{cmss}\selectfont Generator} $L$ term we use\footnote{This is similar to random network distillation \cite{burda2018exploration} however the input is over the space $\mathcal{A}\times\mathcal{S}$.} $L(s_{t}, \boldsymbol{a}_t):=\|\hat{h} - \mathit{h}\|_{2}^{2}$  where $\hat{h}$ is a random initialised network which is the target network which is fixed and $\mathit{h}$ is the \textit{prediction function} that is consecutively updated during training. We constructed $\hat{F}$ using a fixed neural network $f:\mathbb{R}^d \mapsto \mathbb{R}^m$ and a one-hot encoding of the action of {\fontfamily{cmss}\selectfont Generator}. Specifically, $\phi(s_t, \theta^c_t) \coloneqq f(s_t) \cdot i(\theta^c_t)$ where $i(\theta^c_t)$ is a one-hot encoding of the action $\theta^c_t$ picked by {\fontfamily{cmss}\selectfont Generator}. Thus, $\hat{F}(s_t, \theta^c_t; s_{t-1}, \theta^c_{t-1}) = f(s_t) \cdot i(\theta^c_t)  - \gamma^{-1} f(s_{t-1}) \cdot i(\theta^c_{t-1})$. The action set of {\fontfamily{cmss}\selectfont Generator} is $\Theta \coloneqq \{0, 1,..., m\}$ where each element is drawn from $\mathbb{N}$, and $g$ is an MLP $g: \mathbb{R}^d    \mapsto \mathbb{R}^m$. Extra details are Sec. \ref{sec:algorithm} of the Appendix.

% \begin{algorithm}[H]
% \DontPrintSemicolon
% \KwInput{Initial agent policies $\pi^1_0,\ldots,\pi^N_0$,  {\fontfamily{cmss}\selectfont Generator} policies $\mathfrak{g}_{c_{0}}, g_0$, RL learning algorithm $\Delta$}
% \KwOutput{Optimised agent joint policy $\boldsymbol{\pi^\star}$}
%     \For{$t = 1,T$}
%     {
% 	    Given environment state $s_t$, sample $\boldsymbol{a}_t$ from $\boldsymbol{\pi}(\cdot|s_t)$ and obtain $s_{t+1},  \boldsymbol{r}_{t+1}:=\sum_{i\in\mathcal{N}}r_{i,t+1}$ by applying $\boldsymbol{a}_t$ to environment\;
	    
% 	    Evaluate $\mathfrak{g}_c(\cdot|s_t)$ according to Prop. \ref{prop:switching_times}\;
% 	   % Sample $\mathfrak{g}_2$ \textbf{// Switch is off (which occurs according to Equation \eqref{option_termination_criterion})}\; 
% 	   \eIf{$\mathfrak{g}_2(s_{t})$ = 1}
% 	   {
% 	        {\fontfamily{cmss}\selectfont Generator} samples an action $\theta^c_{t+1}\sim g(\cdot|s_{t+1
% 	    })$\;
% 	    {\fontfamily{cmss}\selectfont Generator} computes $f^i_{t+1} = \hat{F}(s_t, \theta^c_t,  s_{t+1}, \theta^c_{t+1}) $, \;
% 	    Set agent $i$ reward with intrinsic reward $r_i = r_{i,t+1} + f^i_{t+1}$\;
% 	   }
% 	   {
% 	        Set $r_i = r_{i,t+1}$\;
% 	   }
%     Update $(\pi^i)_{i\in\mathcal{N}}$, $(\mathfrak{g}_c, g)$ using $(s_t, \boldsymbol{a}_t, r_i, s_{t+1})$ and $(s_t, \boldsymbol{a}_t, (r_i)_{i\in\mathcal{N}}, s_{t+1})$ resp. and $\Delta$ \textbf{// Learn the individual policies}
%     }
% \caption{\textbf{L}earnable \textbf{I}ntrinsic-reward \textbf{G}eneration \textbf{S}election Algorithm (\textbf{LIGS})}
% \label{algo:Opt_reward_shap_psuedo} 
% \end{algorithm}

\section{Convergence and Optimality of LIGS} \label{sec:convergence}

% LIGS enables {\fontfamily{cmss}\selectfont Generator} to learn an intrinsic-reward function with which the agents can jointly coordinate and learn their optimal joint policy. 
% The framework aims at enabling {\fontfamily{cmss}\selectfont Generator} to
% learn an optimal reward function with which the $N$ agents then learn the optimal joint policy $\boldsymbol{\hat{\pi}}\in\boldsymbol{\Pi}$ for the task set by the environment. 
We now show that LIGS converges and that the solution ensures a higher performing agent policies than what would be achieved by solving $\mathfrak{M}$ directly.
% 
% During training {\fontfamily{cmss}\selectfont Generator} modifies the rewards of the $N$ agents. As the agents' rewards are changing according to the learning process for {\fontfamily{cmss}\selectfont Generator} this can occasion convergence issues. Moreover, since {\fontfamily{cmss}\selectfont Generator} has an objective that is different from all other agents the framework no longer adheres to a fully cooperative (team) setting --- the problem becomes an non-cooperative MG with $N+1$ independent agents. Formally, our MG is defined by a tuple $\mathcal{G}=\langle \mathcal{N}\times\{0\},\mathcal{S},\mathcal{A},\Theta,P,R^\theta,R_c,\gamma\rangle$ where the new elements are the set of agents $\mathcal{N}\times\{0\}$, % \Taher{Do we need to keep the curly N? Can we just have the other elements of the tuple? We make clear that our dec-MDP is 2 agents prevously}  
%  $R^\theta=r_i+F$. 
%  We now prove the convergence of our learning method, LIGS to the solution. 
%  In what follows, we show that this corresponds to learn and the $N$ agents learn their optimal joint policy for the task. 
% 
% 
% 
% \footnote{Since {\fontfamily{cmss}\selectfont Generator} policy has state dependency, it is easy to see that a state input of $F$ is not beneficial.}$^{,}$
% 
% %  To do this, we first study the stable point solutions of $\mathcal{G}$.  
% 
% In \textit{Non-cooperative} MGs, the solution concept is a fixed point known as a \textit{Nash equilibrium} (NE) \cite{fudenberg1991tirole}. Unlike MDPs and dec-MDPs, the existence of a solution in Markov policies is not guaranteed for (Non-cooperative) MGs \cite{blackwell1968big} and is rarely computable (except for special cases such as \textit{team} and \textit{zero-sum} MGs \cite{shoham2008multiagent}).\footnote{In \textit{zero-sum} MGs their rewards sum to zero \cite{von2002computing}.}
% % \footnote{Some dec-MDPs have no solution of Markov policies \cite{blackwell1968big}.} 
% MGs also often have multiple NE that can be inefficient \cite{mguni2019coordinating}; in $\mathcal{G}$ the outcome of such NE profiles would be a poor performing agent $\{1,\ldots,N\}$ policies. 
The addition of {\fontfamily{cmss}\selectfont Generator}'s RL process which modifies $N$ agents' rewards during learning can produce convergence issues \cite{zinkevich2006cyclic}.  Also to ensure the framework is useful, we must verify that the solution of $\mathcal{G}$ corresponds to solving the MDP, $\mathfrak{M}$. 
% 
% 
To resolve these issues, we first study the stable point solutions of $\mathcal{G}$.  Unlike MDPs, the existence of a solution in Markov policies is not guaranteed for MGs \cite{blackwell1968big} and is rarely computable (except for special cases such as \textit{team} and \textit{zero-sum} MGs \cite{shoham2008multiagent}).
% \footnote{In \textit{team} MGs all agents maximise the same objective; in \textit{zero-sum} MGs their rewards sum to zero \cite{von2002computing}. }
% \footnote{Some MGs have no solution of Markov policies \cite{blackwell1968big}.} 
MGs also often have multiple stable points that can be inefficient \cite{mguni2019coordinating}; in $\mathcal{G}$ such stable points would lead to a poor performing agent joint policy.
% 
We resolve these challenges with the following scheme: 

\noindent\textbf{[I]} LIGS preserves the optimal solution of $\mathfrak{M}$.\newline 
\noindent\textbf{[II]}  The MG induced by LIGS has a stable point which is the convergence point of MARL.\newline 
\noindent\textbf{[III]} LIGS yields a team payoff that is (weakly) greater than that from solving $\mathfrak{M}$ directly.\newline 
% \noindent\textbf{[IV]}  $\mathcal{G}$ has a stable point in Markov policies and is the convergence point of LIGS.\newline 
\noindent\textbf{[IV]}  LIGS  converges to the solution with a linear function approximators.
% We then perform a study of Algorithm \ref{algo:Opt_reward_shap_psuedo} and show that the algorithm provably converges to the NE of $\mathcal{G}$ in polynomial time. We defer the proofs of the results in this section to the Appendix.

% % The Nash equilibrium concept we seek is in strategies, that is when each agent executes its best-response policy. However, associated to each policy $\alpha, (\beta)$ is some behavioural (stochastic) policy $\pi_1, (\pi_c)$. In this sense, the subtraction in \eqref{P2_obj} takes the form:
% $
% % \mathbb{E}_{\mathbb{P},\alpha(f),f}\left[\sum_{t=0}^\infty \gamma^t\hat{R}(s_t,a_t,t,\theta^c_t,t-1,\theta^c_{t-1})-\sum_{k\geq 0}^\infty \gamma^tc(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}+\gamma^tE(s_t)\right]
% % -\mathbb{E}_{\mathbb{P}}\left[\sum_{t=0}^\infty \gamma^tR(s_t,a_t)\right]$.
% % \yali{curious about Eq. 3, how to define or parameterize it?}
% % 
% % 
% Note that the objective is constructive. 

% % , we can deduce that
% % \begin{align}\nonumber
% % &\mathbb{E}_{\mathbb{P},\alpha(f)}\left[\sum_{t=0}^\infty \gamma^t\left\{\hat{R}(s_t,a_t,t,\theta^c_t,t-1,\theta^c_{t-1})\right\}\right]
% % -\mathbb{E}_{\mathbb{P}}\left[\sum_{t=0}^\infty \gamma^tR(s_t,a_t)\right]
% % \\&=\sum_{k=1}^\infty\left\{\mathbb{E}_{\mathbb{P},\pi^{1,k-1}}\left[\sum_{t=0}^\infty \gamma^t\hat{R}(s_t,a_t,t,\theta^c_t,t-1,\theta^c_{t-1})\left(\frac{d(\alpha_k(\pi^{2,k}), \mathbb{P})}{d(\alpha_{k-1}(\pi^{2,k-1}), \mathbb{P})}-1\right)\right]\right\}
% % \\&\equiv\sum_{k=1}^\infty\left\{\mathbb{E}_{\mathbb{P},\pi^{1,k-1}}\left[\sum_{t=0}^\infty \gamma^t\hat{R}(s_t,a_t,t,\theta^c_t,t-1,\theta^c_{t-1})\left({\rm KL}\left(\pi^{1,k}\|\pi^{1,k-1}\right)-1\right)\right]\right\}\label{p2_obj_radon_nik}
% % \end{align}
% 
% \textit{Non-cooperative Markov Games (MGs)}

% When the agents do not share the same objective, the dec-MDP framework no longer applies. In this case, the setup becomes a Markov game $\mathcal{G}=\langle \mathcal{N},\mathcal{S},\left(\mathcal{A}_{i}\right)_{i\in\mathcal{N}},P,\left(r_i\right)_{i\in\mathcal{N}},\gamma\rangle$. 
% % where there exists at least one pair $i,j\in\mathcal{N}$ such that $r_i\neq R_j$ for some $i\neq j\in\mathcal{N}$. 
% In this instance, the objective of each agent $i\in\mathcal{N}$ is given by $v^{\pi^i,\pi^{-i}}_i(s)=\mathbb{E}_{\pi^i,\pi^{-i}}[\sum_{t=0}^\infty \gamma^tr_i(s_t,\boldsymbol{a}_t)]$. In this setting, the set of stable points are now described by a Markov perfect equilibrium which we now define: % \textcolor{gray}{We begin by recalling that a \textit{Markov policy} is a policy $\pi^i: \mathcal{S} \times \mathcal{A}_i \rightarrow [0,1]$ which requires as input only the current system state (and not the game history or the other agent's action or policy \cite{mguni2018viscosity}). With this, we give a formal description of the NE of $\mathcal{G}$ in Markov strategies.} 
% \begin{definition}
% A policy profile $\boldsymbol{\hat{\pi}}=(\hat{\pi}^i,\hat{\pi}^{-i})\in\boldsymbol{\Pi}$ is a Markov perfect equilibrium
% % \footnote{Recall that a \textit{Markov policy} is a policy $\pi^i: \mathcal{S} \times \mathcal{A}_i \rightarrow [0,1]$ which requires as input only the current system state (and not the game history or the other agent's action or policy \cite{mguni2018viscosity}). With this, we give a formal description of the NE of $\mathcal{G}$ in Markov strategies}
% if $\forall i\in\mathcal{N}, \;\forall \hat{\pi}'^i\in\Pi_i$: $
% v_i^{(\hat{\pi}^i,\hat{\pi}^{-i})}(s)\geq v_i^{(\hat{\pi}'^i,\hat{\pi}^{-i})}(s),\forall S$.
% \end{definition}
% The MPE is the corresponding NE solution concept for MGs. It describes a configuration in policies in which no agent can increase their payoff by changing (unilaterally) their policy. Crucially, it defines the stable points to which independent learners converge (if they converge at all). 
% 
In what follows, we denote by $\boldsymbol{\Pi}:=\times_{i\in\mathcal{N}}\Pi_i$. The results are built under Assumptions 1 - 7 (Sec. \ref{sec:notation_appendix} of the Appendix) which are standard in RL and stochastic approximation theory.
% 
We now prove the result \textbf{[I]} which shows the solution to $\mathfrak{M}$ is preserved under the influence of LIGS: 

\begin{proposition}
\label{preservation_lemma} The following statements hold:
% \begin{itemize}
    % \item [i)] 
% \hspace{-8 mm}\item [ii)] $
% \underset{n\to \infty}{\lim}\; v^{\boldsymbol{\pi},g}_c(s)  = \mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\boldsymbol{R} \right],
% \;\;\forall s\in\mathcal{S}$, 
% \end{itemize}
\begin{itemize}
    \item [i)] $
\underset{\boldsymbol{\pi}\in\boldsymbol{\Pi}}{\max}\; v^{\boldsymbol{\pi},g}(s,\cdot)=\underset{\boldsymbol{\pi}\in\boldsymbol{\Pi}}{\max}\; v^{\boldsymbol{\pi}}(s),\;\forall s\in\mathcal{S}, \forall i \in\mathcal{N}, \forall g$ where $v^{\boldsymbol{\pi}}(s)=\mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{t=0}^\infty \gamma^tR\right]$.
\hspace{-8 mm}\item [ii)] The {\fontfamily{cmss}\selectfont Generator}'s optimal policy maximises $v^{\boldsymbol{\pi}}(s)=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^tR(s_t,\boldsymbol{a}_t)\right]$ for any
$s\in\mathcal{S}$. 
\end{itemize}

\end{proposition}
Result (i) says that the agents' problem is preserved under {\fontfamily{cmss}\selectfont Generator}'s influence. Moreover the agents' (expected) total return is that from the environment (extrinsic rewards). Result (ii) establishes that {\fontfamily{cmss}\selectfont Generator}'s optimal policy induces it to maximise the agents' joint (extrinsic) total return. The result is proven by a careful adaptation of the policy invariance result in \cite{ng1999policy} to our MARL switching control setting where the intrinsic-reward is not added at all states. % (but only at states for which the switch is on).

% Statement ii) says that asymptotically the {\fontfamily{cmss}\selectfont Generator}'s total return is unaffected by the presence of $L$ and the {\fontfamily{cmss}\selectfont Generator}'s total return is a measure of the total environment rewards received by the agents. 

Building on Prop. \ref{preservation_lemma}, we deduce the following result:
\begin{corollary}\label{invariance_prop}
% \JW{It is better to give the reason why you set this function class. Otherwise, I'm afraid it could be challenged by reviewers.}\DM{Agreed. I am thinking of postponing this specialisation to the theory section where it's used.}
LIGS preserves the dec-MDP played by the agents. In particular, let $(\boldsymbol{\hat{\pi}},g)$ be a stable point policy profile\footnote{By stable point profile we mean 
% a configuration in joint policies in which no agent can profitably deviate unilaterally from their policy, i.e. 
a Markov perfect equilibrium \cite{fudenberg1991tirole}.} of the MG induced by LIGS, $\mathcal{G}$ then $\boldsymbol{\hat{\pi}}$ is a solution to the dec-MDP, $\mathfrak{M}$. 
% with objective given by $
% v^{\pi}(s)=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^tR(s_t,a_t)|s_0\equiv s\right]$.
\end{corollary}
Therefore, the introduction of {\fontfamily{cmss}\selectfont Generator} does not alter the fundamentals of the problem. 
% The result adapts the policy invariance results in \cite{ng1999policy} to our multi-agent switching control framework where RS is performed only at selected states. 
% 
% 
% We now show that $\mathcal{G}$ belongs to a special class of MGs which we prove \textit{always} possess stable point in (deterministic) Markov policies. We later exploit the properties of games in this class to prove the convergence of our algorithms. 
% 
% 
% 
% We therefore have the following facts:
% \begin{enumerate}
%     \item The (global) optimal point is preserved since the shaping-reward is potential based.
%     \item Secondly, The game is a single controller game.
%     \item The game is ARAT
% \end{enumerate} 
% 
% 
% 
% % Next we study the fixed points of $\mathcal{G}$. 
% To this end, we introduce concepts that describe the structure of the game $\mathcal{G}$. 
% % Consider a dec-MDP $\mathcal{G}=\left\langle \mathcal{N}=\{1,2\},\mathcal{S},\left(\mathcal{A}_{i}\right)_{i\in\mathcal{N}},P,\left(R_{i}\right)_{i\in\mathcal{N}},\left(\gamma_{i}\right)_{i\in\mathcal{N}}\right\rangle$, then $\mathcal{G}$ is a dec-MDP with additive
% a dec-MDP has additive rewards and additive transitions (\textbf{ARAT}) \cite{raghavan1985stochastic} if the transition and agents' reward functions can be decoupled into separate functions of each agent's action so that $r_i(s,\boldsymbol{a})=\sum_{j\in\mathcal{N}}u^i_j(s,a^j)$ and $P(\cdot|s,\boldsymbol{a})=\sum_{j\in\mathcal{N}}P_j(\cdot|s,a^j)$, $\forall \boldsymbol{a}\in\boldsymbol{\mathcal{A}},\forall s\in\mathcal{S}$. 
%     %  
%      a dec-MDP is \textbf{single controller} (SC) \cite{shoham2008multiagent} if at most one agent influences the dynamics so that for a agent $i\in\mathcal{N}$ $ P(s,(a^i,a^{-i}),s') =P(s,(a'^i,a'^{-i}),s')$ if $a^i = a'^i$ $\forall s,s'\in\mathcal{S}, \forall (a^i,a^{-i}), (a'^i,a'^{-i}) \in \boldsymbol{\mathcal{A}}$.
% % 
% It can be immediately seen that $\mathcal{G}$ is ARAT with $u_c\equiv F$ and $P_c\equiv \boldsymbol{0}$. That $\mathcal{G}$ is single controller is manifest. 
% % Single-controller and ARAT dec-MDPs can be solved by linear programming \cite{shoham2008multiagent}. In the RL setting, we need to extend such results which we do in the following analysis.
% 
% 
Our next task is to prove the existence of a stable point of the MG induced by LIGS and show it is a limit point of a sequence of Bellman operations. 
% 
% To construct the Bellman operator, we first introduce the following object:
% 
% Given a value function $\{v_i\}_{i\in\mathcal{N}}$, the quantity $\mathcal{M}v_i$ measures the expected future stream of rewards for agent $i$ after an immediate switch minus the cost of switching.
% 
% 
% The interpretation of $\mathcal{M}$ is the following: suppose that agent 1 is using the policy $\pi$ and at time $\tau_k^-$ the system is at a state $s_{\tau_k^-}$ and {\fontfamily{cmss}\selectfont Generator} performs a switch to $z=I(\tau_{k+1})$. A cost of $c(\tau_k,z)$ is then incurred by {\fontfamily{cmss}\selectfont Generator} and the system switches from $I(\tau_{k})$ to $I(\tau_{k+1})$. Using $\pi$, agent 1 executes $a_{\tau_k}\sim \pi(\cdot|s_{\tau_k})$ while {\fontfamily{cmss}\selectfont Generator} executes $\theta^c_{\tau_k}\sim  f(\cdot|s_{\tau_k})$. Lastly, recall $v_c^{(\pi,f)}$ is {\fontfamily{cmss}\selectfont Generator} value function under the policies $(\pi,f)$, then $\mathcal{M}^{\boldsymbol{\pi},f}v_2^{(\pi,f)}$ describes {\fontfamily{cmss}\selectfont Generator} expected return following a switch.
% 
% Having defined the operator $\mathcal{M}$, we are now in a position to give the Bellman operator for $\mathcal{G}$. Let $\psi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ be a given function, then for any $ s_{\tau_k}\in\mathcal{S}$ and for any $\tau_k; k=0,1,\ldots$, we define the Bellman operator $T_\psi$ of $\mathcal{G}$ acting on a function $\Lambda:\mathcal{S}\times\mathbb{N}\to\mathbb{R}$ by:\\  $T_\psi \Lambda(s_{\tau_k},I(\tau_k)):=\max\Big\{\mathcal{M}^{\boldsymbol{\pi},f}\Lambda(s_{\tau_k},I(\tau_k)),\\\;\;  \psi(s_{\tau_k},a)+\gamma\underset{a\in\mathcal{A}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau_k})\Lambda(s',I(\tau_k))\Big\}$.
% 
% We now show that a solution to the game $\mathcal{G}$ i.e. an NE exists and that the class of ARAT SC games (to which $\mathcal{G}$ belongs) enjoy a special property that allow its NE to be obtained using dynamic programming:
% 
To do this we prove that a stable solution of $\mathcal{G}$ exists and that $\mathcal{G}$ has a special property that permits its stable point to be found using dynamic programming. We begin by defining key objects for the analysis:
% % \begin{theorem}\label{theorem:existence}
% % Let $V:\mathcal{S}\times\boldsymbol{\mathcal{A}}\to\mathbb{R}$, then $\mathfrak{g}$ possesses a stable point given by $\underset{k\to\infty}{\lim}T^kV^{\boldsymbol{\pi}}=\underset{{\boldsymbol{\hat{\pi}}}\in\boldsymbol{\Pi}}{\sup}V^{\boldsymbol{\hat{\pi}}}$
% % % \begin{align}
% % % \underset{k\to\infty}{\lim}T_\phi^kV^{\boldsymbol{\pi}}=\underset{{\boldsymbol{\hat{\pi}}}\in\boldsymbol{\Pi}}{\sup}V^{\boldsymbol{\hat{\pi}}},  \label{value_iteration_expression}
% % % \end{align}
% % %  and ${\boldsymbol{\pi^\star}}\in\underset{{\boldsymbol{\pi'}}\in\boldsymbol{\Pi}}{\arg\sup}\;F^{\boldsymbol{\pi'}}$.
% % where $T$ is the Bellman operator of $\mathcal{G}$.
% % $T_\psi \Lambda(s_{\tau_k}):= (\hat{\boldsymbol{R}}-L)(s_{\tau_k},\boldsymbol{a})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a},s_{\tau_k})\Lambda(s')$ for some function $\Lambda:\mathcal{S}\to\mathbb{R}$.
% \end{theorem}
% The result is proven by showing that $\mathcal{G}$ admits a \textit{dual team game representation} 
% $\mathcal{G}^\dagger=\left\langle \{1,2\},\mathcal{S},\mathcal{A},\mathcal{B},P,\phi,\gamma\right\rangle$ ($R_1=R_2=\phi$) 
% with stable points that correspond to the MPE of $\mathcal{G}$. We then prove $T$ is a contraction with a limit point that corresponds to the unique stable point of $\mathcal{G}$. 

Given a $V^{\boldsymbol{\pi},g}:\mathcal{S}\times\mathbb{N}\to\mathbb{R},\;\forall\boldsymbol{\pi}\in\boldsymbol{\Pi}$ and $g$, define by $
\mathcal{M}^{\boldsymbol{\pi},g}V^{\boldsymbol{\pi},g}(s_{\tau_k},I_{\tau_k}):=R(s_{\tau_k},\boldsymbol{a}_{\tau_k})+F^{(\theta_{\tau_k},\theta_{\tau_{k-1}})}+c(I_k,I_{k-1})+\gamma\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a}_{\tau_k},s)V^{\boldsymbol{\pi},g}(s',I(\tau_{k+1}))$, $\forall s_{\tau_k}\in\mathcal{S}$ where $\boldsymbol{a}_{\tau_k}\sim \boldsymbol{\pi}(\cdot|s_{\tau_k})$, $\theta_{\tau_k}\sim  g(\cdot|s_{\tau_k})$ and $\tau_k$ is a  {\fontfamily{cmss}\selectfont Generator} switching time.
% For any function  $\psi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$, 
We define the Bellman operator $T$ of $\mathcal{G}$ by $T V^{\boldsymbol{\pi},g}(s_{\tau_k},I_{\tau_k}):=\max\Big\{\mathcal{M}^{\boldsymbol{\pi},g}V^{\boldsymbol{\pi},g}(s_{\tau_k},I_{\tau_k}),  R(s_{\tau_k},\boldsymbol{a}_{\tau_k})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a},s_{\tau_k})V^{\boldsymbol{\pi},g}(s',I_{\tau_k})\Big\}$. 

The following result establishes that the solution of the MG $\mathcal{G}$, can be computed using RL methods:

\begin{theorem}\label{theorem:existence_2}
Let $V:\mathcal{S}\times\boldsymbol{\mathcal{A}}\to\mathbb{R}$,  then for any $s\in\mathcal{S}$ the game $\mathcal{G}$ has a stable point given by $\underset{k\to\infty}{\lim}T^kV^{\boldsymbol{\pi}}=\underset{{\boldsymbol{\hat{\pi}}}\in\boldsymbol{\Pi}}{\sup}V^{\boldsymbol{\hat{\pi}}}=V^{\boldsymbol{\pi^\star}}$ where $\boldsymbol{\pi^\star}\in\boldsymbol{\Pi}$ is a stable joint policy of $\mathcal{G}$.
\end{theorem}
Theorem \ref{theorem:existence_2} proves that the MG $\mathcal{G}$ (which is the game that is induced when {\fontfamily{cmss}\selectfont Generator} plays with the $N$ agents) has a stable point which is the limit of a dynamic programming method. In particular, it proves the that the stable point of $\mathcal{G}$ is the limit point of the sequence $T^1V,T^2V,\ldots,$. Crucially, (by Corollary \ref{invariance_prop}) the limit point corresponds to the solution of the dec-MDP $\mathcal{M}$.  Theorem \ref{theorem:existence_2} is proven by firstly proving that $\mathcal{G}$ has a dual representation as an MDP whose solution corresponds to the stable point of the MG.  
% 
% 
Theorem \ref{theorem:existence_2} enables us to tackle the problem of finding the solution to $\mathcal{G}$ using distributed learning methods i.e. MARL to solve $\mathcal{G}$.   
% With this, let $\phi:=\hat{\boldsymbol{R}}-L$ and define $Y_{l_k}(s_{l_k}, (\boldsymbol{a}_{l_k},\theta^c_{l_k}),  s'_{l_k})
% := \phi_{l_k}(s_{l_k},(\boldsymbol{a}_{l_k},\theta^c_{l_k}))+\gamma\underset{(\boldsymbol{a}',\theta'^{0})}{\max}\;\bar{v}^{\pi^i,\pi^{j}}_{l}(s'_{l_k},(\boldsymbol{a}',\theta'^c))$. 
% \begin{align}
% Y_{l_k}(s_{l_k}, &(\boldsymbol{a}_{l_k},\theta^c_{l_k}),  s'_{l_k})
% := \phi_{l_k}(s_{l_k},(\boldsymbol{a}_{l_k},\theta^c_{l_k}))+\gamma\underset{(\boldsymbol{a}',a'^{0})}{\sup}\;\bar{v}^{\pi^i,\pi^{j}}_{l}(s'_{l_k},(\boldsymbol{a}',a'^c)). \nonumber
% \end{align}
% 
% At each iteration $k=0,1,\ldots$ we solve the minimisation: 
% $% \underset{\mathcal{F}}{\inf} \sum_{l_k=1}^{n_k}\left(Y_{l_k}(s_{l_k},(\boldsymbol{a}_{l_k},\theta^c_{l_k}),s'_{l_k})-\left[\mathcal{F}\right](s_{l_k},(\boldsymbol{a}_{l_k},\theta^c_{l_k}))\right)^2.$
% The (fitted) Q-learning method is naturally suggested by Theorem \ref{theorem:existence_2} and finds the optimal policies of the game. 
% This is in stark contrast to Non-cooperative MGs whose MPE solutions must be computed using fixed point methods which are generally intractable \cite{chen2009settling}. 
Moreover, Prop. \ref{invariance_prop} indicates by computing the stable point of $\mathcal{G}$ leads to a solution of $\mathfrak{M}$. These results combined prove \textbf{[II]}.

% Theorem \ref{theorem:existence} also suggests a method of computing the MPE policies which is described in Algorithm \ref{algo:Opt_reward_shap_psuedo}. 
% 
% It therefore remains to establish whether the solutions to $\mathcal{G}$ produce better outcomes for the controller. 
% 

Our next result characterises {\fontfamily{cmss}\selectfont Generator} policy $\mathfrak{g}_c$ and the optimal times to activate $F$. The result yields a key aspect of our algorithm for executing  {\fontfamily{cmss}\selectfont Generator} activations of intrinsic rewards:
\begin{proposition}\label{prop:switching_times}
The policy $\mathfrak{g}_c$ is given by the following: $\mathfrak{g}_c(s_t,I_t)=H(\mathcal{M}^{\boldsymbol{\pi},g}V^{\boldsymbol{\pi},g}- V^{\boldsymbol{\pi},g})(s_t,I_t),\;\;\forall (s_t,I_t)\in\mathcal{S}\times\{0,1\}$, where $V^{\boldsymbol{\pi},g}$ is the solution in Theorem \ref{theorem:existence_2} and $H$ is the Heaviside function, moreover {\fontfamily{cmss}\selectfont Generator}'s switching times 
% $\{\tau_k\}_{k\geq 1}$ 
are $\tau_k=\inf\{\tau>\tau_{k-1}|\mathcal{M}^{\boldsymbol{\pi},g}V^{\boldsymbol{\pi},g}= V^{\boldsymbol{\pi},g}\}$. 
 

% moreover, the set $A$ takes the form $A=\{s\in \mathcal{S},m\in M|\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\geq \psi(s_{t},I(\tau))\}.$
\end{proposition}

% First, we introduce the following concept: 

% \begin{definition}
% We say that the controller is \textbf{better-off} if its .  
% \end{definition}

In general, introducing intrinsic rewards or shaping rewards may undermine learning and worsen overall performance. We now prove that the LIGS framework introduces an intrinsic reward which yields better performance for the $N$ agents as compared to solving $\mathfrak{M}$ directly (\textbf{[III]}).  

% The following result establishes that the set of is better off under the influence of LIGS: 
\begin{theorem}\label{NE_improve_prop}
% Any NE policy profile of $\mathcal{G}$ is one in which agent 1 is better-off, that is 
Each agent's expected return $v^{\boldsymbol{\pi},g}$ whilst playing $\mathcal{G}$ is (weakly) higher than the expected return for $\mathfrak{M}$ (without {\fontfamily{cmss}\selectfont Generator}) i.e. $v^{\boldsymbol{\pi},g}(s,\cdot)\geq v^{\boldsymbol{\pi}}(s),\;\forall s \in\mathcal{S},\;\forall i \in\mathcal{N}$. 
\end{theorem}

Theorem \ref{NE_improve_prop} shows that {\fontfamily{cmss}\selectfont Generator}'s influence leads to an improvement in the system performance for the $N$ agents. Note that by Prop. \ref{preservation_lemma}, Theorem \ref{NE_improve_prop} compares the environment (extrinsic) rewards accrued by the agents so that the presence of {\fontfamily{cmss}\selectfont Generator} increases the total expected environment rewards.
% Unlike RS methods in general, the NE generated \textit{never} lead to a reduction the payoff for agent 1 as compared to its payoff without $F$.
% 
% \section{Convergence}
% 
% Computing the solution to dec-MDPs is in general, notoriously difficult and is known to be NP-hard \cite{conitzer2002complexity}.
% Theorem \ref{theorem:existence} establishes the existence of a stable point solution of the dec-MDP. 
% We have however yet to establish a convergence guarantee of LIGS. 
% 
% 
% 
% We now extend our analysis to cover the game in which {\fontfamily{cmss}\selectfont Generator} uses switching controls to select when to alter the environment reward. We now show that a stable solution of $\mathcal{G}$ in which {\fontfamily{cmss}\selectfont Generator} uses switching controls exists and is the convergence point of a dynamic programming method:
% 
% \textcolor{gray}{In this setting, as before {\fontfamily{cmss}\selectfont Generator} has an objective that is different from all other agents so that the setting is non-cooperative MG $\mathcal{G}=\langle \mathcal{N}\times\{0\},\mathcal{S},\mathcal{A},\Theta,Z,O,P,R^\theta,R_{0,c},\gamma\rangle$ and, given the {\fontfamily{cmss}\selectfont Generator} objective in \eqref{coordinator_switch_obj}, we see that the {\fontfamily{cmss}\selectfont Generator} reward $R_{0,c}$ is given by $R_{0,c}=\boldsymbol{R}^{\boldsymbol{\theta}} +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
% +L$. }
% 
% 
% To begin, we introduce the \textit{intervention operator} which is defined by the following: \begin{definition}Let $\boldsymbol{\pi}\in\boldsymbol{\Pi}$ and $f\in\Pi^c$ be a joint policy for agents $1,\ldots,N$ and a {\fontfamily{cmss}\selectfont Generator} policy respectively, then for any $ s_{\tau_k}\in\mathcal{S}$ and for any $\tau_k$, we define the intervention operator $\mathcal{M}^{\boldsymbol{\pi},f}$ acting on a function $\Lambda:\mathcal{S}\times\mathbb{N}\to\mathbb{R}$ by the following: $
% \mathcal{M}^{\boldsymbol{\pi},f}\Lambda(s_{\tau_k},I(\tau_k)):=\hat{R}(z_{\tau_k},\boldsymbol{a}_{\tau_k},\theta^c_{\tau_k},\cdot)+c(I_k,I_{k-1})+\gamma\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a}_{\tau_k},s)\Lambda(s',I(\tau_{k+1}))$ where $\boldsymbol{a}_{\tau_k}\sim \boldsymbol{\pi}(\cdot|s_{\tau_k})$ with $\theta^c_{\tau_k}\sim  g(\cdot|s_{\tau_k})$.
% \end{definition}
% Given a value function $\{v_i\}_{i\in\mathcal{N}}$, the quantity $\mathcal{M}v_i$ measures the expected future stream of rewards for agent $i$ after an immediate switch minus the cost of switching.
% 
% 
% The interpretation of $\mathcal{M}$ is the following: suppose that agent 1 is using the policy $\pi$ and at time $\tau_k^-$ the system is at a state $s_{\tau_k^-}$ and {\fontfamily{cmss}\selectfont Generator} performs a switch to $z=I(\tau_{k+1})$. A cost of $c(\tau_k,z)$ is then incurred by {\fontfamily{cmss}\selectfont Generator} and the system switches from $I(\tau_{k})$ to $I(\tau_{k+1})$. Using $\pi$, agent 1 executes $a_{\tau_k}\sim \pi(\cdot|s_{\tau_k})$ while {\fontfamily{cmss}\selectfont Generator} executes $\theta^c_{\tau_k}\sim  f(\cdot|s_{\tau_k})$. Lastly, recall $v_2^{(\pi,f)}$ is {\fontfamily{cmss}\selectfont Generator} value function under the policies $(\pi,f)$, then $\mathcal{M}^{\boldsymbol{\pi},f}v_2^{(\pi,f)}$ describes {\fontfamily{cmss}\selectfont Generator} expected return following a switch.
% 
% Having defined the operator $\mathcal{M}$, we are now in a position to give the Bellman operator for $\mathcal{G}$. Let $\psi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ be a given function, then for any $ s_{\tau_k}\in\mathcal{S}$ and for any $\tau_k; k=0,1,\ldots$, we define the Bellman operator $T_\psi$ of $\mathcal{G}$ acting on a function $\Lambda:\mathcal{S}\times\mathbb{N}\to\mathbb{R}$ by:\\  $T_\psi \Lambda(s_{\tau_k},I(\tau_k)):=\max\Big\{\mathcal{M}^{\boldsymbol{\pi},f}\Lambda(s_{\tau_k},I(\tau_k)),\\\;\;  \psi(s_{\tau_k},a)+\gamma\underset{a\in\mathcal{A}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau_k})\Lambda(s',I(\tau_k))\Big\}$.
% 
% We now show that a solution to the game $\mathcal{G}$ i.e. an NE exists and that the class of ARAT SC games (to which $\mathcal{G}$ belongs) enjoy a special property that allow its NE to be obtained using dynamic programming:
% 
% \begin{theorem}\label{theorem:existence_2}
% Let $V:\mathcal{S}\times\boldsymbol{\mathcal{A}}\to\mathbb{R}$. Suppose now that {\fontfamily{cmss}\selectfont Generator} uses switching controls. Then $\mathfrak{g}_s$ has a stable point given by $\underset{k\to\infty}{\lim}T^kV^{\boldsymbol{\pi}}=\underset{{\boldsymbol{\hat{\pi}}}\in\boldsymbol{\Pi}}{\sup}V^{\boldsymbol{\hat{\pi}}}$ where $T$ is the Bellman operator of $\mathcal{G}$
% \begin{align}
% \underset{k\to\infty}{\lim}T_\phi^kV^{\boldsymbol{\pi}}=\underset{{\boldsymbol{\hat{\pi}}}\in\boldsymbol{\Pi}}{\sup}V^{\boldsymbol{\hat{\pi}}},  \label{value_iteration_expression}
% \end{align}
%  and ${\boldsymbol{\pi^\star}}\in\underset{{\boldsymbol{\pi'}}\in\boldsymbol{\Pi}}{\arg\sup}\;F^{\boldsymbol{\pi'}}$.
% 
% where $T$ is the Bellman operator of $\mathcal{G}$ acting on a function $\Lambda:\mathcal{S}\times\mathbb{N}\to\mathbb{R}$ defined by $T \Lambda(s_{\tau_k},I(\tau_k)):=\max\Big\{\mathcal{M}^{\boldsymbol{\pi},g}\Lambda(s_{\tau_k},I(\tau_k)),  (\hat{\boldsymbol{R}}-L)(s_{\tau_k},\boldsymbol{a})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a},s_{\tau_k})\Lambda(s',I(\tau_k))\Big\}$ and where the intervention operator $\mathcal{M}^{\boldsymbol{\pi},f}$ is defined by : $
% \mathcal{M}^{\boldsymbol{\pi},f}\Lambda(s_{\tau_k},I(\tau_k)):=\hat{R}(z_{\tau_k},\boldsymbol{a}_{\tau_k},\theta^c_{\tau_k},\cdot)+c(I_k,I_{k-1})+\gamma\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a}_{\tau_k},s)\Lambda(s',I(\tau_{k+1}))$ and $\boldsymbol{a}_{\tau_k}\sim \boldsymbol{\pi}(\cdot|s_{\tau_k})$ with $\theta^c_{\tau_k}\sim  g(\cdot|s_{\tau_k})$..
% 
% Theorem \ref{theorem:existence_2} establishes the solution to $\mathcal{G}$ can be computed using LIGS that is polynomial in its inputs. 
% This means that {\fontfamily{cmss}\selectfont Generator} converges an optimal reward function (that improves each agent's performance) and the $N$ agents learn their optimal value function for the task.
% 
% \textcolor{gray}{Having constructed a procedure to find the optimal joint policy $\boldsymbol{\hat{\pi}}\in \boldsymbol{\Pi}$, our next result characterises {\fontfamily{cmss}\selectfont Generator} policy $\mathfrak{g}_c$ and the times that {\fontfamily{cmss}\selectfont Generator} must activate the switch on $F$. 
% \begin{proposition}\label{prop:switching_times}
% The policy $g$ is given by the following expression $\forall (s_0,I_0)\in\mathcal{S}\times\{0,1\}$: $g_0(s_0,I_0)=H(\mathcal{M}^{\boldsymbol{\pi},g}V^{\boldsymbol{\pi},g}- V^{\boldsymbol{\pi},g})(s_0,I_0) $% \label{G_2_obstacle_expression}
% where $V$ is the solution in Theorem \ref{theorem:existence_2} and $H$ is the Heaviside function, moreover LIGS's switching times 
% % $\{\tau_k\}_{k\geq 1}$ 
% are given by $\tau_k=\inf\{\tau>\tau_{k-1}|\mathcal{M}^{\boldsymbol{\pi},g}V^{\boldsymbol{\pi},g}(\cdot)= V^{\boldsymbol{\pi},g}(\cdot)\}$. 
% % moreover, the set $A$ takes the form $A=\{s\in \mathcal{S},m\in M|\mathcal{M}^{\boldsymbol{\pi},f}\psi(s_{\tau},I(\tau))\geq \psi(s_{t},I(\tau))\}.$
% \end{proposition}
% Hence, Prop. \ref{prop:switching_times} also characterises the (categorical) distribution $\mathfrak{g}_c$. Moreover, given the function $v_2$, the times $\{\tau_k\}$ can be determined by evaluating if $\mathcal{M}V=V$ holds.} 
% 
We complete our analysis by extending Theorem \ref{theorem:existence_2} to capture (linear) function approximators which proves \textbf{[IV]}. We first define a \textit{projection} $\Pi$ by: $
\Pi \Lambda:=\underset{\bar{\Lambda}\in\{\Phi r|r\in\mathbb{R}^p\}}{\arg\min}\left\|\bar{\Lambda}-\Lambda\right\|$ for any function $\Lambda$.
% 
%     
% We are now in position to give the convergence result:
\begin{theorem}\label{primal_convergence_theorem}
LIGS converges to the stable point of  $\mathcal{G}$, 
moreover, given a set of linearly independent basis functions $\Phi=\{\phi_1,\ldots,\phi_p\}$ with $\phi_k\in L_2,\forall k$. LIGS converges to a limit point $r^\star\in\mathbb{R}^p$ which is the unique solution to  $\Pi \mathfrak{F} (\Phi r^\star)=\Phi r^\star$ where
    $\mathfrak{F}\Lambda:=\hat{R}+\gamma P \max\{\mathcal{M}\Lambda,\Lambda\}$ . Moreover, $r^\star$ satisfies: $
    \left\|\Phi r^\star - Q^\star\right\|\leq (1-\gamma^2)^{-1/2}\left\|\Pi Q^\star-Q^\star\right\|$.
\end{theorem}
The theorem establishes the convergence of LIGS to a stable point (of $\mathcal{G}$) with the use of linear function approximators. The second statement bounds the proximity of the convergence point by the smallest approximation error that can be achieved given the choice of basis functions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% EXPERIMENTS %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{Section:Experiments}
% \DM{TBA ablation study (where switches are activated. Discussion on what LIGS is doing in the experiments and how it relates to the proposed benefits of the framework}
We performed a series of experiments to test if LIGS: \textbf{1.} learns to efficiently coordinate the agents' joint exploration \textbf{2.} Optimises convergence points by inducing coordination. \textbf{3.} Handles sparse rewards environments. In all tasks, we compared the performance of LIGS against leading MARL solvers MAPPO \cite{yu2021surprising}, QMIX \cite{rashid2018qmix}; intrinsic reward MARL algorithms LIIR \cite{du2019liir}, LICA \cite{zhou2020learning} and a leading MARL exploration algorithm MAVEN \cite{mahajan2019maven}.
We then compared LIGS against these baselines in Cooperative Foraging Tasks \cite{papoudakis2020comparative} and StarCraft Micromanagement II \cite{samvelyan2019starcraft}.
% Lastly, we ran a detailed suite of ablation studies (Supp. material).

\subsection{Cooperative Foraging Tasks}\label{exp:foraging}



\begin{figure}
\centering
\includegraphics[scale=0.27]{Figures/LBF_c.png}\hspace{10 mm}
\includegraphics[scale=0.36]{Figures/subopt.png}\hspace{8 mm}
\includegraphics[scale=0.18]{Figures/sparse.png}\vspace{-0.5 mm}
\includegraphics[width=.32\linewidth]{Figures/lbf/jointexp.pdf}
\includegraphics[width=.32\linewidth]{Figures/lbf/subopt.pdf}
\includegraphics[width=.32\linewidth]{Figures/lbf/sparse.pdf}
\captionof{figure}{\textit{Left.} Coordinated Exploration. \textit{Centre}. Optimal joint policies. \textit{Right.} Sparse rewards.}
\vspace{-2mm}
\label{fig:foraging}
\end{figure}



% % \subsubsection{Experiment 1: Joint exploration} \label{exp1: joint}
% \begin{figure}[H]
%     \centering
%         \vspace{-4mm}
%     \begin{subfigure}{0.4\textwidth}
%     \hspace{-10 mm}\includegraphics[scale=0.35]{Figures/LBF_c.png} \hspace{-10 mm}
%     \includegraphics[scale=0.175]{Figures/coordinated.jpg}
%         \includegraphics[scale=0.175]{Figures/sparse.png}
%     % \includegraphics[scale=0.4]{Figures/LBF_aac.png}
%     \caption{TBA.}
%       \centering
%     % \vspace{-4mm}
%     \begin{tabular}{|c|c|c|} 
%   \hline
%   \textbf{Agent 1/Agent 2} & Top & Bottom \\ 
%   \hline
%   Top & $r/2,r/2$ & $r.-r$ \\ 
%   \hline
%   Bottom & $-r,r$ & $R,R$ \\ 
%   \hline
% \end{tabular}
%     % \includegraphics[scale=0.22]{Figures/coordinated.jpg}
%     \caption{TBA.}
%     \label{Figure:LearningCurves}
%     \vspace{-5mm}
%     \end{subfigure}
%         \label{Figure:LearningCurves}
%     \vspace{-5mm}
%     \begin{subfigure}{0.4\textwidth}
% \centering
%     \vspace{-4mm}
%     \includegraphics[scale=0.3]{Figures/lbf/jointexp.pdf}
%     \caption{Coordinated Exploration \DM{can we change the name to LIGS. Can we use capital letters for the baselines}}
%     \label{fig:joint_exploration}
%     \vspace{-5mm}
%         \centering
%     \includegraphics[scale=0.3]{Figures/lbf/sparse.pdf}
%     \caption{Sparse Rewards}
%     \label{fig:my_label}
%     \end{subfigure}
%     \vspace{-5mm}
% \end{figure}


% \begin{wrapfigure}{R}{0.6\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[scale=0.3]{Figures/lbf/joint_exp_v3.pdf}
%     \caption{Coordinated Exploration \DM{can we change the name to LIGS. Can we use capital letters for the baselines}}
%     \label{fig:joint_exploration}
%     \vspace{-5mm}
% \end{wrapfigure}
\begin{figure}
\centering
\includegraphics[scale=0.3]{Figures/additional_experiments/lbf_heatmap.pdf}\hspace{1 mm}
\includegraphics[scale=0.3]{Figures/additional_experiments/lbf_opp_dists.pdf}\hspace{1 mm}
\includegraphics[scale=0.25]{Figures/additional_experiments/heatmap.pdf}
\captionof{figure}{\textit{Left.} Heatmap of Exp. 1 showing where {\fontfamily{cmss}\selectfont Generator} adds rewards. \textit{Centre.} Plot of distance to other agent when {\fontfamily{cmss}\selectfont Generator} activates rewards in Exp 1. \textit{Right.} Corresponding heatmap for Exp. 2.}
\vspace{-1mm}
\label{fig:heatmap}
\end{figure}

\textbf{Experiment 1: Coordinated exploration.} We tested our first claim that LIGS promotes coordinated exploration among agents. To investigate this, we used a version of the level-based foraging environment \cite{papoudakis2020comparative} as follows: there are $n$ agents each with level $a_i$. Moreover, there are 3 apples with level $K$ such that $\sum_{i=1}^N a_i = K$. The only way to collect the reward is if all agents collectively enact the {\fontfamily{cmss}\selectfont collect} action when they are beside an apple. This is a challenging joint-exploration problem since to obtain the reward, the agents must collectively explore joint actions across the state space (rapidly) to discover that simultaneously executing {\fontfamily{cmss}\selectfont collect} near an apple produces rewards. To increase the difficulty, we added a penalty for the agents failing to coordinate in collecting the apples. For example, if only one agent uses the {\fontfamily{cmss}\selectfont collect} action near an apple, it gets a negative reward. This results in a non-monotonic reward structure. Fig. \ref{fig:foraging} shows the performance curves.
Fig, \ref{fig:foraging} shows LIGS demonstrates superior performance over the baselines.\newline 
% 
% 
\textbf{Experiment 2: Optimal joint policies. }
We next tested our second claim that LIGS can promote convergence to joint policies that achieve higher system rewards. To do this, we constructed a challenging experiment in which the agents must avoid converging to suboptimal policies that deliver positive but low rewards. In this experiment, the grid is divided horizontally in three sections; top, middle and bottom. All grid locations in the top section give a small reward $r/n$ to the agent visiting them where $n$ is the number of tiles in the each section. The middle section does not give any rewards. The bottom section rewards the agents depending on their relative positions. If one agent is at the top and the other at the bottom, the agent at the bottom receives a reward $-r/n$ each time the other agent receives a reward. If both agents are at the bottom, then one of the tiles in this section will give a reward $R, r/2<R<r$ to both agents. The bottom section gives no reward otherwise. The agents start in the middle section and as soon as they cross to one section they cannot return to the middle. As is shown in Fig. \ref{fig:foraging}, LIGS learns to acquire rewards rapidly in comparison to the baselines with MAPPO requiring around 400k episodes to match the rewards produced by LIGS. \newline 
% 
% There are three possible options (assuming that no agent stays in the middle for all game). The agents can coordinate themselves to go to the top section and earn $r/2$ each one on average. They can go to opposite sections, leading to rewards expected rewards $-r$ and $r$ respectively. Finally, they can choose to go the bottom section obtaining a reward $R$. \newline
% \subsubsection{Experiment 3: Sparse reward}
% \begin{wrapfigure}{R}{0.6\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[scale=0.22]{Figures/coordinated.jpg}
%     \caption{TBA.}
%     \label{Figure:LearningCurves}
%     \vspace{-5mm}
% \end{wrapfigure}
% \begin{wrapfigure}{R}{0.4\textwidth}
%     \centering
%     \includegraphics[scale=0.3]{Figures/lbf/sparse_v3.pdf}
%     \caption{Sparse Rewards}
%     \label{fig:my_label}
% \end{wrapfigure}
% 
\textbf{Experiment 3: Sparse rewards. }
We tested our claim that LIGS can promote learning in MAS with sparse rewards. We simulate a sparse reward setting using a competitive game between two teams of agents. One team is controlled by LIGS while the other actions of the agents belonging to the other team are determined by a fixed policy. The goal is to collect the apple faster than the opposing team. Collecting the apple results in a reward of 1, and rewards are 0 otherwise. This is a challenging sparse reward since informative reward signals occur only apple when the apple is collected. As is shown in Fig. 1 both LIGS and MAPPO perform well on the sparse rewards environment, whilst the other baselines are all unable to learn any behaviour on this environment. % 

We next investigated the workings of the LIGS framework. We studied the locations where {\fontfamily{cmss}\selectfont Generator} added intrinsic rewards in Experiments 1 and 2. As is shown in the heatmap visualisation in Fig. \ref{fig:heatmap}, for Experiment 2, we observe that {\fontfamily{cmss}\selectfont Generator} learns to add intrinsic rewards that guide the agents towards the optimal reward (bottom right) and away from the suboptimal rewards at the top (where some other baselines converge). This supports our claim that LIGS learns to guide the agents towards jointly optimal policies. Moreover, as Fig. \ref{fig:heatmap} shows, LIGS's switching mechanism means that {\fontfamily{cmss}\selectfont Generator} only adds intrinsic rewards as the most useful locations for guiding the agents towards their target. For Experiment 1, Fig. \ref{fig:heatmap} shows that {\fontfamily{cmss}\selectfont Generator} learns to guide the agents towards the apple which delivers the high rewards. Crucially Fig. \ref{fig:heatmap} (Right) demonstrates a striking benefit of the LIGS framework, namely it only activates the intrinsic rewards around the apple when \textit{both} agents are at most 2 cells away from the apple. Since the agents receive positive rewards only when they arrive at the apple simultaneously, this ensures the agents are encouraged to coordinate their arrival and receive the maximal positive rewards and avoids encouraging arrivals at times that lead to penalties.    
% \vspace{-5 mm}
\subsection{Learning Performance in StarCraft Multi-Agent Challenge}


\begin{figure}
    \centering
    % \vspace{-4mm}
    \includegraphics[scale=0.35]{Figures/SMAC/smac-3.pdf}
    \caption{\emph{Median win rate over the course of learning on SMAC.} LIGS outperforms the baselines on all maps. LIIR, LICA, and MAVEN are generally not visible as their win rate is negligible.}
    \label{Figure:SMAC_learning_curves}
    % \vspace{-11mm}
\end{figure}
% \vspace{-5 mm}
% \begin{wrapfigure}{H}{0.7\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.7\textwidth]{Figures/SMAC/SMAC.pdf}
%     \caption{\emph{Median win rate over the course of learning on SMAC Maps.} LIGS outperforms the baselines on all maps. LIIR, LICA, and MAVEN are generally not visible as they do not achieve a positive win rate.}
%     \label{Figure:SMAC_learning_curves}
%     \vspace{-5mm}
% \end{wrapfigure}
% 
% \begin{wrapfigure}{R}{0.6\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[scale=1]{Figures/SMAC/SMAC.pdf}
%     \caption{TBA.}
%     \label{Figure:LearningCurves}
%     \vspace{-5mm}
% \end{wrapfigure}
% 
To study the performance of LIGS in highly complex environments, we compared its performance with the aforementioned baselines on three StarCraft Multi-Agent Challenge (SMAC) maps \cite{samvelyan2019starcraft} \emph{5m vs. 6m} (hard), \emph{6h vs. 8z}, and \emph{Corridor}.
% , and \emph{MMM2} (super hard). 
These maps vary in a range of MARL attributes such as number of units to control, environment reward density, unit action sets, and (partial)-observability. In Fig. \ref{Figure:SMAC_learning_curves}, we report our results showing `Win Rate' vs `Steps'. These curves are generated by computing the median win rate (vs the opponent) of the agent at regular intervals during learning. We ran $3$ of each algorithm (further setup details are in the Supp. material Sec \ref{sec:app_imp_details}). LIGS outperforms the baselines in all maps. In \emph{5m vs. 6m}, the baselines do not approach the performance of LIGS. In  \emph{Corridor} MAPPO requires over an extra million steps to match the performance of LIGS. In \emph{6h vs. 8z}, LIGS slightly outperforms the baselines. 
% but it is plausible (considering performance on the other maps), that in a longer run the gap between our method and the baselines would increase. 
In summary, LIGS shows performance gains over all baselines in SMAC maps which encompass diverse MAS attributes.


% In this suite of experiments,  we test our claim that LIGS leads to superior performance in complex tasks. We compared LIGS with state of the art MARL baselines in Corridor, 6h vs 8z, 5m vs 6m and XX maps within StarCraft Micromanagement II \cite{samvelyan2019starcraft}. These challenging environments incorporate reward sparsity and require coordination among the agents to complete the task. Fig. \ref{Figure:LearningCurves} shows learning curves. As is shown in \ref{Figure:LearningCurves}, LIGS markedly outperforms the best competing baseline MAPPO. Moreover, unlike reward-shaping which can undermine performance, LIGS ensures that the intrinsic reward generated does not undermine performance.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.3\textwidth]{Figures/SMAC/CORRIDOR.jpg}
%     \includegraphics[width=0.3\textwidth]{Figures/SMAC/1.jpg}
%     \includegraphics[width=0.3\textwidth]{Figures/SMAC/2.jpg}
%     \includegraphics[width=0.3\textwidth]{Figures/SMAC/2.jpg}
%     \caption{SMAC corridor, 6h vs 8z, 5m vs 6m}
%     \label{fig:my_label}
% \end{figure}

\vspace{-2mm}
\section{Conclusion}

% \vspace{-2mm}
We introduced LIGS, a novel framework for generating intrinsic rewards which significantly boosts performance of MARL algorithms. Central to LIGS is a powerful adaptive learning mechanism that generates intrinsic rewards according to the task and the MARL learners' joint behaviour. 
% LIGS can induce complex exploratory behaviour and ensures the underlying task is preserved. 
% LIGS is the ability to selectively choose the set of states to add  intrinsic rewards enabling rapid learning of the intrinsic rewards that lead to better performance. 
% 
% Crucially, this yields a novel complementary set of problems, one which is for the system of agents to learn their optimal joint policies and the other for {\fontfamily{cmss}\selectfont Generator} to learn where to add the intrinsic rewards and their optimal magnitudes.  
% 
Our experiments show LIGS induces superior performance in MARL algorithms in a range of tasks.

\section{Acknowledgements}
We would like to thank Matthew Taylor and Aivar Sootla for their helpful comments. 

% 
% The most significant contribution of this paper, however, is the novel construction that ties together multi-agent RL with switching controls which leads to new solution method. We believe this powerful approach can be adopted to solve other open challenges in MARL.

\onecolumn
\bibliographystyle{iclr2022_conference}
\bibliography{main}


% \clearpage
% CHECKLIST
% \begin{itemize}
%     \item Restate the claims that you will be testing. That is, what claims are you running experiments to check?
%     \item Claim 1: LIGS leads to superior joint exploration compared to competitors and non-RS baselines
%     \item Claim 2: LIGS, as a result of superior exploration, leads to better coordinated policies 
%     \item Claim 3: LIGS, as a result of superior exploration, is able to tackle sparse reward environments
%     \item Use positive language. LIGS successfully demonstrates these claims
%     \item Then, say that you test LIGS against baselines on a standard MARL baseline
%     \item Need to also describe the baselines that we included and why we included these baselines
%     \item  Describe experimental methodology: runs, seeds
% \end{itemize}
% % \subsection{Tasks}
% % \hspace{-45 mm}\begin{tabular}{c|c|c|c |c}
% %     \textbf{Experiment} & \textbf{Description} & \textbf{Person} & \textbf{Status} &  \textbf{Completion Date}\\
% %     \hline
% %      Joint exploration & \includegraphics[scale=0.5]{Figures/LBF_c.png} & Jianhong & Not complete & 17.09.2021 \\
% %     \hline
% %      Optimised coordinated policies & \includegraphics[scale=0.22]{Figures/coordinated.jpg} & Nicolas & Not complete & 17.09.2021 \\
% %     \hline
% %      Sparse reward & \includegraphics[scale=0.5]{Figures/sparse.png} & Jianhong & Not complete & 17.09.2021 \\
% %     \hline
% %      Subgoal discovery & \includegraphics[scale=0.5]{Figures/LBF_aac.png} & Nicolas & Not complete & 17.09.2021 \\
% %     \hline
% %      Large scale & \includegraphics[scale=0.2]{Figures/sc22.jpg} & Taher + Huawei Cloud & Not complete & 20.09.2021 \\
% % \end{tabular}


% % \begin{figure}
% %     \centering
% %     \includegraphics[width=\columnwidth]{Figures/mappo_marco_rnd.png}
% %     \caption{Caption}
% %     \label{fig:my_label}
% % \end{figure}
% % \begin{figure}
% %     \centering
% %     \includegraphics[width=\columnwidth]{Figures/Switch2-v0_screenshot.png}
% %     \caption{Caption}
% %     \label{fig:my_label}
% % \end{figure}
% \subsection{Experiment 1: Joint Exploration}
% In this experiment, we test our claim that LIGS leads to superior joint exploration.
% \begin{itemize}
%     \item Describe environment
%     \item Payoff depends on agents executing a specific combination of actions, partial execution of these actions has negative reward. Note, in particular, fewer people coordinating is better than more people coordinating but not as good as all people coordinating. Hard exploration problem.
%     \item Why this environment? You need a strong explorer to get past the trough of partial coordination being bad. It's conceptually simple. 
%     \item Metric: (1) did you find the optimal joint action? (2) How many actions did you explore? (3) How fast did you find the optimal action?
%     \item Actually, if we really want to show exploration, we want to see that we cover the state-action space, no? Maybe have a plot showing percentage of state-action space covered?
%     \item Other environment: limited version of some Starcraft map. Harder to see state-action coverage, but we can say that reaching better performance indirectly indicates that you are better explorer
%     \item Starcraft maybe 2m\_vs\_1z?
%     \item Above map requires coordination between both marines to avoid being killed by Zealot
% \end{itemize}

% \subsection{Experiment 2: Coordination \& Joint Performance}
% In this experiment, we test our claim that LIGS leads to superior joint policies due to its ability to promote coordination among the agents.
% \begin{itemize}
%     \item This experiment should show that LIGS gets to better joint payoffs than competitors
%     % \item Environment: moving fire? Starcraft?
%     \item Need to justify why this is a good environment to check this problem. It will be a good environment if there are sub-optimal solutions
%     \item Maybe in moving fire environment, give reward if one agent puts out fire, but also if two do
%     \item I think it would be conceptually simpler to use a similar environment as used in (1).
%     % \item Check how many reviewers complained about simplicity in NeurIPS?
%     \item If our environment surpasses those sub-optimal points, its proof that we get to better joint policies
% \end{itemize}

% \subsection{Experiment 3: Sparse Reward Environments}
% In this experiment, we test our claim that LIGS promotes better performance in sparse reward environments.
% \begin{itemize}
%     \item Environment: version of two state MDP where there's no reward except for a specific combination of actions?
%     \item SMAC 2m\_vs\_1z would also be good here
%     \item Why does this environment work? Because if you can solve it while others can't, you can solve sparse reward envs.
% \end{itemize}

% \subsection{Experiment 4:  Complex Tasks}
% In this experiment, we test our claim that LIGS leads outperforms baselines on benchmarks in complex tasks.
% \begin{itemize}
%     \item Essentially just show better performance on SMAC
% \end{itemize}




% \section*{Baselines}
% \begin{center}
% \begin{tabularx}{\textwidth} { 
%   | >{\centering\arraybackslash}X | >{\centering\arraybackslash}X |}
%  \hline
%  \textbf{Baseline} & \textbf{Link} \\\hline\hline
%     MAPPO (No RS) & -\\
%     LIIR (RS) & https://github.com/yalidu/liir\\
%     LICA (RS) & https://github.com/mzho7212/LICA\\
%     MAVEN (RS) & https://github.com/AnujMahajanOxf/MAVEN\\

%     ICQL & https://arxiv.org/pdf/1906.02138.pdf\\    
%     QTRAN & https://github.com/Sonkyunghwan/QTRAN\\
%     QMIX & https://github.com/oxwhirl/wqmix\\    
%     IQL (Ind. Q) & https://github.com/oxwhirl/pymarl\\
%     IAC (Ind. AC) & -\\
%     Noisy-MAPPO (No RS) & https://github.com/hijkzzz/noisy-mappo\\
%     Multi-explore (RS) & https://github.com/shariqiqbal2810/Multi-Explore \\
%  \hline
% \end{tabularx}
% \end{center}
% @DAVID - https://arxiv.org/pdf/2103.15941.pdf

\clearpage


% \begin{center}
% \begin{tabularx}{\textwidth} { 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X | }
%  \hline
%  \textbf{Baseline} & \textbf{Paper} & \textbf{Link} & \textbf{Continuous/Discrete?} \\\hline\hline
%   MAPPO   & - & - &- \\
%   LIIR   & 
% LIIR & https://github.com/yalidu/liir &- \\
%   MAVEN   & - &  https://github.com/AnujMahajanOxf/MAVEN &- \\
% Multi-explore   & - & https://github.com/shariqiqbal2810/Multi-Explore &- \\    
% ICQL   & https://arxiv.org/pdf/1906.02138.pdf &  &- \\    
%     QTRAN   & - & https://github.com/Sonkyunghwan/QTRAN.
%  &- \\
%     QMIX   & https://github.com/oxwhirl/wqmix & - &- \\    
%     indep. q learners   & https://github.com/oxwhirl/pymarl & - &- \\
%     indep. actor-critic learners   & - & - &- \\
%   \\ [1ex] 
%  \hline
% \end{tabularx}
% \end{center}





% \begin{figure}[h!]
% % \begin{table}[htb!]
% 	%\setlength{\extrarowheight}{2pt}
% \hspace{-40 mm}	\subfloat[$s(1)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(1)}$  & $a^2_{s(1)}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(1)}$ & $2$ & $0$ \\\cline{2-4}
% 		& $b^2_{s(1)}$ & $0$ & $1$ \\\cline{2-4}
% 	\end{tabular}}
% 		\subfloat[$s(2)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(2)}$  & $a^2_{s(2)}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(2)}$ & $-1$ & $0$ \\\cline{2-4}
% 		& $b^2_{s(2)}$ & $0$ & $3$ \\\cline{2-4}
% 	\end{tabular}}
% 			\subfloat[$s(2')$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(2')}$  & $a^2_{s(2')}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(2')}$ & $-1$ & $0$ \\\cline{2-4}
% 		& $b^2_{s(2')}$ & $0$ & $3$ \\\cline{2-4}
% 	\end{tabular}}
% 		\subfloat[$s(3)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(3)}$  & $a^2_{s(3)}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(3)}$ & $5$ & $2$ \\\cline{2-4}
% 		& $b^2_{s(3)}$ & $2$ & $2$ \\\cline{2-4}
% 	\end{tabular}}
% 	\subfloat[$s(3)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(3')}$  & $a^2_{s(3')}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(3')}$ & $-100$ & $-50$ \\\cline{2-4}
% 		& $b^2_{s(3')}$ & $-50$ & $-50$ \\\cline{2-4}
% 	\end{tabular}}
% 	    \caption{\textbf{Example: Failure of CT-DE methods} Three agent Dec-POMPD. agent 3 has an action set $\{x,y\}$ and randomises so that $x$ is played with probability $p$ and $y$ is played with probability $1-p$. All agents observe the current payoff matrix (but not the state), call this observation $z(s)$. Notice that since the agent 3 action is hidden variable; $z(s(2))=z(s(2'))$ and $z(s(3))=z(s(3'))$ so that these states are indistinguishable (they are all in the information set at that round). We have the following (incomplete) transition laws: $P(s(2)|\cdot,a_3=x,\cdot)=1, P(s(2')|\cdot,a_3=y,\cdot)=1$ and $P(s(3)|s(2),(a^2_{s(2)},b^2_{s(2)}))=1, P(s(3')|s(2'),,(a^2_{s(2')},b^2_{s(2')})))=1$. The dec-POMDP solution is a Bayes Nash equilibrium, that is the agents maximise using the distribution induced by $p$. A centralised critic observes both all actions and the state.
% 	   % The stable points are $(a^1_{s(1)},b^1_{s(1)}),(a^2_{s(1)},b^2_{s(1)})$, $(a^2_{s(2)},b^1_{s(2)}),(a^1_{s(2)},b^2_{s(2)})$ and $(a^1_{s(2)},b^1_{s(2)}),(a^1_{s(2)},b^2_{s(2)})$. Fully decentralised methods in general converge to either stable point.}
% 	   }
% 	    %\label{fig:classic_games}
% 	    \label{fig:classic_games_potential}
% 	    \vspace{-10pt}
% \end{figure}

% \begin{itemize}
%     \item \textbf{Intuitive simple scenarios covering each challenge:} See Sec. 6.1 - 6.3 in Learning to Shape Rewards using a Game of Switching Controls (us, 2021). 

% % \begin{itemize}
% %         \item MARL coordination in outcomes (social welfare)
% %         \item MARL coordinated exploration
% %         \item Others?
% %     \end{itemize} 
% \end{itemize}
% \begin{enumerate}
% %     \item \textbf{Stochastic Matrix games} (show CT-DE failure). See Fig. 1.
    
% % \item  \textbf{Batch of maze environments} 
    
% %     \textbf{1} Test joint coordination - stochastic coordination game  (show convergence to socially optimal outcome). 
    
% %     \textit{\textbf{Outputs:} Show convergence point and performance curves.}\\ 
    
%     \textbf{2}. Coordinated exploration (show optimally coordinated exploration)
    
%     \textit{\textbf{Outputs:} heatmap of exploration.}\\
    
%     \textbf{3}. Show sparse reward setting (convergence to optimal joint policy) 
    
%     \textit{\textbf{Outputs:} Performance curves.}\\

% % \textbf{4}. Non monotonic setting SGs.
%     \item  \textbf{Suite of StarCraft experiments}: Sparse reward environment, coordination experiment,.... \textbf{Outputs:} Performance curves.
%     \item  \textbf{Multi-agent Mujoco.} \textbf{Outputs:} Performance curves.
%     \item \textbf{Merge env.? }(CT-DE failure) \textbf{Outputs:} Collision rate.
%     \item \textbf{ABLATION STUDIES:}
%     \begin{enumerate}
%         \item \textbf{1:Adaptivity}. when MARL agents use different types of policies (e.g. more stochastic) the framework adapts. \textbf{Outputs:} performance curves 
%     \item \textbf{Switching control ablation}. As in our single-agent submission--- show this removing the switching control reduces performance. \\\textbf{Outputs:} performance curves
%     \item \textbf{Functional form of shaping reward ablation} --- show this doesn't matter. \textbf{Outputs:} performance curves
%         \item \textbf{Choice of (MA)RL policy - plug \& play} --- show choice of MARL algorithm doesn't matter. \\\textbf{Outputs:} performance curves
% \end{enumerate}
%     \end{enumerate}
% % \section{Appendix}
% \clearpage
\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\part{{\Large{Appendix}}} % Start the appendix part
\parttoc

\newpage
\section{Algorithm}\label{sec:algorithm}

\begin{algorithm}[h]
    \label{algo:Opt_reward_shap} 
    \DontPrintSemicolon
    \KwInput{ Environment $E$ \;
             \hspace{3em} Initial {\fontfamily{cmss}\selectfont agent} policies $\boldsymbol{\pi}_0=(\pi^1_0,\ldots \pi^N_0)$ with parameters  $\theta_{\pi^1_0},\ldots\theta_{\pi^1_N}$,  Initial {\fontfamily{cmss}\selectfont Generator} switch policy $\mathfrak{g}_{c_{0}}$ with parameters $\theta_{\mathfrak{g}_{c_{0}}}$, Initial {\fontfamily{cmss}\selectfont Generator} action policy $g_0$ with parameters $\theta_{g_0}$,  Randomly initialised fixed neural network $\phi(\cdot, \cdot)$, Neural networks $h$ (fixed) and $\hat{h}$ for Augmented RND with parameter $\theta_{\hat{h}}$, Buffer $B$, Number of rollouts $N_r$, rollout length $T$, Number of mini-batch updates $N_u$, Switch cost $c$, discount factor $\gamma$, learning rate $\alpha$.\;
             }
    \KwOutput{Optimised {\fontfamily{cmss}\selectfont agent} policies $\boldsymbol{\pi^\star}=(\pi^{\star,1},\ldots,\pi^{\star,N})$}
    $\boldsymbol{\pi}=(\pi^1,\ldots,\pi^N), g, \mathfrak{g}_{c} \gets \boldsymbol{\pi}_0, g_0,\mathfrak{g}_{c_{0}}$\;
    \For{$n = 1, N_r$}
    {
        \textbf{// Collect rollouts}\;
        \For{$t = 1, T$}
        {
            Get environment states $s_t$ from $E$ \;
            Sample $\boldsymbol{a}_t=(a^1_t,\ldots,a^N_t)$ from $(\pi^1(s_t),\ldots,\pi^N(s_t))$ \;
            Apply action $\boldsymbol{a}_t$ to environment $E$, get rewards $\boldsymbol{r}_t=(r^1_t,\ldots,r^N_t)$ and next state $s_{t+1}$ \;
            Sample $q_t$ from $\mathfrak{g}_{c}(s_t)$ \textbf{ // Switching control} \;
            \eIf{$q_t = 1$}
            {   
                Sample $\theta^c_t$ from $g(s_t)$ \;
                Sample $\theta^c_{t+1}$ from $g(s_{t+1})$ \;
                $f^i_t = \gamma \phi(s_{t+1}, \theta^c_{t+1}) - \phi(s_{t}, \theta^c_{t})$ \textbf{// Calculate $F(s_t, \theta^c_t, s_{t+1}, \theta^c_{t+1})$}\;
            }
            {   
                $\theta^c_t, f^i_t = 0, 0$ \textbf{// Dummy values}
            }
            Append $(s_t, \boldsymbol{a}_t, g_t, \theta^c_t, \boldsymbol{r}_t, f^i_t, s_{t+1})$ to $B$
        }
        \For{$u = 1, N_u$}
        {
            Sample data $(s_t, \boldsymbol{a}_t, g_t, \theta^c_t, \boldsymbol{r}_t, f^i_t, s_{t+1})$ from $B$\;
            \eIf{$g_t = 1$}
            {   
                Set reward to $\boldsymbol{r}_t^s = \boldsymbol{r}_t + f^i_t$
            }
            {
                Set reward to $\boldsymbol{r}_t^s = \boldsymbol{r}_t$
            }
            \textbf{// Update Augmented RND} \;
            $\text{Loss}_{\text{RND}} = ||h(s_t,\boldsymbol{a}_t) - \hat{h}(s_t,\boldsymbol{a}_t)||^2$ \;
            $\theta_{\hat{h}} \gets \theta_{\hat{h}} - \alpha \nabla \text{Loss}_{\text{RND}} $ \;
            \textbf{// Update {\fontfamily{cmss}\selectfont Generator} }\;
            $l_t = ||h(s_t,\boldsymbol{a}_t) - \hat{h}(s_t)||^2$ \textbf{// Compute $L(s_t,\boldsymbol{a}_t)$} \;
            $c_t = c g_t$ \;
            Compute $\text{Loss}_{g}$ using $(s_t, a_t, g_t, c_t, \boldsymbol{r}_t, f^i_t, l_t, s_{t+1})$ using PPO loss \textbf{// Section \ref{sec:learning_proc}} \;
            Compute $\text{Loss}_{ \mathfrak{g}_{c}}$ using $(s_t, a_t, g_t, c_t, \boldsymbol{r}_t, f^i_t, l_t, s_{t+1})$ using PPO loss \textbf{// Section \ref{sec:learning_proc}}\;
            $\theta_{g} \gets \theta_{g} - \alpha \nabla \text{Loss}_{g}$ \;
            $\theta_{\mathfrak{g}_{c}} \gets \theta_{\mathfrak{g}_{c}} - \alpha \nabla \text{Loss}_{\mathfrak{g}_{c}}$ \;
            \textbf{// Update agent $j$, for each $ j \in 1,\ldots, N$}\;
            Compute $\text{Loss}_{ \pi^j}$ using $(s_t, \boldsymbol{a}_t, r^{j,s}_t:=r^j_t+f^i_t, s_{t+1})$ using PPO loss \textbf{// Section \ref{sec:learning_proc}} \;
            $\theta_{\pi^j} \gets \theta_{\pi^j} - \alpha \nabla \text{Loss}_{\pi^j}$ \;
        }
    }
	\caption{\textbf{L}earnable \textbf{I}ntrinsic-Reward \textbf{G}eneration \textbf{S}election algorithm (LIGS)}
\end{algorithm}
\newpage

\section{Ablation study: Plug \& Play}\label{sec:plug_n_play}
In order to validate our claim that LIGS freely adopts RL learners, we tested the ability of LIGS to boost performance in a complex coordination task using independent Proximal policy optimization algorithm (IPPO) \cite{schulman2017Proximal} as the base learner. In this experiment, two agents are spawned at opposite sides of the grid. The red agent is spawned in the left hand side and the blue agent is spawned in the right hand side of the grid in Fig. \ref{fig:my_label} (right). The goal of the agents is to arrive at their corresponding goal states (indicated by the coloured square, where the colour corresponds to the agent whose goal state it is) at the other side of the grid. Upon arriving at their goal state the agents receive their reward. However, the task is made difficult by the fact that only one agent can pass through the corridor at a time. Therefore, in this setup, the only way for the agents to complete the task is for the agents to successfully coordinate, i.e. one agent is required to allow the other agent to pass through before attempting to traverse the corridor. 

It is known that independent learners in general, struggle to solve such tasks since their ability to coordinate systems of RL learners is lacking \cite{yang2020multi}.  This is demonstrated in Fig. \ref{fig:my_label} (left) which displays the performance curve of for IPPO which fails to score above $0$. As claimed, when incorporated into the LIGS framework, the agents succeed in coordinating to solve the task. This is indicated by the performance of IPPO + LIGS (blue).  

\begin{figure}[h!]
    \centering
\includegraphics[width=0.4\textwidth]{Figures/Appendices/Coord.pdf}
\vspace{3mm}\hspace{10 mm}
    \includegraphics[width=0.4\textwidth]{Figures/Appendices/Switch2-v0.pdf}
    \caption{\textit{Left.} Performance curves for IPPO and IPPO with LIGS. \textit{Right.} Coordination environment.  }
    \label{fig:my_label}
\end{figure}
\newpage
\section{Results on Additional StarCraft Micromanagement II Maps}
Here we present results on two more maps: MMM2 and 3s5z vs 3s6z  As can be seen in Fig. \ref{fig:smac_maps_additional}, LIGS outperforms the leading baseline MAPPO in all maps yielding a $50\%$ improvement in win rate over MAPPO in MMM2.

\begin{figure}[h!]
    \centering
    % \includegraphics[width=0.6\textwidth]{Figures/additional_experiments/6h_vs_8z.pdf}
    \includegraphics[width=0.45\textwidth]{Figures/additional_experiments/mmm2.pdf}
      \includegraphics[width=0.45\textwidth]{Figures/additional_experiments/3s5z.pdf}
      \caption{Performance of LIGS in additional SMAC maps}
    \label{fig:smac_maps_additional}
\end{figure}

\newpage
\section{Flexibility of LIGS to Accommodate different Exploration Bonus Terms $L$}
To demonstrate the robustness of our method to different choices of exploration bonus terms in {\fontfamily{cmss}\selectfont Generator}'s objective, we conducted an Ablation study on the $L$-term (c.f. Equation \ref{generator_objective}) where we replaced the RND $L$ term with a basic count-based exploration bonus. To exemplify the high degree of flexibility, we replaced the RND with a simple exploration bonus term  $L(s)=\frac{1}{\text{Count(s)}+1}$ for any given state $s\in\mathcal{S}$ where Count$(s)$ refers to a simple count of the number of times the state $s$ has been visited.  We conducted the Ablation study on all three Foraging environments presented in Sec. \ref{exp:foraging}. We note that despite the simplicity of the count-based measure, generally the performance of both versions of LIGS is comparable and in fact the count-based variant is superior to the RND version for the joint exploration environment.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/Appendices/sparse_new_countbased.pdf}
    \includegraphics[width=0.45\textwidth]{Figures/Appendices/subopt_new_countbased.pdf}
        \includegraphics[width=0.45\textwidth]{Figures/Appendices/jointexp_new_countbased.pdf}
    \caption{Performance of LIGS compared with  the exploration bonus replaced by count-based method on the three tasks in the Foraging environment.}
    \label{fig:count_based_comparison}
\end{figure}

% \newpage
% \section{Comparison against Intrinsic Reward using Random Network Distillation}
%  Fig. \ref{fig:L_ablation} shows performance curves of vanilla MAPPO MAPPO base MARL learners whose objectives now include an RND term directly in their objectives (MAPPO+RND) and LIGS. The environment is the coordination environment shown in Fig. \ref{fig:my_label}.  We observe that LIGS significantly  outperforms MAPPO+RND which demonstrates the improvement delivered by the LIGS framework against high performing exploratory intrinsic reward methods. Due to the added benefit of switching controls and intrinsic reward selection performed by {\fontfamily{cmss}\selectfont Generator}, we observe that LIGS is able to significantly augment the benefits of applying RND directly.

% \begin{figure}[h!]
%     \centering
% \includegraphics[width=0.8\textwidth]{Figures/additional_experiments/reviewer_crmy_robustness_to_L.pdf}
% % \vspace{3mm}\hspace{10 mm}
%     \caption{LIGS (blue) outperforms RND as an intrinsic reward term in the agents' objectives (green) and vanilla MAPPO  (orange).}
%     \label{fig:L_ablation}
% \end{figure}


\newpage
\section{Further Experiment Demonstrating LIGS improved use of Exploration Bonuses.}
As we have shown above, LIGS can accommodate a variety of exploration bonuses and perform well. Here, we did a experiment to further justify using LIGS against simpler exploration bonus methods. We compared LIGS against and MAPPO with an RND  intrinsic reward in the agents' objectives (MAPPO+RND) and vanilla MAPPO. Fig. \ref{fig:us_vs} shows performance of these two methods on coordination environment shown in Fig. \ref{fig:my_label}. We note that LIGS markedly outperforms both MAPPO+RND and vanilla MAPPO. Due to the added benefit of switching controls and intrinsic reward selection performed by {\fontfamily{cmss}\selectfont Generator}, we observe that LIGS is able to significantly augment the benefits of applying RND directly to the agents' objectives.

\begin{figure}[h!]
    \centering
\includegraphics[width=0.8\textwidth]{Figures/additional_experiments/US_vs.pdf}
% \vspace{3mm}\hspace{10 mm}
    \caption{Performance curves for LIGS, MAPPO with RND intrinsic rewards and vanilla MAPPO. The additional machinery of switching-controls and intrinsic reward selection allows LIGS to make better use of exploration bonuses. In this case, LIGS demonstrates significant improvement over MAPPO with RND intrinsic rewards.}
    \label{fig:us_vs}
\end{figure}

\newpage
\section{Further Implementation Details}\label{sec:app_imp_details}

% For simplicity, each $g_m$ is a deterministic policy. 
% For simplicity, the termination is determined by a pr which also avoids long running options.

Details of {\fontfamily{cmss}\selectfont Generator} and $F$ (intrinsic-reward)\\
\begin{tabular}{c|l}
\textbf{Object} & \textbf{Description}\\
\hline
$f$ & Fixed feed forward NN that maps $\mathbb{R}^d \mapsto \mathbb{R}^m$ \\
    & [512, \texttt{ReLU}, 512, \texttt{ReLU}, 512, $m$] \\
$\Theta$ & Discrete action set which is size of output of $f$,\\
    & i.e., $\Theta$ is set of integers $\{1,...,m\}$ \\
$g$ & Fixed feed forward NN that maps $\mathbb{R}^d \mapsto \mathbb{R}^m$ \\
    & [512, \texttt{ReLU}, 512, \texttt{ReLU}, 512, $m$] \\
Intrinsic-reward function $\phi$ &  $\phi(s,\theta^c)=f(s)\cdot \theta^c$\\
$F$ & $\gamma \phi(s_{t+1}, \theta^c_{t+1})$ - $\phi(s_{t},\theta^c_{t}),$\;\; $\gamma=0.95$\\
\end{tabular}

$d$=Dimensionality of states; $m\in \mathbb{N}$ - tunable free parameter.

\underline{In all experiments} we used the above form of $F$ as follows: a state $s_t$ is input to the $g$ network and the network outputs logits $p_t$. We softmax and sample from $p_t$ to obtain the action $\theta^c_t$. This action is one-hot encoded. Then, the action $\theta^c_t$ is multiplied with $f(s_t)$ to compute the second term of $F$. A similar process is used to compute the first term. In this way the policy of {\fontfamily{cmss}\selectfont Generator} chooses the intrinsic-reward.

\subsection{Hyperparameter Settings}
In the table below we report all hyperparameters used in our experiments. Hyperparameter values in square brackets indicate ranges of values that were used for performance tuning.

\begin{center}
    \begin{tabular}{c|c} 
        \toprule
        Clip Gradient Norm & 1\\
        $\gamma_{E}$ & 0.99\\
        $\lambda$ & 0.95\\
        Learning rate & $1$x$10^{-4}$ \\
        Number of minibatches & 4\\
        Number of optimisation epochs & 4\\
        Number of parallel actors & 16\\
        Optimisation algorithm & Adam\\
        Rollout length & 128\\
        Sticky action probability & 0.25\\
        Use Generalized Advantage Estimation & True\\
        \midrule
        Coefficient of extrinsic reward & [1, 5]\\
        Coefficient of intrinsic reward & [1, 2, 5, 10, 20, 50]\\
        {\fontfamily{cmss}\selectfont Generator} discount factor & 0.99\\
        Probability of terminating option & [0.5, 0.75, 0.8, 0.9, 0.95]\\
        $L$ function output size & [2, 4, 8, 16, 32, 64, 128, 256]\\
        \bottomrule
    \end{tabular}
\end{center}
\clearpage
\section{{\fontfamily{cmss}\selectfont Generator} Termination Times}\label{sec:termination_times}

There are various possibilities for the \textit{termination} times $\{\tau_{2k}\}$ (recall that $\{\tau_{2k+1}\}$ are the times
% _{k\geq 1}
which the $F$ is \textit{switched on} using $\mathfrak{g}_c$). One is for {\fontfamily{cmss}\selectfont Generator} to determine the sequence. Another is to build a construction of $\{\tau_{2k}\}$ that directly incorporates the information gain that a state visit provides: let $w:\Omega \to \{0,1\}$  be a random variable with ${\rm Pr}(w=1)=p$ and ${\rm Pr}(w=0)=1-p$ where $p\in ]0,1]$. Then for any $k=1,2,\ldots,$ and denote by $\Delta L(s_{\tau_k}):=L(s_{\tau_k})-L(s_{\tau_k-1})$, then we can set:
\begin{align}
I(s_{\tau_{2k+1}+j})=\begin{cases}
			I(s_{\tau_{2k+1}}), & \text{if $w\Delta L(s_{\tau_k+j})>0,$}\\
            I(s_{\tau_{2k+2}}), & w\Delta L(s_{\tau_k+j})\leq 0.\label{option_termination_criterion}
		 \end{cases} 
\end{align}

To explain, since  $\{\tau_{2k}\}_{k\geq 0}$ are the times at which $F$ is switched off then if  $F$ is deactivated at exactly after $j$ time steps then   $I(s_{\tau_{2k+1}+l})=I(s_{\tau_{2k+1}})$ for any $0\leq l<j$ and $I(s_{\tau_{2k+1}+j})=I(s_{\tau_{2k+2}})$ . We now see that \eqref{option_termination_criterion} terminates $F$ when either the random variable $w$ attains a $0$ or when $\Delta L(s_{\tau_k+j})\leq 0$ which occurs when the exploration bonus in the current state is lower than that of the previous state.  

 
%  \begin{algorithm}[t!]
% \begin{algorithmic}[1]
% 	\caption{\textbf{C}oordinating \textbf{A}daptive \textbf{R}einforcement \textbf{M}ulti-Agent  Algorithm  (LIGS)}
% 	\label{algo:Opt_reward_shap_psuedo} 
% % 	\begin{algorithmic}[1]
% % 
% 		\FOR{$N$}
% 		    \FOR{$N_{steps}$}
%     		    \STATE At time $t=0,1,\ldots$ each agent $i\in 1,\ldots N$ applies $a^i_t$ to the environment which returns $s_{t+1},  r_{t+1}$
%     		  \STATE {\fontfamily{cmss}\selectfont Generator} samples an action $\theta^c_{t+1}\sim g(\cdot|s_{t+1
%     		    })$
%     		    \STATE {\fontfamily{cmss}\selectfont Generator} computes $F(\cdot)$ given $s_{t+1},\theta^c_{t+1},\theta^c_{t=\tau_k}$ for some $k$
%     		    \STATE Reward $\hat{R}=R+F$ is computed
%     		        \ENDFOR
% 		    \STATE{\textbf{// Learn the individual policies}}
% 	        \STATE Update the value function
% 	        \STATE Update policies of agent $i\in\mathcal{N}$ and {\fontfamily{cmss}\selectfont Generator} via the value function in 11.
% 	       % \STATE Optimise $G_2(m|\cdot)$ according to objective \ref{P2_obj}
%          \ENDFOR
% \end{algorithmic}         
% \end{algorithm}
 
%  \begin{algorithm}[t!]
% \begin{algorithmic}[1]
% 	\caption{\textbf{S}elective \textbf{C}oordinating \textbf{A}daptive \textbf{L}earning  \textbf{A}lgorithm  (LIGS)}
% 	\label{algo:Opt_reward_shap_psuedo} 
% % 	\begin{algorithmic}[1]
% % 
% 		\FOR{$N$}
% 		    \FOR{$N_{steps}$}
%     		    \STATE 
% 	    Given environment state $s_t$, each agent $i\in\mathcal{N}$ samples $a^i_t$ from $\pi^i(s_t)$ and obtain $s_{t+1},  r^i_{t+1}$ by applying $\boldsymbol{a}_t$ to environment\;
	    
% 	   \STATE Evaluate $\mathfrak{g}_c(s_{t})$ according to Prop. \ref{prop:switching_times}\;
% 	   % Sample $\mathfrak{g}_c$ \textbf{// Switch is off (which occurs according to Equation \eqref{option_termination_criterion})}\; 
	   
% 	   \eIf{ If $\mathfrak{g}_c(s_{t})$ = 1
%     {\fontfamily{cmss}\selectfont Generator} samples an action $\theta^c_{t+1}\sim g(\cdot|s_{t+1
%     		    })$\;
% 	    {\fontfamily{cmss}\selectfont Generator} computes $f^i_{t+1} = \hat{F}(s_t, \theta_{t},  s_{t+1}, \theta_{t+1}) $, \;
	    
% 	  Set agent $i$ reward $r^i = r^i_{t+1} + f^i_{t+1}$\;}
	  
%  \eIf{ If $\mathfrak{g}_c(s_{t}) = 0$
	    
% 	  Set agent $i$ reward $r^i = r^i_{t+1} $}
	   
%     % \STATE Update $\pi, \mathfrak{g}_c, g$ using $s_t, a_t, r, s_{t+1}$ and $\Delta$ 
%     		       \ENDFOR
% 		    \STATE{\textbf{// Learn the individual policies}}
% 	        \STATE Update the value function 
% 	        \STATE Update policies of agent $i\in\mathcal{N}$ and {\fontfamily{cmss}\selectfont Generator} via the value function.
% 	       % \STATE Optimise $G_c(m|\cdot)$ according to objective \ref{P2_obj}
%          \ENDFOR
% \end{algorithmic}         
% \end{algorithm}   

% \section*{Algorithm Details}
% \begin{itemize}
%     \item Use observation and action in RND in MARL
%     \item Action should be 1-hot encoded
%     \item L-term depends on $s_{t+1}$, not $s_t$
%     \item 0-out rewards at either end of shaping trajectories
%     \item Probabilisitc/Information gain termination criterion for shaping reward streams
%     \item Critics w/ and w/o shaping reward
%     \item Add coefficients to weight extrinsic vs. intrinsic rewards
%     \item Scale first term in $F$
% \end{itemize}


% \clearpage   
%   \section*{Ideas for improvement (1/3)}
% \begin{center}
% \begin{tabularx}{\textwidth} { 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X  
%   | >{\centering\arraybackslash}X | }
%  \hline
%  \textbf{Issue/Question} & \textbf{Suggested Resolution/Check} & \textbf{Notes} & \textbf{Owner}& \textbf{Status}  \\\hline\hline
%   Is the shaping reward agent permutation invariant?   &Check the output of $F$ should be different for each agent & Permutation invariance is consistent with our formalisation  &TJ &
%   \\\hline
%   Is the RND adding something useful?   &  Check $L\to 0$ as number of state-action pair visitations tends to $\infty$. Check RND term in agent 0 objective is properly scaled. Check $L:\boldsymbol{\mathcal{A}}\times\mathcal{S}\to \mathbb{R}$ is better than $L:\mathcal{S}\to \mathbb{R}$. Try also $L:\boldsymbol{\mathcal{A}}\times\mathcal{S}\to \mathbb{R}=L_1(s,\boldsymbol{a})+L_2(s)$ & Are the state definition and the action space definition appropriate here?   &TBA &
%     \\\hline
%   Do we need a shaper for each learning process? &  Implement $N$ reward-shaper framework: $N$ reward shapers with shared trunk    &  Having 1 shaper for all agents may be inappropriate. Can test each agent having its own shaper. All shaper NNs share a common trunk  &TBA &
%       \\\hline
%   Are the rewards scaled appropriately?  & Testing checks and magnitudes & Needs to be done for intrinsic versus extrinsic rewards + $L$ term and cost for the agent 0 objective  &TJ &
%     \\ [1ex] 
%  \hline
% \end{tabularx}
% \end{center}    
% \section*{Ideas for improvement (2/3)}
% \begin{center}
% \begin{tabularx}{\textwidth} { 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X  
%   | >{\centering\arraybackslash}X | }
%  \hline
%  \textbf{Issue/Question} & \textbf{Suggested Resolution/Check} & \textbf{Notes} & \textbf{Owner}& \textbf{Status}  \\\hline\hline
%   Are these optimised: \textbf{Learning rates, size of switching cost, termination criteria}   & Perform tuning  & -&TBA &
%   \\\hline
%   Are we using a good $\phi$ function?
%   & Check RND (as done previously), generic neural network & & TBA &
%   \\\hline
% %   Does the RND in {\fontfamily{cmss}\selectfont Generator} objective behave as we expect? & Check input of $\mathcal{S}\times\mathcal{A}$ against input of just $\mathcal{S}$. Check performance in each case, check if the value of $L$ is diminishing.& &TBA
% %   \\\hline
%   Are the training speeds for agent 0 and agents $1-N$ relatively ok? & Check effect on slowing down agent 0 training speed: try i) reducing learning rate ii) freezing the updates for some iterations. &&TBA &
%   \\\hline
%   What is the optimal weighting on terms within agent $i>0$ and agent 0 objectives  &Introduce weighting coefficients to be optimised & Check to see if there are general principles  &TJ &
%   \\\hline
%   Is the hypothesis that removing $F$ from agent 0 objective helps true? &   & 
%     \\\hline
%   What is the optimal observation set of shaper - include subset of joint actions?& Think of reasonable observation sets for the shaper & -  &TBA& 
%     \\ [1ex] 
%  \hline
% \end{tabularx}
% \end{center}    
% \newpage
% \section*{Ideas for improvement (3/3)}
% \begin{center}
% \begin{tabularx}{\textwidth} { 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X  
%   | >{\centering\arraybackslash}X | }
%  \hline
%  \textbf{Issue/Question} & \textbf{Suggested Resolution/Check} & \textbf{Notes} & \textbf{Owner}& \textbf{Status}  \\\hline\hline
%  Can a mechanism that switches off shaping mechanism help?  & Idea is after some training, the shaping reward is no longer useful. Try a termination criterion that switches off shaping reward after $n$ e.g. 2 episodes of decreasing reward  &TJ &
%         \\\hline
%   Dealing with cont. states discrete actions? for augmented RND term and $\phi$.    & Perform tuning  & -&TBA &
%   \\\hline
%   What is the best exploration bonus term
%   & for discrete we can use counting measure & & TBA &
%   \\\hline
% %   Does the RND in {\fontfamily{cmss}\selectfont Generator} objective behave as we expect? & Check input of $\mathcal{S}\times\mathcal{A}$ against input of just $\mathcal{S}$. Check performance in each case, check if the value of $L$ is diminishing.& &TBA
% %   \\\hline
%   Are the training speeds for agent 0 and agents $1-N$ relatively ok? & Check effect on slowing down agent 0 training speed: try i) reducing learning rate ii) freezing the updates for some iterations. &&TBA &
%   \\ [1ex] 
%  \hline
% \end{tabularx}
% \end{center}    
% % \section*{Tasks}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Task} & \textbf{Outputs} & \textbf{Notes} & \textbf{Owner} \\\hline\hline
% %   check current implementation, switch to value based method   & - & Different value based methods. Decide which is best &TJ, Jianhong \\
% %  \hline
% %  Augment implementation to MAS setting with joint action input & Updated MARL ready algo & RND $L$ term to be updated to now take joint action (test without also). Agent $0$ to have reward which takes the global reward of all agents. $F$ term is identical for all agents &TJ \\
% %  \hline
% %  Perform simple proof of concept experiment for MARL exploration in StarCraft & Performance curve vs training, compare against standard baseline (use standard baseline as plug and play) & Simple experiment can be exploration-based with suboptimal team nash equilibrium. Perhaps use fully decentralised MARL algo &TJ \\
% %   \hline
% %   Perfect code and send to friends
% %   & & &TJ, Jianhong \\ 
% %     \hline
% % Perform multi-agent Mujoco experiments & Performance curves against baselines and metrics &  & Jack
% %  \\     
% %   \hline
% %  Design simple conceptual experiments & design setup and metrics &  & .., DM, TJ
% %  \\     \hline
% %  Do literature review
% %   & Comprehensive study or relevant MARL frameworks and methods  &  & DM 
% %   \\
% %  \hline
% %   Find and run all relevant MARL baselines
% %   & List of MARL baselines \& run them on our experiments  &  & .. + 
% %   \\
% %      \hline
% %   Update proofs and writing for MARL setup
% %   &  &  & DM 
% %   \\
% % %      \hline
% % %   Extend Brian Swenson proof to SGs
% % %   &  &  &  Zhihao
% % %   \\
% % %      \hline
% % %   Do optimal dependent convergence proof
% % %   &  &  &  DM
% % %   \\
% % %      \hline
% % %   Do optimal independent consensus convergence proof
% % %   &  &  &  DM
% %   \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}
    
% % \section*{Outputs}
% % \begin{center}
% % \begin{tabularx}{\textwidth} { 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X 
% %   | >{\centering\arraybackslash}X | }
% %  \hline
% %  \textbf{Task} & \textbf{Outputs} & \textbf{Notes} & \textbf{Owner} \\\hline\hline
% %   check current implementation, switch to value based method   & - & Different value based methods. Decide which is best &TJ, Jianhong \\
% %  \hline
% %  Augment implementation to MAS setting with joint action input & Updated MARL ready algo & RND $L$ term to be updated to now take joint action (test without also). Agent $0$ to have reward which takes the global reward of all agents. $F$ term is identical for all agents &TJ \\
% %  \hline
% %  Perform simple proof of concept experiment for MARL exploration in StarCraft & Performance curve vs training, compare against standard baseline (use standard baseline as plug and play) & Simple experiment can be exploration-based with suboptimal team nash equilibrium. Perhaps use fully decentralised MARL algo &TJ \\
% %   \hline
% %   Perfect code and send to friends
% %   & & &TJ, Jianhong \\ 
% %     \hline
% % Perform multi-agent Mujoco experiments & Performance curves against baselines and metrics &  & Jack
% %  \\     
% %   \hline
% %  Design simple conceptual experiments & design setup and metrics &  & .., DM, TJ
% %  \\     \hline
% %  Do literature review
% %   & Comprehensive study or relevant MARL frameworks and methods  &  & DM 
% %   \\
% %  \hline
% %   Find and run all relevant MARL baselines
% %   & List of MARL baselines \& run them on our experiments  &  & .. + 
% %   \\
% %      \hline
% %   Update proofs and writing for MARL setup
% %   &  &  & DM 
% %   \\
% % %      \hline
% % %   Extend Brian Swenson proof to SGs
% % %   &  &  &  Zhihao
% % %   \\
% % %      \hline
% % %   Do optimal dependent convergence proof
% % %   &  &  &  DM
% % %   \\
% % %      \hline
% % %   Do optimal independent consensus convergence proof
% % %   &  &  &  DM
% %   \\ [1ex] 
% %  \hline
% % \end{tabularx}
% % \end{center}



% % \textcolor{blue}{\section{More detail on result 2. @Zhihao, @Kuba}}

% % \textbf{Discussion 16/04/2021 (updated 21/04/2021)}

% % 1. In Swenson's paper they proved that, in one-shot (no states) potential game, there is a mapping between the initialisation of policy and the NE that best response dynamics converges to.

% % 2. It does not apply to our setting, stochastic game (with states), immediately. However, we have a result that provides us with an analogue of the aforementioned potential - the dynamic potential function (DPF).

% % 3. We think that we can extend the Swenson's result to the stochastic game, by using this connection of DPF to potential. 
% % \newline
% % The analogue of policy initialisation would be the choice of the reward-shaping method, which influences DPF.

% % 4. For the above, it would be useful to figure out exactly how to adopt the best response dynamic to stochastic games. The answer may be given in "Fictitious Play in Zero-sum Stochastic Games" by Sayin, et. al.
% % \kuba{Rejection. The paper doesn't help. The result is derived for zero-sum games, and the proof policy heavily relies on the fact that the game is zero-sum.}

% % What we'd like to prove:
% % \begin{theorem}
% % Suppose $\mathcal{G}$ is a stochastic potential game, then for almost every condition, the solutions of \eqref{best_resp_dyn_sg} are unique.
% % \end{theorem}

% % \textcolor{blue}{In Swenson, 2018 is it proven that under best response dynamics, there exists a (deterministic) map between the policy initalisation and the NE point of the game.}

% % \textcolor{blue}{Our aim is to construct a modified version of this result that applies to a stochastic setting. That is, given some initialisation in the policy space, there exists a map between the initialisation and the NE convergence point of a stochastic game which admits a potentiality condition.} 

% % \textcolor{blue}{In the following, we will discuss these notions in a bit more detail.}



% % \textcolor{blue}{
% % A (normal form) game is a potential game if there exists a function $\phi:\boldsymbol{\mathcal{A}}\to\mathbb{R}$ such that the following holds for any 
% % $(a^i,a^{-i}),(a'^i,a^{-i})\in\boldsymbol{\mathcal{A}}$ where $a^{-i}_{t}:=(a^1,\ldots a^{i-1},a^{i+1},\ldots, a^N)$, $\forall i\in\mathcal{N}, \forall s\in \mathcal{S}$:
% % \begin{align}
% % r_i(a^i,a^{-i})- r_i(a'^i,a^{-i})
% % =\phi(a^i,a^{-i})-\phi(a'^i,a^{-i}). \label{potential_condition_static}
% % \end{align}}
% % \textcolor{blue}{The Nash equilibria (NE) of potential games are characteresied by the local optima of the function $\phi$. In general, PGs have multiple NE. }
% % \textcolor{blue}{
% % In Swenson, 2018 it is proven that there is a map between the policy initialisation and the NE (so each initialisation of the policy has associated to it a single NE to which any best-response algorithm will converge). We wish to extend this result to our setting of a stochastic game.}

% % \textcolor{blue}{
% % In stochastic games (SGs) we must augment the reward function to accommodate a state. The corresponding notion of a potential game can be Incorporated within an SG. Our notion of a stochastic potential game requires each stage game to be a potential game (recall an SG is a sequence of stage games played at each round). Therefore we say that an SG is a c-SPG if for all states there exists a function $\phi:\mathcal{S}\times\boldsymbol{\mathcal{A}}\to\mathbb{R}$ such that the following holds for any 
% % $(a^i,a^{-i}),(a'^i,a^{-i})\in\boldsymbol{\mathcal{A}},\forall i\in\mathcal{N}, \forall s\in \mathcal{S}$:
% % \begin{align}
% % r_i(s,(a^i,a^{-i}))- r_i(s,(a'^i,a^{-i}))
% % =\phi(s,(a^i,a^{-i}))-\phi(s,(a'^i,a^{-i})). \label{potential_condition_static_state}
% % \end{align}}
% % \textcolor{blue}{
% % Using this, we have shown that a stochastic potential game obeys a potential-like condition at the level of the value functions --- that is if each stage game satisfies the potential condition XX then we have:
% % \begin{align}
% % \mathbb{E}_{s\sim P(\cdot|)}\left[ v^{\boldsymbol{\pi}}_i(s)-v^{\boldsymbol{\pi'}}_i(s)
% % \right]=\mathbb{E}_{s\sim P(\cdot|)}\left[B^{\boldsymbol{\pi},g}(s)-B^{\boldsymbol{\pi'},g}(s)\right].\label{potential_relation_proof}
% % \end{align}
% % where $\boldsymbol{\pi}\equiv (\pi^1,\ldots,\pi^i,\ldots,\pi^N)$ and $\boldsymbol{\pi'}\equiv (\pi^1,\ldots,\pi'^i,\ldots,\pi^N)$.}

% % \textcolor{blue}{
% % The proof is done by induction starting with finite length games, firstly showing that SGs of length $T$ obey \eqref{potential_relation_proof} at the $T-1^{th}$ step and then performing an inductive step. Thereafter the proof is extended to infinite length games ($T\to\infty$) using a continuity at infinity argument.}

% % \textcolor{blue}{Interestingly, the Nash equilibrium solution of the SG can by found using dynamic programming/value iterative methods:}

% % \textcolor{blue}{
% % \begin{align}
% % B_k(s)\gets {\rm solve}\left[\phi(s,\boldsymbol{a})+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,\boldsymbol{a})B_{k-1}(s')\right]\label{iteration_step}
% % \end{align}
% % so that each round the a game which has a payoff matrix defined by $\phi(s,\boldsymbol{a})+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,\boldsymbol{a})B_{k-1}(s')$ is solved. Each round of this iteration therefore involves solving a potential game. The game can be solved using a best-response method and/or other standard game theory methods.}

% % \textcolor{blue}{
% % This observation may be helpful; \eqref{iteration_step} at each stage the agents are solving a potential game which can be solved by standard best-response algorithms. The result in Swenson, 2018 says that under these conditions, the initialisation of the best-response algorithm determines which solution we end up at. Now our hypothesis is that by using this arguments recursively we can back-propagate the line of reasoning to the initialisation of the best-response algorithm to state $0$ (which is the initialisation of the policy).  }  

% % \zhihao{Could we deduce that each state game's (e.g. state  $s_k$) solution is depended on initialisation because it is a potential game, and each state game's initialisation is depended on previous stage's solution (e.g. state $s_{k-1}$), so the solution of $s_k$ depends on the initialisation $s_0$?}

% % \kuba{What do you mean by initialisation of $s_{0}$? If what you mean has something to do with initial state distribution, then that would likely not lead us to the desired result, as we want to construct a map (RS - to - NE), which does not have much to do with the distribution. It seems to me that binding performance at state $s_{k}$ to $s_{0}$ is unnecessary, as ultimately, the overall performance is the one that starts in $s_{0}$ anyway. Did I misunderstand something?}

% % \zhihao{rep Kuba: I think David's idea is RS can change the initialisation of the start state, so we design a better RS can lead to a better convergence point (as initialisation determines the unique solution of our game by the theorem we want to prove) }

% % \textcolor{blue}{\subsection*{Note on best-response dynamics}}
% % \textcolor{blue}{
% % Best-response algorithms (by construction) induce the following dynamics on the joint strategies of the agents:
% % \begin{align}
% %     \boldsymbol{\dot{x}}\in \rm{BR}(\boldsymbol{x})-\boldsymbol{x}
% % \end{align}
% % where $\rm{BR}$ denotes the best-response joint action. The set of NE are stable points of BR dynamics.}

% % \textcolor{blue}{In Leslie et. al, 2020. A corresponding notion of best-response dynamics for SGs is described --- see equation 3.1. This notion now includes an update on state dependent policy and continuation payoff (this is the expected future stream of rewards the agents receive under their current policy).}



% % \begin{algorithm}[t!]
% % \begin{algorithmic}[1]
% % 	\caption{\textbf{M}ulti-agent \textbf{A}daptive \textbf{R}einforcement-Learning \textbf{C}entralised \textbf{O}ptimiser  (LIGS)}
% % 	\label{algo:Opt_reward_shap_psuedo} 
% % % 	\begin{algorithmic}[1]
% % % 
% % 		\FOR{$N$}
% % 		    \FOR{$N_{steps}$}
% %     		    \STATE At time $t=0,1,\ldots$ each agent $i\in 1,\ldots N$ applies $a^i_t$ to the environment which returns $s_{t+1},  r_{t+1}$
% %     		    \STATE Evaluate $\mathfrak{g}_c(s_{t})$ according to Equation \eqref{G_2_obstacle_expression}
% %     		    \STATE Whenever $ \mathfrak{g}_c(s_{t})=m$ for some $m\in M$, {\fontfamily{cmss}\selectfont Generator} samples an action $\theta^c_{t+1}\sim f_m(\cdot|s_{t+1
% %     		    })$
% %     		    \STATE {\fontfamily{cmss}\selectfont Generator} computes $F(\cdot)$ given $s_{t+1},\theta^c_{t+1},\theta^c_{t=\tau_k}$ for some $k$
% %     		    \STATE Reward $\hat{R}=R+F$ is computed
% %     		    \STATE If the switch is off (which occurs according to Equation \eqref{option_termination_criterion}) then sample $\mathfrak{g}_c$
% % 		    \ENDFOR
% % 		    \STATE{\textbf{// Learn the individual policies}}
% % 	        \STATE Update the value function
% % 	        \STATE Update policies of agent i and {\fontfamily{cmss}\selectfont Generator} via the value function in 11.
% % 	       % \STATE Optimise $G_2(m|\cdot)$ according to objective \ref{P2_obj}
% %          \ENDFOR
% % \end{algorithmic}         
% % \end{algorithm}


% %
% % \begin{itemize}
% %     \item \textbf{Description of scenarios/applications of the framework:} similar to what we did with the single agent RS environment. 
    
% %         \textbf{So far}
% %     \begin{itemize}
% %         \item MARL coordination in outcomes (social welfare)
% %         \item MARL coordinated exploration
% %         \item Others?
% %     \end{itemize} 
% % \end{itemize}
% % \begin{itemize}
% %     \item \textbf{Experiment 1:}  Batch of maze environments testing: \textbf{1} joint coordination (to achieve socially optimal outcome). \textbf{2}. joint exploration. \textbf{3}. trap experiment for exploration-based heuristic (as in our single agent reward-shaping paper). \textbf{4} safe goal and optimal goal experiment
% %     \item \textbf{Experiment 2:} Large scale multi-agent team game which requires coordination e.g. StarCraft
% %     \item \textbf{Experiment 3:} Complex multi-agent environment with sparse rewards e.g. Google football
% %     \item \textbf{Ablation study 1:} \textit{Adaptivity}. when MARL agents use different types of policies (e.g. more stochastic) the framework adapts 
% %     \item \textbf{Ablation study 2:} \textit{Switching control ablation}. As in our single-agent submission
% %     \item \textbf{Ablation study 3:} \textit{Functional form of shaping reward ablation}
% % \end{itemize}



% % \begin{figure}
% %     \centering
% %     \includegraphics[width=0.5\textwidth]{Figures/Studies/RND_study.png}
% %     \caption{RND STUDY}
% %     \label{fig:my_label}
% % \end{figure}

% % \begin{figure}
% %     \centering
% %     \includegraphics{Figures/Studies/F_study.png}
% %     \caption{F Study: Include F in loss (top) or don't (bottom)}
% %     \label{fig:my_label}
% % \end{figure}

% % \begin{figure}
% %     \centering
% %     \includegraphics{Figures/Studies/V_study.png}
% %     \caption{V-study: Loss fn includes V (top), or doesn't (bottom)}
% %     \label{fig:my_label}
% % \end{figure}

% % \begin{figure}
% %     \centering
% %     \includegraphics{Figures/Studies/timestep_study.png}
% %     \caption{Time-step study: Use observation for L-term (top) or next-observation (bottom)}
% %     \label{fig:my_label}
% % \end{figure}

% % \begin{figure}
% %     \centering
% %     \includegraphics[width=\textwidth]{Figures/Studies/switch_cost_study.png}
% %     \caption{Switch cost study}
% %     \label{fig:my_label}
% % \end{figure}
\clearpage
\section{Notation \& Assumptions}\label{sec:notation_appendix}


We assume that $\mathcal{S}$ is defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and any $s\in\mathcal{S}$ is measurable with respect
to the Borel $\sigma$-algebra associated with $\mathbb{R}^p$. We denote the $\sigma$-algebra of events generated by $\{s_t\}_{t\geq 0}$
by $\mathcal{F}_t\subset \mathcal{F}$. In what follows, we denote by $\left( \mathcal{V},\|\|\right)$ any finite normed vector space and by $\mathcal{H}$ the set of all measurable functions.  Where it will not cause confusion (and with a minor abuse of notation) for a given function $h$ we use the shorthand $h^{(\pi^{i},\pi^{-i})}(s)= h(s,\pi^i,\pi^{-i})\equiv\mathbb{E}_{\pi^i,\pi^{-i}}[h(s,a^i,a^{-i})]$.


The results of the paper are built under the following assumptions which are standard within RL and stochastic approximation methods:

\textbf{Assumption 1}
The stochastic process governing the system dynamics is ergodic, that is  the process is stationary and every invariant random variable of $\{s_t\}_{t\geq 0}$ is equal to a constant with probability $1$.


\textbf{Assumption 2}
The constituent functions of the agents' objectives $R$, $F$ and $L$ are in $L_2$.

\textbf{Assumption 3}
For any positive scalar $c$, there exists a scalar $\mu_c$ such that for all $s\in\mathcal{S}$ and for any $t\in\mathbb{N}$ we have: $
    \mathbb{E}\left[1+\|s_t\|^c|s_0=s\right]\leq \mu_c(1+\|s\|^c)$.

\textbf{Assumption 4}
There exists scalars $C_1$ and $c_1$ such that for any function $J$ satisfying $|J(s)|\leq C_2(1+\|s\|^{c_2})$ for some scalars $c_2$ and $C_2$ we have that: $
    \sum_{t=0}^\infty\left|\mathbb{E}\left[J(s_t)|s_0=s\right]-\mathbb{E}[J(s_0)]\right|\leq C_1C_2(1+\|s_t\|^{c_1c_2})$.

\textbf{Assumption 5}
There exists scalars $c$ and $C$ such that for any $s\in\mathcal{S}$ we have that: $
    |J(s,\cdot)|\leq C(1+\|s\|^c)$ for $J\in \{r_i,F,L\}$.

We also make the following finiteness assumption on set of switching control policies for {\fontfamily{cmss}\selectfont Generator}:

\textbf{Assumption 6}
For any policy $\mathfrak{g}_c$, the total number of interventions is $K<\infty$.

We lastly make the following assumption on $L$ which can be made true by construction:

\textbf{Assumption 7}
Let $n(s)$ be the state visitation count for a given state $s\in\mathcal{S}$. For any $\boldsymbol{a}\in\boldsymbol{\mathcal{A}}$, the function $L(s,\boldsymbol{a})= 0$ for any $n(s)\geq M$ where $0<M\leq \infty$.

\clearpage
\section{Proof of Technical Results}\label{sec:proofs_appendix}


We begin the analysis with some preliminary lemmata and definitions which are useful for proving the main results.

\begin{definition}{A.1}
An operator $T: \mathcal{V}\to \mathcal{V}$ is said to be a \textbf{contraction} w.r.t a norm $\|\cdot\|$ if there exists a constant $c\in[0,1[$ such that for any $V_1,V_2\in  \mathcal{V}$ we have that:
\begin{align}
    \|TV_1-TV_2\|\leq c\|V_1-V_2\|.
\end{align}
\end{definition}

\begin{definition}{A.2}
An operator $T: \mathcal{V}\to  \mathcal{V}$ is \textbf{non-expansive} if $\forall V_1,V_2\in  \mathcal{V}$ we have:
\begin{align}
    \|TV_1-TV_2\|\leq \|V_1-V_2\|.
\end{align}
\end{definition}
% \begin{definition}{A.3}
% The \textbf{residual} of a vector $V\in \mathcal{V}$ w.r.t the operator $T: \mathcal{V}\to  \mathcal{V}$ is:
% \begin{align}
%     \epsilon_T(V):= \|TV-V\|.
% \end{align}
% \end{definition}




\begin{lemma} \label{max_lemma}
For any
$f: \mathcal{V}\to\mathbb{R},g: \mathcal{V}\to\mathbb{R}$, we have that:
\begin{align}
\left\|\underset{a\in \mathcal{V}}{\max}\:f(a)-\underset{a\in \mathcal{V}}{\max}\: g(a)\right\| \leq \underset{a\in \mathcal{V}}{\max}\: \left\|f(a)-g(a)\right\|.    \label{lemma_1_basic_max_ineq}
\end{align}
\end{lemma}
\begin{proof}
We restate the proof given in \cite{mguni2019cutting}:
\begin{align}
f(a)&\leq \left\|f(a)-g(a)\right\|+g(a)\label{max_inequality_proof_start}
\\\implies
\underset{a\in \mathcal{V}}{\max}f(a)&\leq \underset{a\in \mathcal{V}}{\max}\{\left\|f(a)-g(a)\right\|+g(a)\}
\leq \underset{a\in \mathcal{V}}{\max}\left\|f(a)-g(a)\right\|+\underset{a\in \mathcal{V}}{\max}\;g(a). \label{max_inequality}
\end{align}
Deducting $\underset{a\in \mathcal{V}}{\max}\;g(a)$ from both sides of (\ref{max_inequality}) yields:
\begin{align}
    \underset{a\in \mathcal{V}}{\max}f(a)-\underset{a\in \mathcal{V}}{\max}g(a)\leq \underset{a\in \mathcal{V}}{\max}\left\|f(a)-g(a)\right\|.\label{max_inequality_result_last}
\end{align}
After reversing the roles of $f$ and $g$ and redoing steps (\ref{max_inequality_proof_start}) - (\ref{max_inequality}), we deduce the desired result since the RHS of (\ref{max_inequality_result_last}) is unchanged.
\end{proof}

\begin{lemma}{A.4}\label{non_expansive_P}
The probability transition kernel $P$ is non-expansive, that is:
\begin{align}
    \|PV_1-PV_2\|\leq \|V_1-V_2\|.
\end{align}
\end{lemma} 
\begin{proof}
The result is well-known e.g. \cite{tsitsiklis1999optimal}. We give a proof using the Tonelli-Fubini theorem and the iterated law of expectations, we have that:
\begin{align*}
&\|PJ\|^2=\mathbb{E}\left[(PJ)^2[s_0]\right]=\mathbb{E}\left(\left[\mathbb{E}\left[J[s_1]|s_0\right]\right)^2\right]
\leq \mathbb{E}\left[\mathbb{E}\left[J^2[s_1]|s_0\right]\right] 
= \mathbb{E}\left[J^2[s_1]\right]=\|J\|^2,
\end{align*}
where we have used Jensen's inequality to generate the inequality. This completes the proof.
\end{proof}





\section*{Proof of Prop. \ref{preservation_lemma}}
\begin{proof}[Proof of Prop. \ref{preservation_lemma}]
To prove the proposition it suffices to prove that the term $\sum_{t=0}^T\gamma^{t}F(\theta_t,\theta_{t-1})I(t)$ converges to $0$ in the limit as $T\to \infty$. As in classic potential-based reward shaping \cite{ng1999policy}, central to this observation is the telescoping sum that emerges by construction of $F$.

First recall $v^{\boldsymbol{\pi},g}(s,I_0)$, for any $(s,I_0)\in\mathcal{S}\times\{0,1\}$ is given by:
\begin{align}
&v^{\boldsymbol{\pi},g}_i(s,I_0)=\mathbb{E}_{\boldsymbol{\pi},g}\left[\sum_{t=0}^\infty \gamma^t\left\{R(s_t,\boldsymbol{a}_t)+\hat{F}(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1})I_t\right\}\right]
\\&=\mathbb{E}_{\boldsymbol{\pi},g}\left[\sum_{t=0}^\infty \gamma^tR(s_t,\boldsymbol{a}_t)+\sum_{t=0}^\infty \gamma^t\hat{F}(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1})I_t\right]
\\&=\mathbb{E}_{\boldsymbol{\pi},g}\left[\sum_{t=0}^\infty \gamma^tR(s_t,\boldsymbol{a}_t)\right]+\mathbb{E}_{\boldsymbol{\pi},g}\left[\sum_{t=0}^\infty \gamma^t\hat{F}(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1}))I_t\right]
\end{align}

Hence it suffices to prove that $\mathbb{E}_{\boldsymbol{\pi},g}\left[\sum_{t=0}^\infty \gamma^t\hat{F}(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1}))I_t\right]=0$.

Recall there a number of time steps that elapse between $\tau_k$ and $\tau_{k+1}$, now
  
\begin{align*}
&\sum_{t=0}^\infty\gamma^{t}\hat{F}(s_t,\theta^c_t;s_{t-1},\theta^c_{t-1}))I(t)
\\&=\sum_{t=\tau_1}^{\tau_2}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1}) +\sum_{t=\tau_3}^{\tau_4}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})
\\&\quad+\ldots+ \sum_{t=\tau_{(2k-1)}}^{\tau_{2k
}}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})+\ldots+
% 
% 
% 
\\&=\sum_{t=\tau_1+1}^{\tau_2}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})+\gamma^{\tau_1}\phi(s_{\tau_1},\theta^c_{\tau_1})
\\&\quad +\sum_{t=\tau_3+1}^{\tau_4}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})+\gamma^{\tau_3}\phi(s_{\tau_3},\theta^c_{\tau_3})
\\&\quad+\ldots+ \sum_{t=\tau_{(2k-1)}+1}^{\tau_{2k
}}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})+\gamma^{2k+1}\phi(s_{\tau_{2k+1}},\theta^c_{\tau_{2k+1}})+\ldots+
% 
% 
% 
\\&=\sum_{t=\tau_1}^{\tau_2-1}\gamma^{t+1}\phi(s_{t+1},\theta^c_{t+1})-\gamma^{t}\phi(s_t,\theta^c_{t})+\gamma^{\tau_1}\phi(s_{\tau_1},\theta^c_{\tau_1})
\\&+\sum_{t=\tau_3}^{\tau_4-1}\gamma^{t+1}\phi(s_{t+1},\theta^c_{t+1})-\gamma^{t}\phi(s_t,\theta^c_{t})+\gamma^{\tau_3}\phi(s_{\tau_3},\theta^c_{\tau_3})
\\&\quad+\ldots+ \sum_{t=\tau_{(2k-1)}}^{\tau_{2K-1}}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})+\gamma^{\tau_{2k-1}}\phi(s_{\tau_{2k-1}},\theta^c_{\tau_{2k-1}})+\ldots+
% 
% 
% 
\\&=\sum_{k=1}^\infty\sum_{t=\tau_{2k-1}}^{\tau_{2K-1}}\gamma^{t+1}\phi(s_{t+1},\theta^c_{t+1})-\gamma^{t}\phi(s_t,\theta^c_{t})-\sum_{k=1}^\infty\gamma^{\tau_{2k-1}}\phi(s_{\tau_{2k-1}},\theta^c_{\tau_{2k-1}})
\\&=\sum_{k=1}^\infty\gamma^{\tau_{2k}}\phi(s_{\tau_{2k}},\theta^c_{\tau_{2k}})-\sum_{k=1}^\infty\gamma^{\tau_{2k-1}}\phi(s_{\tau_{2k-1}},\theta^c_{\tau_{2k-1}})
\\&=\sum_{k=1}^\infty\gamma^{\tau_{2k}}\phi(s_{\tau_{2k}},0)-\sum_{k=1}^\infty\gamma^{\tau_{2k-1}}\phi(s_{\tau_{2k-1}},0)=0
\end{align*}
where we have used the fact that by construction  $\theta^c_t\equiv 0$ whenever $t=\tau_1,\tau_2,\ldots$ and by construction $\phi(s,0)\equiv 0$ for any $s$. 

% The summation performed above for any $\tau_k$'s will lead to the same result and everywhere else $\theta^c_t=0$.

With this we readily deduce that $v^{\boldsymbol{\pi},g}(s)=\mathbb{E}_{\boldsymbol{\pi},g}\left[\sum_{t=0}^\infty \gamma^tR(s_t,\boldsymbol{a}_t)\right]$ which is a measure of environment rewards only
from which statement (i) can be readily deduced.

For part (ii) we  note first that it is easy to see that $v^{\boldsymbol{\pi},g}_c(s_0,I_0)$ is bounded above, indeed using the key result in the proof of part (i) and the properties of $c$ we have that

\begin{align}
v^{\boldsymbol{\pi},g}_c(s_0,I_0)&=\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}} +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
+L_n(s_t,\boldsymbol{a}_t)\right)\right]
\\&=\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
+L_n(s_t,\boldsymbol{a}_t)\right)+\sum_{t=0}^\infty \gamma^tF^{\boldsymbol{\theta}}I_t\right]
\\&\leq \mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R +L_n(s_t,\boldsymbol{a}_t)\right)\right]
\\&\leq \left|\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R+L_n(s_t,\boldsymbol{a}_t)\right)\right]\right|
\\&\leq \mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left\|R +
L_n\right\|\right]
\\&\leq  \sum_{t=0}^\infty \gamma^t\left(\left\|R\right\| +\left\|
L_n\right\|\right)
\\&=\frac{1}{1-\gamma}\left(\left\|R\right\| +\left\|
L_n\right\|\right)
\end{align}
using the triangle inequality, the definition of $R^{\boldsymbol{\theta}}$ and the (upper-)boundedness of $L$ and $R$ (Assumption 5).
We now note that by the dominated convergence theorem we have that $\forall (s_0,I_0)\in\mathcal{S}\times\{0,1\}$
\begin{align}
&\underset{n\to \infty}{\lim}\; v^{\boldsymbol{\pi},g}_c(s_0,I_0)  = \underset{n\to \infty}{\lim}\; \mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}} +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
+L_n(s_t,{a}_t)\right)\right]
\\&=\mathbb{E}_{\boldsymbol{\pi},g}\underset{n\to \infty}{\lim}\;\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}} +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
+L_n(s_t,{a}_t)\right)\right]
\\&=\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}} +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}\right)\right]
\\&=\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}\right)\right]=c\frac{K}{1-\gamma}+v^{\boldsymbol{\pi}}(s_0),
\end{align}
where $K$ is the total number of switch activations or \textit{interventions}  and where again we have used the key result in the proof of (i) and Assumption 6 in the last step, after which we deduce (ii) since $v^{\boldsymbol{\pi}}$ and $v^{\boldsymbol{\pi},g}_c$ differ by only a constant.

Note that by (ii) we heron may consider the quantity for the {\fontfamily{cmss}\selectfont Generator} expected return:
\begin{align}
    \hat{v}^{\boldsymbol{\pi},g}_c(s_0,I_0)=\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}\right)\right].
\end{align}

\end{proof}




% In general $\theta^c_t$ need not be $0$ but similar to the above, in our setup the intermediate terms $\gamma\phi(s_t,\theta^c_t)-\phi(s_{t-1},\theta^c_{t-1})$ when $\theta^c_t\neq 0$ will cancel when we perform the summation  so we only need to care about the terms at the end points. For the sake of completion we demonstrate this point:

% $\sum_{t=\tau_{k+1}}^{\tau_{k+1}}\gamma^{t}\phi(s_t,\theta^c_t)-\gamma^{t-1}\phi(s_{t-1},\theta^c_{t-1})$
% $=\left(\gamma^{\tau_{k}+1}\phi(s_{\tau_{k}+1},\theta^c_{\tau_{k}+1})-\gamma^{\tau_k}\phi(s_{\tau_k},\theta^c_{\tau_k})\right)+\left(\gamma^{\tau_{k}+2}\phi(s_{\tau_{k}+2},\theta^c_{\tau_{k}+2})-\gamma^{\tau_{k}+1}\phi(s_{\tau_{k}+1},\theta^c_{\tau_{k}+1})\right)+\ldots$
% $=\gamma^{\tau_{k+1}}\phi(s_{\tau_{k+1}},\theta^c_{\tau_{k+1}})-\gamma^{\tau_k}\phi(s_{\tau_k},\theta^c_{\tau_k})$


% Therefore for any $s$, $v^{\boldsymbol{\pi},g}_1(s)\geq v^{\pi}_1(s)$ indeed represents a performance improvement.  

\section*{Proof of Theorem \ref{theorem:existence_2}}
\begin{proof}
Theorem \ref{theorem:existence_2} is proved by firstly showing that when the agents jointly maximise the same objective there exists a fixed point equilibrium of the game when all agents use Markov policies and {\fontfamily{cmss}\selectfont Generator} uses switching control. The proof then proceeds by showing that the MG $\mathcal{G}$ admits a dual representation as an MG in which all agents $\mathcal{N}\times\{c\}$ jointly maximise the same objective which has MPE that can be computed by solving an MDP. Thereafter, we use both results to prove the existence of a fixed point for the game as a limit point of a sequence generated by successively applying the Bellman operator to a test function.  

Therefore, the scheme of the proof is summarised with the following steps:
\begin{itemize}
    \item[\textbf{I)}] Prove that the solution to Markov Team games (that is games in which all agents maximise an \textit{identical objective}) in which one of the agents uses switching control is the limit point of a sequence of Bellman operators (acting on some test function).
    \item[\textbf{II)}] Prove that for the MG $\mathcal{G}$ that is there exists a function $B^{\boldsymbol{\pi},g}:\mathcal{S}\times \{0,1\}\to \mathbb{R}$ such that\footnote{This property is analogous to  the condition in Markov potential games \cite{macua2018learning,mguni2021learning}} $
 v^{\boldsymbol{\pi},g}_i(z)-v^{\boldsymbol{\pi'},g}_i(z)
=B^{\boldsymbol{\pi},g}(z)-B^{\boldsymbol{\pi'},g}(z),\;\;\forall z\equiv (s,I_0)\in\mathcal{S}\times \{0,1\}, \forall i\in\mathcal{N}\times\{0\}$.
    \item[\textbf{III)}] Prove that the MG, $\mathcal{G}$ has a dual representation as a \textit{Markov Team Game} which admits a representation as an MDP.
\end{itemize}





\subsection*{Proof of Part \textbf{I}}


Our first result proves that the operator  $T$ is a contraction operator. First let us recall that the \textit{switching time} $\tau_k$ is defined recursively $\tau_k=\inf\{t>\tau_{k-1}|s_t\in A,\tau_k\in\mathcal{F}_t\}$ where $A=\{s\in \mathcal{S},m\in M|\mathfrak{g}_c(m|s_t)>0\}$.
To this end, we show that the following bounds holds:
\begin{lemma}\label{lemma:bellman_contraction}
The Bellman operator $T$ is a contraction, that is the following bound holds:
\begin{align*}
&\left\|T\psi-T\psi'\right\|\leq \gamma\left\|\psi-\psi'\right\|.
\end{align*}
\end{lemma}

\begin{proof}
Recall we define the Bellman operator $T_\psi$ of $\mathcal{G}$ acting on a function $\Lambda:\mathcal{S}\times\mathbb{N}\to\mathbb{R}$ by
% 
\begin{align}
T_\psi \Lambda(s_{\tau_k},I(\tau_k)):=\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}\Lambda(s_{\tau_k},I(\tau_k)),\left[ \psi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a},s_{\tau_k})\Lambda(s',I(\tau_k))\right]\right\}\label{bellman_proof_start}
\end{align}

In what follows and for the remainder of the script, we employ the following shorthands:
\begin{align*}
&\mathcal{P}^{\boldsymbol{a}}_{ss'}=:\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a},s), \quad\mathcal{P}^{\boldsymbol{\pi}}_{ss'}=:\sum_{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}|s)\mathcal{P}^{\boldsymbol{a}}_{ss'}, \quad \mathcal{R}^{\boldsymbol{\pi}}(z_{t}):=\sum_{\boldsymbol{a}_t\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_t|s)\hat{R}(z_t,\boldsymbol{a}_t,\theta_t,\theta_{t-1})
\end{align*}

To prove that $T$ is a contraction, we consider the three cases produced by \eqref{bellman_proof_start}, that is to say we prove the following statements:

i) $\qquad\qquad
\left| \Theta(z_t,\boldsymbol{a},\theta^c_t,\theta^c_{t-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_t}\psi(s',\cdot)-\left( \Theta(z_t,\boldsymbol{a},\theta^c_t,\theta^c_{t-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_t}\psi'(s',\cdot)\right)\right|\leq \gamma\left\|\psi-\psi'\right\|$

ii) $\qquad\qquad
\left\|\mathcal{M}^{\boldsymbol{\pi},g}\psi-\mathcal{M}^{\boldsymbol{\pi},g}\psi'\right\|\leq    \gamma\left\|\psi-\psi'\right\|,\qquad \qquad$
  (and hence $\mathcal{M}$ is a contraction).

iii) $\qquad\qquad
    \left\|\mathcal{M}^{\boldsymbol{\pi},g}\psi-\left[ \Theta(\cdot,\boldsymbol{a})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}\psi'\right]\right\|\leq \gamma\left\|\psi-\psi'\right\|.
$
where $z_t\equiv (s_t,I_t)\in\mathcal{S}\times \{0,1\}$.

We begin by proving i).

Indeed, for any $\boldsymbol{a}\in\boldsymbol{\mathcal{A}}$ and $\forall z_t\in\mathcal{S}\times\{0,1\}, \forall \theta_t,\theta_{t-1}\in \Theta, \forall s'\in\mathcal{S}$ we have that 
\begin{align*}
&\left| \Theta(z_t,\boldsymbol{a},\theta^c_t,\theta^c_{t-1})+\gamma\mathcal{P}^\pi_{s's_t}\psi(s',\cdot)-\left[ \Theta(z_t,\boldsymbol{a},\theta^c_t,\theta^c_{t-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\;\mathcal{P}^{\boldsymbol{a}}_{s's_t}\psi'(s',\cdot)\right]\right|
\\&\leq \underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\left|\gamma\mathcal{P}^{\boldsymbol{a}}_{s's_t}\psi(s',\cdot)-\gamma\mathcal{P}^{\boldsymbol{a}}_{s's_t}\psi'(s',\cdot)\right|
\\&\leq \gamma\left\|P\psi-P\psi'\right\|
\\&\leq \gamma\left\|\psi-\psi'\right\|,
\end{align*}
again using the fact that $P$ is non-expansive and Lemma \ref{max_lemma}.

We now prove ii).


For any $\tau\in\mathcal{F}$, define by $\tau'=\inf\{t>\tau|s_t\in A,\tau\in\mathcal{F}_t\}$. Now using the definition of $\mathcal{M}$ we have that for any $s_\tau\in\mathcal{S}$
\begin{align*}
&\left|(\mathcal{M}^{\boldsymbol{\pi},g}\psi-\mathcal{M}^{\boldsymbol{\pi},g}\psi')(s_{\tau},I(\tau))\right|
\\&\leq \underset{\boldsymbol{a}_\tau,\theta^c_\tau,\theta^c_{\tau-1}\in \boldsymbol{\mathcal{A}}\times \Theta^2}{\max}    \Bigg|\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_\tau,\theta^c_{\tau-1})+c(I_\tau,I_{\tau-1})+\gamma\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi(s_{\tau},I(\tau'))
\\&\qquad\qquad-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_\tau,\theta^c_{\tau-1})+c(I_\tau,I_{\tau-1})+\gamma\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi'(s_{\tau},I(\tau'))\right)\Bigg| 
\\&= \gamma\left|\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi(s_{\tau},I(\tau'))-\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi'(s_{\tau},I(\tau'))\right| 
\\&\leq \gamma\left\|P\psi-P\psi'\right\|
\\&\leq \gamma\left\|\psi-\psi'\right\|,
\end{align*}
using the fact that $P$ is non-expansive. The result can then be deduced easily by applying max on both sides.

We now prove iii). We split the proof of the statement into two cases:

\textbf{Case 1:} 
\begin{align}\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\right)<0.
\end{align}

We now observe the following:
\begin{align*}
&\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))-\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))
\\&\leq\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}
\\&\qquad-\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))
\\&\leq \Bigg|\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}
\\&\qquad-\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}
\\&+\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}
\\&\qquad-\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\Bigg|
\\&\leq \Bigg|\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}
\\&\qquad-\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}\Bigg|
\\&\qquad+\Bigg|\max\left\{\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I({\tau})),\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))\right\}\\&\qquad\qquad-\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\Bigg|
\\&\leq \gamma\underset{a\in\mathcal{A}}{\max}\;\left|\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi(s',I(\tau))-\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi'(s',I(\tau))\right|
\\&\qquad+\left|\max\left\{0,\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\right)\right\}\right|
\\&\leq \gamma\left\|P\psi-P\psi'\right\|
\\&\leq \gamma\|\psi-\psi'\|,
\end{align*}
where we have used the fact that for any scalars $a,b,c$ we have that $
    \left|\max\{a,b\}-\max\{b,c\}\right|\leq \left|a-c\right|$ and the non-expansiveness of $P$.



\textbf{Case 2: }
\begin{align*}\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\right)\geq 0.
\end{align*}

For this case, first recall that for any $\tau\in\mathcal{F}$, $-c(I_\tau,I_{\tau-1})>\lambda$ for some $\lambda >0$.
% 
\begin{align*}
&\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\right)
\\&\leq \mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau},I(\tau))-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\right)-c(I_\tau,I_{\tau-1})
\\&\leq \Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+c(I_\tau,I_{\tau-1})+\gamma\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\psi(s',I(\tau'))
\\&\qquad\qquad\qquad\qquad\quad-\left(\Theta(z_\tau,\boldsymbol{a}_\tau,\theta^c_{\tau},\theta^c_{\tau-1})+c(I_\tau,I_{\tau-1})+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}_{s's_\tau}\psi'(s',I(\tau))\right)
\\&\leq \gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\left|\mathcal{P}^{\boldsymbol{\pi}}_{s's_\tau}\mathcal{P}^{\boldsymbol{a}}\left(\psi(s',I(\tau'))-\psi'(s',I(\tau))\right)\right|
\\&\leq \gamma\left|\psi(s',I(\tau'))-\psi'(s',I(\tau))\right|
\\&\leq \gamma\left\|\psi-\psi'\right\|,
\end{align*}
again using the fact that $P$ is non-expansive. Hence we have succeeded in showing that for any $\Lambda\in L_2$ we have that
\begin{align}
    \left\|\mathcal{M}^{\boldsymbol{\pi},g}\Lambda-\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\left[ \psi(\cdot,a)+\gamma\mathcal{P}^{\boldsymbol{a}}\Lambda'\right]\right\|\leq \gamma\left\|\Lambda-\Lambda'\right\|.\label{off_M_bound_gen}
\end{align}
Gathering the results of the three cases gives the desired result. 
\end{proof}
\subsection*{Proof of Part \textbf{II}}

To prove Part \textbf{II}, we prove the following result:
\begin{proposition}\label{dpg_proposition}
For any $\boldsymbol{\pi}\in\boldsymbol{\Pi}$ and for any {\fontfamily{cmss}\selectfont Generator} policy $g$, there exists a function $B^{\boldsymbol{\pi},g}:\mathcal{S}\times \{0,1\}\to \mathbb{R}$ such that
\begin{align}
 v^{\boldsymbol{\pi},g}_i(z)-v^{\boldsymbol{\pi'},g}_i(z)
=B^{\boldsymbol{\pi},g}(z)-B^{\boldsymbol{\pi'},g}(z),\;\;\forall z\equiv (s,I_0)\in\mathcal{S}\times \{0,1\}\label{potential_relation_proof}
\end{align}
where in particular the function $B$ is given by:
\begin{align}
B^{\boldsymbol{\pi},g}(s_0,I_0) =\mathbb{E}_{\boldsymbol{\pi},g}\left[ \sum_{t=0}^\infty \gamma^t\left(R^{\boldsymbol{\theta}} +\sum_{k\geq 1} c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}\right)\right],\end{align}
for any $(s_0,I_0)\in\mathcal{S}\times \{0,1\}$.
\end{proposition}
\begin{proof}


Note that by the deduction of (ii) in Prop \ref{invariance_prop},  we immediately observe that
\begin{align}
    \hat{v}^{\boldsymbol{\pi},g}_c(s_0,I_0)=B^{\boldsymbol{\pi},g}(s_0,I_0), \;\; \forall (s_0,I_0)\in\mathcal{S}\times\{0,1\}.
\end{align}
We therefore immediately deduce that for any two {\fontfamily{cmss}\selectfont Generator} policies $g$ and $g'$ the following expression holds $\forall (s_0,I_0)\in\mathcal{S}\times\{0,1\}$:
\begin{align}
    \hat{v}^{\boldsymbol{\pi},g}_c(s_0,I_0)-\hat{v}^{\boldsymbol{\pi},g'}_c(s_0,I_0)=B^{\boldsymbol{\pi},g}(s_0,I_0)-B^{\boldsymbol{\pi},g'}(s_0,I_0).
\end{align}

Our aim now is to show that the following expression holds $\forall (s_0,I_0)\in\mathcal{S}\times\{0,1\}$:
\begin{align}\nonumber
 v^{\boldsymbol{\pi},g}_{i}(I_{0},s_{0})-v^{\boldsymbol{\pi'},g}_{i}(I_{0},s_{0})=B^{\boldsymbol{\pi},g}(I_{0},s_{0})-B^{\boldsymbol{\pi'},g}(I_{0},s_{0}),\;\; \forall i \in\mathcal{N}
\end{align}
For the finite horizon case, the result is proven by induction on the number of time steps until the end of the game. Unlike the infinite horizon case, for the finite horizon case the value function and policy have an explicit time dependence. 


We consider the case of the proposition at time $ T-1$ that is we evaluate the value functions at the penultimate time step. In this case, we have that:

% Consider the case when the first deviation occurs at $T-1$ that is there is a deviation by some agent $i\in\mathcal{N}$ from policy $\pi_i\in\Pi_i$ to some other policy, say $\pi'_i\in\Pi_i$, then for any $\theta\in\Theta$ and for any $s\in S$ and for all $i \in\mathcal{N}$ we have that:
\begin{align}\nonumber
&\mathbb{E}_{s_{T-1}\sim d_\theta}\left[B^{\boldsymbol{\pi},g}_{T-1}(I_{T-1},s_{T-1})-B^{\boldsymbol{\pi'},g}_{T-1}(I_{T-1},s_{T-1})\right]
% \\&\nonumber\begin{aligned}=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})
% \phi(s_{T-1},\boldsymbol{a}_{T-1})
% \\&-\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})\phi(s_{T-1},\boldsymbol{a'}_{T-1})
% \\&+\gamma\sum_{s_{T}\in S}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})B^{\boldsymbol{\pi},g}_{T}(I_{T},s_{T})
% \\&-
% \gamma\sum_{s_{T}\in S}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})B^{\boldsymbol{\pi'},g}_{T}(I_{T},s_{T})\Bigg]
% \end{aligned}
\\&\begin{aligned}=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})\left[
R(s_{T-1},\boldsymbol{a}_{T-1})+\sum_{k\geq 0}\sum_{j= T-1}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]
\\&+\gamma\sum_{s_T\in S}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1}) P(s_T;\boldsymbol{a}_{T-1})B^{\boldsymbol{\pi},g}_{T}(I_{T},s_{T})
\\&\nonumber-\Bigg(
\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})\left[
R(s_{T-1},\boldsymbol{a'}_{T-1})+\sum_{k\geq 0}\sum_{j= T-1}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]
\\&+\gamma\sum_{s_T\in S}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1}) P(s_T;\boldsymbol{a'}_{T-1})B^{\boldsymbol{\pi'},g}_{T}(I_{T},s_{T})\Bigg)\Bigg]
\end{aligned}
% \\&\begin{aligned}=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})
% \phi(s_{T-1},\boldsymbol{a}_{T-1})
% \\&-\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})\phi(s_{T-1},\boldsymbol{a'}_{T-1})
% \\&+\gamma\sum_{s_{T}\in S}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})v_i^{(\pi^i,\pi^{-i})}(x_T)
% \\&-
% \gamma\sum_{s_{T}\in S}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})v_i^{\pi^{i},\pi^{-i}}(x_T)\Bigg].
% \end{aligned}
\\&\begin{aligned}=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})R(s_{T-1},\boldsymbol{a}_{T-1})
\\&-\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})R(s_{T-1},\boldsymbol{a'}_{T-1})
\\&+\gamma \Bigg[\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})B^{\boldsymbol{\pi},g}_{T}(I_{T},s_{T})
\\&-
\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})B^{\boldsymbol{\pi'},g}_{T}(I_{T},s_{T})\Bigg]\Bigg].
\end{aligned}
\end{align}
We now observe that for any $\boldsymbol{\pi}\in\boldsymbol{\Pi}$ and for any $g$ we have that $B^{\boldsymbol{\pi},g}_{T}(I_{T},s_{T})=\mathbb{E}\left[\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})\left[
R(s_{T},\boldsymbol{a}_{T})+\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]\right]$, moreover we have that for any $\boldsymbol{\pi}\in\boldsymbol{\Pi}$ and for any $g$ 
\begin{align*}
&\begin{aligned}
\mathbb{E}\Bigg[\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})&\left[
R(s_{T},\boldsymbol{a}_{T})+\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]
\\&-\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a}_{T};s_{T})\left[
R(s_{T},\boldsymbol{a'}_{T})+\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]\Bigg]
\end{aligned}
\\&\begin{aligned}
=&\mathbb{E}\left[\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})
R(s_{T},\boldsymbol{a}_{T})-\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})
R(s_{T},\boldsymbol{a}_{T})\right]
\\&+\mathbb{E}\left[\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}-\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]
\end{aligned}
\end{align*}
Hence we find that
\begin{align}\nonumber
&\mathbb{E}_{s_{T-1}\sim d_\theta}\left[B^{\boldsymbol{\pi},g}_{T-1}(I_{T-1},s_{T-1})-B^{\boldsymbol{\pi'},g}_{T-1}(I_{T-1},s_{T-1})\right]
\\&\begin{aligned}=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})R(s_{T-1},\boldsymbol{a}_{T-1})
\\&-\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})R(s_{T-1},\boldsymbol{a'}_{T-1})
\end{aligned}
\\&+\gamma \Bigg(\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})
R(s_{T},\boldsymbol{a}_{T})
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})
R(s_{T},\boldsymbol{a}_{T})
\\&+\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\sum_{k\geq 0}\sum_{j= T}^\infty\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T}) c(I_j,I_{j-1})\delta^j_{\tau_k}
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\sum_{k\geq 0}\sum_{j= T}^\infty\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T}) c(I_j,I_{j-1})\delta^j_{\tau_k}\Bigg)\Bigg]
\end{align}
Now
\begin{align}\nonumber
&\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\sum_{k\geq 0}\sum_{j= T}^\infty\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T}) c(I_j,I_{j-1})\delta^j_{\tau_k}
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\sum_{k\geq 0}\sum_{j= T}^\infty\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T}) c(I_j,I_{j-1})\delta^j_{\tau_k}\Bigg]\nonumber
\\&=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T}) \sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\Bigg]
\\&=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T}) \sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})\sum_{k\geq 0}\sum_{j= T}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\Bigg]
\\&=K\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})\Bigg]
\\&=K\left(\sum_{\boldsymbol{a}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T}) -\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T})\right) =0
\end{align}
using Assumption 6.

Hence we find that
\begin{align}\nonumber
&\mathbb{E}_{s_{T-1}\sim d_\theta}\left[B^{\boldsymbol{\pi},g}_{T-1}(I_{T-1},s_{T-1})-B^{\boldsymbol{\pi'},g}_{T-1}(I_{T-1},s_{T-1})\right]
\\&\begin{aligned}=\mathbb{E}_{s_{T-1}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})R(s_{T-1},\boldsymbol{a}_{T-1})
\\&-\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})R(s_{T-1},\boldsymbol{a'}_{T-1})
\end{aligned}
\\&+\gamma \Bigg(\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T}\boldsymbol{\pi}(\boldsymbol{a}_{T-1};s_{T-1})P(s_T;\boldsymbol{a}_{T-1})\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T};s_{T})
R(s_{T},\boldsymbol{a}_{T})
\\&-\sum_{s_T\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-1}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-1};s_{T-1})P(s_T;\boldsymbol{a'}_{T-1})\boldsymbol{\pi'}(\boldsymbol{a'}_{T};s_{T})
R(s_{T},\boldsymbol{a}_{T})\Bigg)\Bigg]
\\&=\mathbb{E}_{s_{T-1}\sim d_\theta}\left[v^{\boldsymbol{\pi},g}_{i,T-1}(s_{T-1})-v^{\boldsymbol{\pi'},g}_{i,T-1}(s_{T-1})\right]
\end{align}

% using the iterated law of expectations in the last line and where
% \begin{align}
% B^{(\pi_i,\pi_{-i})}_T(s):=\mathbb{E}_{s_t\sim P,\pi_i,\pi_{-i}}\left[\sum_{t=0}^T\gamma^t\phi(s_t,\boldsymbol{a}_t)|s\equiv s_0]\right] \label{expression_for_B}.
% \end{align} 

Hence, we have succeeded in proving that the expression (\ref{potential_relation_proof}) holds for $T-k$ when $k=1$.

Our next goal is to prove that the expression holds for any $0<k\leq T$.

\noindent Note that for any $T \geq k> 0$, we can write $B^{\boldsymbol{\pi},g}_{T-k}$ as 
\begin{align}
B^{\boldsymbol{\pi},g}_{T-k}(I_0,s_0)=\mathbb{E}_{\boldsymbol{\pi}}\Bigg[R(s_{T-k},\boldsymbol{a}_{T-k})&+\sum_{k\geq 0}\sum_{j= T-j}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}
\\&+\gamma\sum_{s_{k+1}\in S} P(s';s_{T-k},\boldsymbol{a}_{T-k})B^{\boldsymbol{\pi},g}_{T-(k+1)}(I_{T-(k+1)},s_{T-(k+1)})\Bigg].
\end{align}

Now we consider the case when we evaluate the expression (\ref{potential_relation_proof}) for any $0<k\leq T$. Our inductive hypothesis is the the expression holds for some $0<k\leq T$, that is for any $0<k\leq T$ we have that:
\begin{align}\nonumber 
 &\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})v^{\boldsymbol{\pi},g}_{i,T-k}(I_{T-k},s_{T-k})
\\&-
\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})v^{\boldsymbol{\pi'},g}_{i,T-k}(I_{T-k},s_{T-k})
\\&=\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})B^{\boldsymbol{\pi},g}_{T-k}(I_{T-k},s_{T-k})
\\&-
\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})B^{\boldsymbol{\pi'},g}_{T-k}(I_{T-k},s_{T-k}).\label{inductive_hyp}
\end{align}

It remains to show that the expression holds for $k+1$ time steps prior to the end of the horizon. The result can be obtained using the dynamic programming principle and the base case ($k=1$) result, indeed we have that
\begin{align}\nonumber
&\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\left[B^{\boldsymbol{\pi},g}_{T-(k+1)}(I_{T-(k+1)},s_{T-(k+1)})-B^{\boldsymbol{\pi'},g}_{T-(k+1)}(I_{T-(k+1)},s_{T-(k+1)})\right]
% \\&\nonumber\begin{aligned}=\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})
% \phi(s_{T-(k+1)},\boldsymbol{a}_{T-(k+1)})
% \\&-\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})\phi(s_{T-(k+1)},\boldsymbol{a'}_{T-(k+1)})
% \\&+\gamma\sum_{s_{T-k}\in S}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})B^{\boldsymbol{\pi},g}_{T-k}(I_{T-k},s_{T-k})
% \\&-
% \gamma\sum_{s_{T-k}\in S}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})B^{\boldsymbol{\pi'},g}_{T-k}(I_{T-k},s_{T-k})\Bigg]
% \end{aligned}
\\&\begin{aligned}=\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})\left[
R(s_{T-(k+1)},\boldsymbol{a}_{T-(k+1)})+\sum_{k\geq 0}\sum_{j= T-1}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]
\\&+\gamma\sum_{s_{T-k}\in S}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)}) P(s_{T-k};\boldsymbol{a}_{T-(k+1)})B^{\boldsymbol{\pi},g}_{T-k}(I_{T-k},s_{T-k})
\\&\nonumber-\Bigg(
\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})\left[
R(s_{T-(k+1)},\boldsymbol{a'}_{T-(k+1)})+\sum_{k\geq 0}\sum_{j= T-1}^\infty c(I_j,I_{j-1})\delta^j_{\tau_k}\right]
\\&+\gamma\sum_{s_{T-k}\in S}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)}) P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})B^{\boldsymbol{\pi'},g}_{T-k}(I_{T-k},s_{T-k})\Bigg]\Bigg)
\end{aligned}
% \\&\begin{aligned}=\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})
% \phi(s_{T-(k+1)},\boldsymbol{a}_{T-(k+1)})
% \\&-\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})\phi(s_{T-(k+1)},\boldsymbol{a'}_{T-(k+1)})
% \\&+\gamma\sum_{s_{T-k}\in S}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})v_i^{(\pi^i,\pi^{-i})}(x_{T-k})
% \\&-
% \gamma\sum_{s_{T-k}\in S}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})v_i^{\pi^{i},\pi^{-i}}(x_{T-k})\Bigg].
% \end{aligned}
\\&\begin{aligned}=\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})R(s_{T-(k+1)},\boldsymbol{a}_{T-(k+1)})
\\&-\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})R(s_{T-(k+1)},\boldsymbol{a'}_{T-(k+1)})
\\&+\gamma \Bigg[\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})B^{\boldsymbol{\pi},g}_{T-k}(I_{T-k},s_{T-k})
\\&-
\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})B^{\boldsymbol{\pi'},g}_{T-k}(I_{T-k},s_{T-k})\Bigg]\Bigg].
\end{aligned}
\\&\begin{aligned}=\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\Bigg[&\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})R(s_{T-(k+1)},\boldsymbol{a}_{T-(k+1)})
\\&-\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})R(s_{T-(k+1)},\boldsymbol{a'}_{T-(k+1)})
\\&+\gamma \Bigg[\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})v^{\boldsymbol{\pi},g}_{i,T-k}(I_{T-k},s_{T-k})
\\&-
\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})v^{\boldsymbol{\pi'},g}_{i,T-k}(I_{T-k},s_{T-k})\Bigg]\Bigg].\nonumber
\end{aligned}
\\&=\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\left[v^{\boldsymbol{\pi},g}_{i,T-(k+1)}(I_{T-(k+1)},s_{T-(k+1)})-v^{\boldsymbol{\pi'},g}_{T-(k+1)}(I_{T-(k+1)},s_{i,T-(k+1)})\right]
\end{align}
using the inductive hypothesis and where we have used the fact that
\begin{align}\nonumber
&\mathbb{E}_{s_{T-(k+1)}\sim d_\theta}\Bigg[\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a}_{T-k}\in\boldsymbol{\mathcal{A}}}\sum_{k\geq 0}\sum_{j= T}^\infty\boldsymbol{\pi}(\boldsymbol{a}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a}_{T-(k+1)})\boldsymbol{\pi}(\boldsymbol{a}_{T-k};s_{T-k}) c(I_j,I_{j-1})\delta^j_{\tau_k}
\\&-\sum_{s_{T-k}\in\mathcal{S}}\sum_{\boldsymbol{a'}_{T-(k+1)}\in\boldsymbol{\mathcal{A}}}\sum_{\boldsymbol{a'}_{T-k}\in\boldsymbol{\mathcal{A}}}\sum_{k\geq 0}\sum_{j= T}^\infty\boldsymbol{\pi'}(\boldsymbol{a'}_{T-(k+1)};s_{T-(k+1)})P(s_{T-k};\boldsymbol{a'}_{T-(k+1)})\boldsymbol{\pi'}(\boldsymbol{a'}_{T-k};s_{T-k}) c(I_j,I_{j-1})\delta^j_{\tau_k}\Bigg]\nonumber
\\&=K\left(\sum_{\boldsymbol{a}_{T-k}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi}(\boldsymbol{a}_{T-k}) -\sum_{\boldsymbol{a'}_{T-k}\in\boldsymbol{\mathcal{A}}}\boldsymbol{\pi'}(\boldsymbol{a'}_{T-k})\right) =0
\end{align}    
via similar reasoning as before and after which which we deduce the result in the finite case.

For the infinite horizon case, we must prove that there exists a measurable function $B:\boldsymbol{\Pi}\times\mathcal{S}\to\mathbb{R}$ such that the following holds for any $i\in\mathcal{N}$ and $\forall \pi_i,\pi'_i\in\Pi_i, \forall \pi_{-i}\in\Pi_{-i}$ and $\forall s\in \mathcal{S}$:
\begin{align}
\mathbb{E}_{s\sim P}\left[\left(v^{\boldsymbol{\pi},g}_{i}-v^{\boldsymbol{\pi'},g}_{i}\right)(z)\right]
=\mathbb{E}_{s\sim P}\left[\left(B^{\boldsymbol{\pi},g}-B^{\boldsymbol{\pi'},g}\right)(z)\right].
\label{dpg_equality}
\end{align}
The result is proven by contradiction.

\noindent To this end, let us firstly assume $\exists c\neq 0$ such that
\begin{align*}
\mathbb{E}_{s\sim P}&\left[\left(v^{\boldsymbol{\pi},g}_i-v^{\boldsymbol{\pi'},g}_i\right)(z)\right]-
\mathbb{E}_{s\sim P}\left[\left(B^{\boldsymbol{\pi},g}_i-B^{\boldsymbol{\pi'},g}_i\right)(z)\right] =c.
\end{align*}

Let us now define the following quantities for any $s\in \mathcal{S}$ and for each $\pi_i\in\Pi_i$ and $\pi_{-i}\in\Pi_{-i}$ and $\forall i\in\mathcal{N}$:
\begin{align*}
v_{i,T'}^{\boldsymbol{\pi},g}(z):=\sum_{t=0}^{T'}\mu(s_0)\pi_i(a^i_0,s_0)\pi_{-i}(a^{-i}_0,s_0)
\prod_{j=0}^{t-1}\sum_{s_{j+1}\in \mathcal{S}}\gamma^tP(s_{j+1};s_j,a_j)\pi_i(a^i_j|s_j)\pi_{-i}(a^{-i}_j|s_j) r_i(z_j,\boldsymbol{a}_j)&,    
\end{align*}
and 
\begin{align*}
B_{T'}^{\boldsymbol{\pi},g}(z):=\sum_{t=0}^{T'}\mu(s_0)\pi_i(a^i_0,s_0)\pi_{-i}(a^{-i}_0,s_0)
\prod_{j=0}^{t-1}\sum_{s_{j+1}\in \mathcal{S}}P(s_{j+1};s_j,a_j)\cdot\pi_i(a^i_j|s_j)\pi_{-i}(a^{-i}_j|s_j)
\Theta(z_j,\boldsymbol{a}_j)&,
\end{align*}
so that the quantity $v_{i,T'}^{\boldsymbol{\pi}}(s)$ measures the expected cumulative return until the point $T'<\infty$.

Hence, we deduce that
\begin{align*}\nonumber
v_{i}^{\boldsymbol{\pi}}(z)&\equiv v_{i,\infty}^{\boldsymbol{\pi}}(z)
\\&=
v_{i,T'}^{\boldsymbol{\pi}}(z)
+\gamma^{T'}\mu(s_0)\pi_i(a^i_0,s_0)\pi_{-i}(a^{-i}_0,s_0)
\prod_{j=0}^{T'-1}\sum_{s_{j+1}\in \mathcal{S}}\gamma^tP(s_{j+1};s_j,a_j)
\pi_i(a^i_j|s_j)\pi_{-i}(a^{-i}_j|s_j)v_{i}^{\boldsymbol{\pi}}(s_{T'}).
\end{align*}

Next we observe that:

\begin{align*}\nonumber
c&=\mathbb{E}_{s\sim P}\left[\left(v_{i}^{\boldsymbol{\pi},g}-v_{i}^{\boldsymbol{\pi'},g}\right)(z)\right]-\mathbb{E}_{s\sim P}\left[\left(B^{\boldsymbol{\pi},g}-B^{\boldsymbol{\pi'},g}\right)(z)\right]
\\&\nonumber
=\mathbb{E}_{s\sim P}\left[\left(v_{i,T'}^{\boldsymbol{\pi},g}-v_{i,T'}^{\boldsymbol{\pi'},g}\right)(z)\right]
-\mathbb{E}_{s\sim P}\left[\left(B_{T'}^{\boldsymbol{\pi},g}-B_{T'}^{\boldsymbol{\pi'},g}\right)(s)\right]
\\&\begin{aligned}
+\gamma^{T'}\mathbb{E}_{s_{T'}\sim P}\Bigg[\mu(s_0)\pi_i(a^i_0,s_0)\pi_{-i}(a^{-i}_0,s_0)\prod_{j=0}^{T'-1}\sum_{s_{j+1}\in \mathcal{S}}P(s_{j+1};s_j,a_j)\pi_i(a^i_j|s_j)\pi_{-i}(a^{-i}_j|s_j)
\left(v_{i}^{\boldsymbol{\pi},g}(z_{T'})-B^{\boldsymbol{\pi},g}(z_{T'})\right)&\nonumber
\end{aligned}
\\&\hspace{-3 mm}
-\mu(s_0)\pi'_i(a'^i_0,s_0)\pi_{-i}(a^{-i}_0,s_0)\prod_{j=0}^{T'-1}\sum_{s_{j+1}\in \mathcal{S}}P(s_{j+1};s_j,a'_j)\pi'_i(a'^i_j|s_j)\pi_{-i}(a^{-i}_j|s_j)
\left(v_{i}^{\boldsymbol{\pi'},g}(z_{T'})-B^{\boldsymbol{\pi'},g}(z_{T'})\right)\Bigg].
\end{align*}

Considering the last expectation and its coefficient and denoting the product by $\kappa$, using the fact that by the Cauchy-Schwarz inequality we have $\|AX-BY\|\leq \|A\|\|X\|+\|B\|\|Y\|$, moreover whenever $A,B$ are non-expansive we have that $\|AX - BY \| \leq \|X\| + \|Y \|$, hence we observe the following $
\kappa\leq\|\kappa\|\leq 2\gamma^{T'}\left(\|v_{i}\|+\|B\|\right)$. Since we can choose $T'$ freely and $\gamma \in ]0,1[$, we can choose $T'$ to be sufficiently large so that $
\gamma^{T'}\left(\|v_{i}\|+\|B\|\right)<\frac{1}{4}|c|$.
This then implies that
\begin{align*}
&\Bigg|\mathbb{E}_{s\sim P}\Bigg[\left(v_{i,T'}^{\boldsymbol{\pi},g}-v_{i,T'}^{\boldsymbol{\pi'}}\right)(z)
-\left(B_{T'}^{\boldsymbol{\pi},g}-B_{T'}^{\boldsymbol{\pi'}}\right)(z)\Bigg]\Bigg|
>\frac{1}{2}c,    
\end{align*}
which is a contradiction since we have proven that for any finite $T'$ it is the case that
\begin{align}\nonumber
&\mathbb{E}_{s\sim P}\Bigg[\left(v_{i,T'}^{\boldsymbol{\pi},g}-v_{i,T'}^{\boldsymbol{\pi'}}\right)(z)
-\left(B_{T'}^{\boldsymbol{\pi},g}-B_{T'}^{\boldsymbol{\pi'}}\right)(z)\Bigg]=0,
\end{align}
and hence we deduce the result in the infinite horizon case.
\end{proof}
\subsection*{Proof of Part \textbf{III}}
To prove Part \textbf{III}, we firstly define precisely the notion of a stable point of the MG, $\mathcal{G}$:
\begin{definition}
A policy profile $\boldsymbol{\sigma^\star}=(g^\star,\pi^\star_i,\pi_{-i}^\star)\in\boldsymbol{\Pi}$ is a Markov perfect equilibrium (MPE) in Markov strategies if the following condition holds for any $i\in\mathcal{N}\times\{0\}$:
\begin{align}\label{MP_NE_condition}
&v_i^{(g^\star,\pi^\star_i,\pi^\star_{-i})}(z)\geq v_i^{g^\star,(\pi'_i,\pi^\star_{-i})}(z), \; \forall z\equiv (s_0,I_0)\in \mathcal{S}\times\{0,1\}, \;\forall \pi_i'\in\Pi_i.
\\&v_c^{(g^\star,\pi^\star_i,\pi^\star_{-i})}(z)\geq v_c^{g',(\pi_i,\pi^\star_{-i})}(z), \; \forall z\equiv (s_0,I_0)\in \mathcal{S}\times\{0,1\}, \;\forall g'.\end{align}
\end{definition}
The condition characterises strategic configurations which are stable points of the MG, $\mathcal{G}$. In particular, an MPE is achieved when at any state no agent can improve their expected cumulative rewards by unilaterally deviating from their current policy. We denote by $NE\{\mathcal{G}\}$ the set of MPE strategies for the MG, $\mathcal{G}$.

Next we prove that the set of maxima of the function $B$ are the MPE of the MG $\mathcal{G}$:

\begin{proposition}\label{reduction_prop}
The following implication holds: 
\begin{align}
\boldsymbol{\sigma}\in \underset{{g',\boldsymbol{\pi'}}\in\boldsymbol{\Pi}}{\arg\sup}\; B^{g',{\boldsymbol{\pi'}}}(s)\implies \boldsymbol{\sigma}\in NE\{\mathcal{G}\}.
\end{align}
where $B$ is the function in Prop. \ref{dpg_proposition}.
\end{proposition}
Prop. \ref{reduction_prop} indicates that the game has an equivalent representation in which all agents maximise the same function and thus  play a \textit{team game}.
\begin{proof}
We do the proof by contradiction. Let $\boldsymbol{\sigma}=(\pi_1,\ldots,\pi_N,g)\in \underset{\boldsymbol{\pi'}\in\boldsymbol{\Pi},g'}{\arg\sup}\; B^{\boldsymbol{\pi'},g'}(s)$ for any $s\in\mathcal{S}$. Let us now therefore assume that $\boldsymbol{\sigma}\notin NE\{\mathcal{G}\}$, hence there exists some other policy profile $\boldsymbol{\tilde{\sigma}}=(\pi_1,\ldots,\tilde{\pi}_i,\ldots,\pi_N,g)$ which contains at least one profitable deviation by one of the agents $i\in\mathcal{N}\times\{0,\}$. For now let us consider the case in which the profitable deviation is for a agent $i\in\mathcal{N}$  so that $\pi_i'\neq \pi_i$ for $i\in\mathcal{N}$ i.e. $v^{(\pi'_i,\pi_{-i}),g}_i(s)> v^{(\pi_i,\pi_{-i}),g}_i(s)$ (using the preservation of signs of integration). Prop. \ref{dpg_proposition} however implies that $B^{(\pi'_i,\pi_{-i}),g}(s)-B^{(\pi_i,\pi_{-i}),g}(s)>0$ which is a contradiction since $\boldsymbol{\sigma}=(\pi_i,\pi_{-i},g)$ is a maximum of $B$.  The proof can be straightforwardly adapted to cover the case in which the deviating agent is {\fontfamily{cmss}\selectfont Generator} after which we deduce the desired result.
\end{proof}
The last result completes the proof of Theorem \ref{theorem:existence_2}. 
\end{proof}
\section*{Proof of Proposition \ref{prop:switching_times}}
\begin{proof}[Proof of Prop. \ref{prop:switching_times}]
The proof is given by establishing a contradiction. Therefore suppose that $\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau_k},I(\tau_k))\leq \psi(s_{\tau_k},I(\tau_k))$ and suppose that the switching time $\tau'_1>\tau_1$ is an optimal switching time. Construct the {\fontfamily{cmss}\selectfont Generator} $g'$ and $\tilde{g}$ policy switching times by $(\tau'_0,\tau'_1,\ldots,)$ and $g'^2$ policy by $(\tau'_0,\tau_1,\ldots)$ respectively.  Define by $l=\inf\{t>0;\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{t},I_0)= \psi(s_{t},I_0)\}$ and $m=\sup\{t;t<\tau'_1\}$.
By construction we have that
\begin{align*}
& \quad v^{\boldsymbol{\pi},g'}_c(s,I_0)
\\&=\mathbb{E}\left[R(s_{0},\boldsymbol{a}_{0})+\mathbb{E}\left[\ldots+\gamma^{l-1}\mathbb{E}\left[R(s_{\tau_1-1},\boldsymbol{a}_{\tau_1-1})+\ldots+\gamma^{m-l-1}\mathbb{E}\left[ R(s_{\tau'_1-1},\boldsymbol{a}_{\tau'_1-1})+\gamma\mathcal{M}^{\pi^1,\pi'^2}v^{\boldsymbol{\pi},g'}_c(s',I(\tau'_{1}))\right]\right]\right]\right]
\\&<\mathbb{E}\left[R(s_{0},\boldsymbol{a}_{0})+\mathbb{E}\left[\ldots+\gamma^{l-1}\mathbb{E}\left[ R(s_{\tau_1-1},\boldsymbol{a}_{\tau_1-1})+\gamma\mathcal{M}^{\boldsymbol{\pi},\tilde{g}}v^{\boldsymbol{\pi},g'}_c(s_{\tau_1},I(\tau_1))\right]\right]\right]
\end{align*}
We now use the following observation $\mathbb{E}\left[ R(s_{\tau_1-1},\boldsymbol{a}_{\tau_1-1})+\gamma\mathcal{M}^{\boldsymbol{\pi},\tilde{g}}v^{\boldsymbol{\pi},g'}_c(s_{\tau_1},I(\tau_1))\right]\\\ \text{\hspace{30 mm}}\leq \max\left\{\mathcal{M}^{\boldsymbol{\pi},\tilde{g}}v^{\boldsymbol{\pi},g'}_c(s_{\tau_1},I(\tau_1)),\underset{a_{\tau_1}\in\mathcal{A}}{\max}\;\left[ R(s_{\tau_{k}},\boldsymbol{a}_{\tau_{k}})+\gamma\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a}_{\tau_1},s_{\tau_1})v^{\boldsymbol{\pi},g}_c(s',I(\tau_1))\right]\right\}$.

Using this we deduce that
\begin{align*}
&v^{\boldsymbol{\pi},g'}_2(s,I_0)\leq\mathbb{E}\Bigg[R(s_{0},\boldsymbol{a}_{0})+\mathbb{E}\Bigg[\ldots
\\&+\gamma^{l-1}\mathbb{E}\left[ R(s_{\tau_1-1},\boldsymbol{a}_{\tau_1-1})+\gamma\max\left\{\mathcal{M}^{\boldsymbol{\pi},\tilde{g}}v^{\boldsymbol{\pi},g'}_c(s_{\tau_1},I(\tau_1)),\underset{a_{\tau_1}\in\mathcal{A}}{\max}\;\left[ R(s_{\tau_{k}},\boldsymbol{a}_{\tau_{k}})+\gamma\sum_{s'\in\mathcal{S}}P(s';\boldsymbol{a}_{\tau_1},s_{\tau_1})v^{\boldsymbol{\pi},g}_c(s',I(\tau_1))\right]\right\}\right]\Bigg]\Bigg]
\\&=\mathbb{E}\left[R(s_{0},\boldsymbol{a}_{0})+\mathbb{E}\left[\ldots+\gamma^{l-1}\mathbb{E}\left[ R(s_{\tau_1-1},\boldsymbol{a}_{\tau_1-1})+\gamma\left[T v^{\boldsymbol{\pi},\tilde{g}}_c\right](s_{\tau_1},I(\tau_1))\right]\right]\right]=v^{\boldsymbol{\pi},\tilde{g}}_c(s,I_0))
\end{align*}
where the first inequality is true by assumption on $\mathcal{M}$. This is a contradiction since $g'$ is an optimal policy for {\fontfamily{cmss}\selectfont Generator}. Using analogous reasoning, we deduce the same result for $\tau'_k<\tau_k$ after which deduce the result. Moreover, by invoking the same reasoning, we can conclude that it must be the case that $(\tau_0,\tau_1,\ldots,\tau_{k-1},\tau_k,\tau_{k+1},\ldots,)$ are the optimal switching times. 


% The proof is given by establishing a contradiction. Therefore suppose that $\mathcal{M}^{\boldsymbol{\pi},g}\psi(s_{\tau_k},I(\tau_k))\geq \psi(s_{\tau_k},I(\tau_k))$ and let $\tau'_k>\tau_k$ be an optimal intervention time. Define by $\tilde{j}=\sup\{j=0,1,\ldots|\tau_j<\tau'_{k},\tau_j\in\mathcal{F}_t\}$.
% By construction we have that
% \begin{align*}
% &v^{\pi^1,\pi^2}_2(s_{\tau_{k-1}},I(\tau_{k-1}))=\mathbb{E}\left[R(s_{\tau_k},a_{\tau_k})+\gamma\left[T v^{\pi^1,\pi^2}_2\right](s_{\tau_k},I(\tau_k))\right]
% \\&=\mathbb{E}\left[R(s_{\tau_{k-1}},a_{\tau_{k-1}})+\gamma\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}v^{\pi^1,\pi^2}_2(s_{\tau_k},I(\tau_k)),\underset{a_{\tau_k}\in\mathcal{A}}{\max}\;\left[ R(s_{\tau_{k}},a_{\tau_{k}})+\gamma\sum_{s'\in\mathcal{S}}P(s';a_{\tau_k},s_{\tau_k})v^{\pi^1,\pi^2}_2(s',I(\tau_k))\right]\right\}\right]
% \\&>\mathbb{E}\left[R(s_{\tau_{k-1}},a_{\tau_{k-1}})+\gamma\underset{a_{\tau_k}\in\mathcal{A}}{\max}\;\left[ R(s_{\tau_{k}},a_{\tau_{k}})+\gamma\sum_{s'\in\mathcal{S}}P(s';a_{\tau_k},s_{\tau_k})v^{\pi^1,\pi^2}_2(s',I(\tau_k))\right]\right]
% \\&\geq\underset{a_{\tau_{k-1}},a_{\tau_k}\in\mathcal{A}}{\max}\;\mathbb{E}\left[R(s_{\tau_{k-1}},a_{\tau_{k-1}})+\gamma R(s_{\tau_{k}},a_{\tau_{k}})+\gamma^2\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau_k})v^{\pi^1,\pi^2}_2(s',I(\tau_k))\right]
% \\&\geq\underset{a_{\tau_{k-1}},\ldots, a_{\tau'_k-1}\in\mathcal{A}}{\max}\mathbb{E}\left[R\left(\boldsymbol{\tau}({\tau'_k})\right)+\gamma^{m}\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau'_k-1})v^{\pi^1,\pi^2}_2(s',I(\tau_{\tilde{j}}))\right]\geq v^{\pi^1,\pi'^2}_2(s_{\tau_{k-1}},I(\tau_{k-1}))
% \end{align*}
% for some $m\in\mathbb{N}$, where the first inequality is true by assumption on $\mathcal{M}$. This is a contradiction since $\pi'^2$ is an optimal policy for agent 2. Using analogous reasoning, we deduce the same result for $\tau'_k<\tau_k$ after which deduce the result.
\end{proof}

\section*{Proof of Theorem \ref{NE_improve_prop}}
\begin{proof}[Proof of Theorem \ref{NE_improve_prop}]
The proof which is done by contradiction follows from the definition of $v_c$.
Denote by $v^{\boldsymbol{\pi},g\equiv \boldsymbol{0}}_i$  value function an agent $i\in\mathcal{N}$ \textit{excluding {\fontfamily{cmss}\selectfont Generator}} and its intrinsic-reward function. 
% From this we can see that whenever $v^{\boldsymbol{\pi},g}_c>0$, {\fontfamily{cmss}\selectfont Generator} has produced an improvement in payoff for the agent. 
Indeed, 
let $(\boldsymbol{\hat{\pi}},\hat{g})$ be the policy profile induced by the Nash equilibrium policy profile and assume that the intrinsic-reward $F$ leads to a decrease in payoff for agent $i$. Then by construction 
$    v^{\boldsymbol{\pi},g}(s)< v^{\boldsymbol{\pi},g\equiv \boldsymbol{0}}(s)
$
which is a contradiction since $(\boldsymbol{\hat{\pi}},\hat{g})$ is an MPE profile.
\end{proof}



\section*{Proof of Theorem  \ref{primal_convergence_theorem}}
To prove the theorem, we make use of the following result:
\begin{theorem}[Theorem 1, pg 4 in \cite{jaakkola1994convergence}]
Let $\Xi_t(s)$ be a random process that takes values in $\mathbb{R}^n$ and given by the following:
\begin{align}
    \Xi_{t+1}(s)=\left(1-\alpha_t(s)\right)\Xi_{t}(s)\alpha_t(s)L_t(s),
\end{align}
then $\Xi_t(s)$ converges to $0$ with probability $1$ under the following conditions:
\begin{itemize}
\item[i)] $0\leq \alpha_t\leq 1, \sum_t\alpha_t=\infty$ and $\sum_t\alpha_t<\infty$
\item[ii)] $\|\mathbb{E}[L_t|\mathcal{F}_t]\|\leq \gamma \|\Xi_t\|$, with $\gamma <1$;
\item[iii)] ${\rm Var}\left[L_t|\mathcal{F}_t\right]\leq c(1+\|\Xi_t\|^2)$ for some $c>0$.
\end{itemize}
\end{theorem}
\begin{proof}
To prove the result, we show (i) - (iii) hold. Condition (i) holds by choice of learning rate. It therefore remains to prove (ii) - (iii). We first prove (ii). For this, we consider our variant of the Q-learning update rule:
\begin{align*}
Q_{t+1}(s_t,I_t,\boldsymbol{a}_t)=Q_{t}&(s_t,I_t,\boldsymbol{a}_t)
\\&+\alpha_t(s_t,I_t,\boldsymbol{a}_t)\left[\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-Q_{t}(s_t,I_t,\boldsymbol{a}_t)\right].
\end{align*}
After subtracting $Q^\star(s_t,I_t,\boldsymbol{a}_t)$ from both sides and some manipulation we obtain that:
\begin{align*}
&\Xi_{t+1}(s_t,I_t,\boldsymbol{a}_t)
\\&=(1-\alpha_t(s_t,I_t,\boldsymbol{a}_t))\Xi_{t}(s_t,I_t,\boldsymbol{a}_t)
\\&\qquad\qquad\qquad\qquad\;\;+\alpha_t(s_t,I_t,\boldsymbol{a}_t))\left[\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-Q^\star(s_t,I_t,\boldsymbol{a}_t)\right],  \end{align*}
where $\Xi_{t}(s_t,I_t,\boldsymbol{a}_t):=Q_t(s_t,I_t,\boldsymbol{a}_t)-Q^\star(s_t,I_t,\boldsymbol{a}_t)$.

Let us now define by 
\begin{align*}
L_t(s_{\tau_k},I_{\tau_k},\boldsymbol{a}):=\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-Q^\star(s_t,I_t,a).
\end{align*}
Then
\begin{align}
\Xi_{t+1}(s_t,I_t,\boldsymbol{a}_t)=(1-\alpha_t(s_t,I_t,\boldsymbol{a}_t))\Xi_{t}(s_t,I_t,\boldsymbol{a}_t)+\alpha_t(s_t,I_t,\boldsymbol{a}_t))\left[L_t(s_{\tau_k},a)\right].   
\end{align}

% Let $z_{\tau_k}\equiv s_{\tau_k},I(\tau_k)$
We now observe that
\begin{align}\nonumber
\mathbb{E}\left[L_t(s_{\tau_k},I_{\tau_k},\boldsymbol{a})|\mathcal{F}_t\right]&=\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau_k})\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-Q^\star(s_{\tau_k},a)
\\&= T_\phi Q_t(s,I_{\tau_k},\boldsymbol{a})-Q^\star(s,I_{\tau_k},\boldsymbol{a}). \label{expectation_L}
\end{align}
Now, using the fixed point property that implies $Q^\star=T_\phi Q^\star$, we find that
\begin{align}\nonumber
    \mathbb{E}\left[L_t(s_{\tau_k},I_{\tau_k},\boldsymbol{a})|\mathcal{F}_t\right]&=T_\phi Q_t(s,I_{\tau_k},\boldsymbol{a})-T_\phi Q^\star(s,I_{\tau_k},\boldsymbol{a})
    \\&\leq\left\|T_\phi Q_t-T_\phi Q^\star\right\|\nonumber
    \\&\leq \gamma\left\| Q_t- Q^\star\right\|_\infty=\gamma\left\|\Xi_t\right\|_\infty.
\end{align}
using the contraction property of $T$ established in Lemma \ref{lemma:bellman_contraction}. This proves (ii).

% Define by $\hat{F}(s_{\tau_k},I(\tau_k),a):=\left(\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I(\tau_k)), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a\in\mathcal{A}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau_k})Q(s',I(\tau_k))\right)$
We now prove iii), that is
\begin{align}
    {\rm Var}\left[L_t|\mathcal{F}_t\right]\leq c(1+\|\Xi_t\|^2).
\end{align}
Now by \eqref{expectation_L} we have that
\begin{align*}
  {\rm Var}\left[L_t|\mathcal{F}_t\right]&= {\rm Var}\left[\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-Q^\star(s_t,I_t,a)\right]
  \\&= \mathbb{E}\Bigg[\Bigg(\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}
  \\&\qquad\qquad\qquad\qquad\qquad\quad\quad\quad-Q^\star(s_t,I_t,a)-\left(T_\Phi Q_t(s,I_{\tau_k},\boldsymbol{a})-Q^\star(s,I_{\tau_k},\boldsymbol{a})\right)\Bigg)^2\Bigg]
      \\&= \mathbb{E}\left[\left(\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-T_\Phi Q_t(s,I_{\tau_k},\boldsymbol{a})\right)^2\right]
    \\&= {\rm Var}\left[\max\left\{\mathcal{M}^{\boldsymbol{\pi},g}Q(s_{\tau_k},I_{\tau_k},\boldsymbol{a}), \phi(s_{\tau_k},\boldsymbol{a})+\gamma\underset{a'\in\mathcal{A}}{\max}\;Q(s',I_{\tau_k},\boldsymbol{a'})\right\}-T_\Phi Q_t(s,I_{\tau_k},\boldsymbol{a}))^2\right]
    \\&\leq c(1+\|\Xi_t\|^2),
\end{align*}
for some $c>0$ where the last line follows due to the boundedness of $Q$ (which follows from Assumptions 2 and 4). This concludes the proof of the Theorem.
\end{proof}
% With this, the result can also be extended to fitted Q learning using methods established in \cite{munos2008finite,antos2007fitted} see e.g. Theorem 2, pg 16 in \cite{munos2008finite}.
\section*{Proof of Convergence with Function Approximation}
First let us recall the statement of the theorem:
\begin{customthm}{3}
LIGS converges to a limit point $r^\star$ which is the unique solution to the equation:
\begin{align}
\Pi \mathfrak{F} (\Phi r^\star)=\Phi r^\star, \qquad \text{a.e.}
\end{align}
where we recall that for any test function $\Lambda \in \mathcal{V}$, the operator $\mathfrak{F}$ is defined by $
    \mathfrak{F}\Lambda:=\Theta+\gamma P \max\{\mathcal{M}\Lambda,\Lambda\}$.

Moreover, $r^\star$ satisfies the following:
\begin{align}
    \left\|\Phi r^\star - Q^\star\right\|\leq c\left\|\Pi Q^\star-Q^\star\right\|.
\end{align}
\end{customthm}

The theorem is proven using a set of results that we now establish. To this end, we first wish to prove the following bound:    
\begin{lemma}
For any $Q\in\mathcal{V}$ we have that
\begin{align}
    \left\|\mathfrak{F}Q-Q'\right\|\leq \gamma\left\|Q-Q'\right\|,
\end{align}
so that the operator $\mathfrak{F}$ is a contraction.
\end{lemma}
\begin{proof}
Recall, for any test function $\psi$ , a projection operator $\Pi$ acting $\Lambda$ is defined by the following 
\begin{align*}
\Pi \Lambda:=\underset{\bar{\Lambda}\in\{\Phi r|r\in\mathbb{R}^p\}}{\arg\min}\left\|\bar{\Lambda}-\Lambda\right\|. 
\end{align*}
Now, we first note that in the proof of Lemma \ref{lemma:bellman_contraction}, we deduced that for any $\Lambda\in L_2$ we have that
\begin{align*}
    \left\|\mathcal{M}\Lambda-\left[ \psi(\cdot,a)+\gamma\underset{\boldsymbol{a}\in\boldsymbol{\mathcal{A}}}{\max}\;\mathcal{P}^{\boldsymbol{a}}\Lambda'\right]\right\|\leq \gamma\left\|\Lambda-\Lambda'\right\|,
\end{align*}
(c.f. Lemma \ref{lemma:bellman_contraction}). 

Setting $\Lambda=Q$ and $\psi=\Theta$, it can be straightforwardly deduced that for any $Q,\hat{Q}\in L_2$:
    $\left\|\mathcal{M}Q-\hat{Q}\right\|\leq \gamma\left\|Q-\hat{Q}\right\|$. Hence, using the contraction property of $\mathcal{M}$, we readily deduce the following bound:
\begin{align}\max\left\{\left\|\mathcal{M}Q-\hat{Q}\right\|,\left\|\mathcal{M}Q-\mathcal{M}\hat{Q}\right\|\right\}\leq \gamma\left\|Q-\hat{Q}\right\|,
\label{m_bound_q_twice}
\end{align}
    
We now observe that $\mathfrak{F}$ is a contraction. Indeed, since for any $Q,Q'\in L_2$ we have that:
% \begin{align*}
% \left\|\mathfrak{F}Q-\mathfrak{F}Q'\right\|&=\left\|\Theta+\gamma P \max\{\mathcal{M}Q,Q\}-\left(\Theta+\gamma P \max\{\mathcal{M}Q',Q'\}\right)\right\|
% \\&=\gamma \left\|P \max\{\mathcal{M}Q,Q\}-P \max\{\mathcal{M}Q',Q'\}\right\|
% \\&\leq\gamma \left\| \max\{\mathcal{M}Q,Q\}- \max\{\mathcal{M}Q',Q'\}\right\|
% \\&\leq\gamma \left\| \max\{\mathcal{M}Q,Q\}-\max\{\mathcal{M}\bar{Q},Q\}\right\|+\gamma\left\|\max\{\mathcal{M}\bar{Q},Q\}- \max\{\mathcal{M}Q',Q'\}\right\|
% \\&\leq\gamma \left\| \max\{\mathcal{M}Q,Q\}-\max\{\mathcal{M}Q',Q\}\right\|+\gamma\left\|\max\{\mathcal{M}Q',Q\}- \max\{\mathcal{M}Q',Q'\}\right\|
% \\&\leq\gamma \left\| \mathcal{M}Q,-\mathcal{M}Q'\right\|+\gamma\left\|Q- Q'\right\|
% \\&\leq\gamma(1+\gamma) \left\|Q-Q'\right\|
% \end{align*}
% 
% 
% 
\begin{align*}
\left\|\mathfrak{F}Q-\mathfrak{F}Q'\right\|&=\left\|\Theta+\gamma P \max\{\mathcal{M}Q,Q\}-\left(\Theta+\gamma P \max\{\mathcal{M}Q',Q'\}\right)\right\|
\\&=\gamma \left\|P \max\{\mathcal{M}Q,Q\}-P \max\{\mathcal{M}Q',Q'\}\right\|
\\&\leq\gamma \left\| \max\{\mathcal{M}Q,Q\}- \max\{\mathcal{M}Q',Q'\}\right\|
\\&\leq\gamma \left\| \max\{\mathcal{M}Q-\mathcal{M}Q',Q-\mathcal{M}Q',\mathcal{M}Q-Q',Q-Q'\}\right\|
\\&\leq\gamma \max\{\left\|\mathcal{M}Q-\mathcal{M}Q'\right\|,\left\|Q-\mathcal{M}Q'\right\|,\left\|\mathcal{M}Q-Q'\right\|,\left\|Q-Q'\right\|\}
\\&=\gamma\left\|Q-Q'\right\|,
\end{align*}
using \eqref{m_bound_q_twice} and again using the non-expansiveness of $P$.
\end{proof}
We next show that the following two bounds hold:
\begin{lemma}\label{projection_F_contraction_lemma}
For any $Q\in\mathcal{V}$ we have that
\begin{itemize}
    \item[i)] 
$\qquad\qquad
    \left\|\Pi \mathfrak{F}Q-\Pi \mathfrak{F}\bar{Q}\right\|\leq \gamma\left\|Q-\bar{Q}\right\|$,
    \item[ii)]$\qquad\qquad\left\|\Phi r^\star - Q^\star\right\|\leq \frac{1}{\sqrt{1-\gamma^2}}\left\|\Pi Q^\star - Q^\star\right\|$. 
\end{itemize}
\end{lemma}
\begin{proof}
The first result is straightforward since as $\Pi$ is a projection it is non-expansive and hence:
\begin{align*}
    \left\|\Pi \mathfrak{F}Q-\Pi \mathfrak{F}\bar{Q}\right\|\leq \left\| \mathfrak{F}Q-\mathfrak{F}\bar{Q}\right\|\leq \gamma \left\|Q-\bar{Q}\right\|,
\end{align*}
using the contraction property of $\mathfrak{F}$. This proves i). For ii), we note that by the orthogonality property of projections we have that $\left\langle\Phi r^\star - \Pi Q^\star,\Phi r^\star - \Pi Q^\star\right\rangle$, hence we observe that:
\begin{align*}
    \left\|\Phi r^\star - Q^\star\right\|^2&=\left\|\Phi r^\star - \Pi Q^\star\right\|^2+\left\|\Phi r^\star - \Pi Q^\star\right\|^2
\\&=\left\|\Pi \mathfrak{F}\Phi r^\star - \Pi Q^\star\right\|^2+\left\|\Phi r^\star - \Pi Q^\star\right\|^2
\\&\leq\left\|\mathfrak{F}\Phi r^\star -  Q^\star\right\|^2+\left\|\Phi r^\star - \Pi Q^\star\right\|^2
\\&=\left\|\mathfrak{F}\Phi r^\star -  \mathfrak{F}Q^\star\right\|^2+\left\|\Phi r^\star - \Pi Q^\star\right\|^2
\\&\leq\gamma^2\left\|\Phi r^\star -  Q^\star\right\|^2+\left\|\Phi r^\star - \Pi Q^\star\right\|^2,
\end{align*}
after which we readily deduce the desired result.
\end{proof}


\begin{lemma}
Define  the operator $H$ by the following: $
  HQ(z)=  \begin{cases}
			\mathcal{M}Q(z), & \text{if $\mathcal{M}Q(z)>\Phi r^\star,$}\\
            Q(z), & \text{otherwise},
		 \end{cases}$
\\and $\tilde{\mathfrak{F}}$ by: $
    \tilde{\mathfrak{F}}Q:=\Theta +\gamma PHQ$.

For any $Q,\bar{Q}\in L_2$ we have that
\begin{align}
    \left\|\tilde{\mathfrak{F}}Q-\tilde{\mathfrak{F}}\bar{Q}\right\|\leq \gamma \left\|Q-\bar{Q}\right\|
\end{align}
and hence $\tilde{\mathfrak{F}}$ is a contraction mapping.
\end{lemma}
\begin{proof}
Using \eqref{m_bound_q_twice}, we now observe that
\begin{align*}
    \left\|\tilde{\mathfrak{F}}Q-\tilde{\mathfrak{F}}\bar{Q}\right\|&=\left\|\Theta+\gamma PHQ -\left(\Theta+\gamma PH\bar{Q}\right)\right\|
\\&\leq \gamma\left\|HQ - H\bar{Q}\right\|
\\&\leq \gamma\left\|\max\left\{\mathcal{M}Q-\mathcal{M}\bar{Q},Q-\bar{Q},\mathcal{M}Q-\bar{Q},\mathcal{M}\bar{Q}-Q\right\}\right\|
\\&\leq \gamma\max\left\{\left\|\mathcal{M}Q-\mathcal{M}\bar{Q}\right\|,\left\|Q-\bar{Q}\right\|,\left\|\mathcal{M}Q-\bar{Q}\right\|,\left\|\mathcal{M}\bar{Q}-Q\right\|\right\}
\\&\leq \gamma\max\left\{\gamma\left\|Q-\bar{Q}\right\|,\left\|Q-\bar{Q}\right\|,\left\|\mathcal{M}Q-\bar{Q}\right\|,\left\|\mathcal{M}\bar{Q}-Q\right\|\right\}
\\&=\gamma\left\|Q-\bar{Q}\right\|,
\end{align*}
again using the non-expansive property of $P$.
\end{proof}
\begin{lemma}
Define by $\tilde{Q}:=\Theta+\gamma Pv^{\boldsymbol{\tilde{\pi}}}$ where
\begin{align}
    v^{\boldsymbol{\tilde{\pi}}}(z):= \Theta(s_{\tau_k},a)+\gamma\underset{a\in\mathcal{A}}{\max}\;\sum_{s'\in\mathcal{S}}P(s';a,s_{\tau_k})\Phi r^\star(s',I(\tau_k)), \label{v_tilde_definition}
\end{align}
then $\tilde{Q}$ is a fixed point of $\tilde{\mathfrak{F}}\tilde{Q}$, that is $\tilde{\mathfrak{F}}\tilde{Q}=\tilde{Q}$. 
\end{lemma}
\begin{proof}
We begin by observing that
\begin{align*}
H\tilde{Q}(z)&=H\left(\Theta(z)+\gamma Pv^{\boldsymbol{\tilde{\pi}}}\right)    
\\&= \begin{cases}
			\mathcal{M}Q(z), & \text{if $\mathcal{M}Q(z)>\Phi r^\star,$}\\
            Q(z), & \text{otherwise},
		 \end{cases}
\\&= \begin{cases}
			\mathcal{M}Q(z), & \text{if $\mathcal{M}Q(z)>\Phi r^\star,$}\\
            \Theta(z)+\gamma Pv^{\boldsymbol{\tilde{\pi}}}, & \text{otherwise},
		 \end{cases}
\\&=v^{\boldsymbol{\tilde{\pi}}}(z).
\end{align*}
Hence,
\begin{align}
    \tilde{\mathfrak{F}}\tilde{Q}=\Theta+\gamma PH\tilde{Q}=\Theta+\gamma Pv^{\boldsymbol{\tilde{\pi}}}=\tilde{Q}. 
\end{align}
which proves the result.
\end{proof}
\begin{lemma}\label{value_difference_Q_difference}
The following bound holds:
\begin{align}
    \mathbb{E}\left[v^{\boldsymbol{\hat{\pi}}}(z_0)\right]-\mathbb{E}\left[v^{\boldsymbol{\tilde{\pi}}}(z_0)\right]\leq 2\left[(1-\gamma)\sqrt{(1-\gamma^2)}\right]^{-1}\left\|\Pi Q^\star -Q^\star\right\|.
\label{F_tilde_fixed_point}\end{align}
\end{lemma}
\begin{proof}

By definitions of $v^{\boldsymbol{\hat{\pi}}}$ and $v^{\boldsymbol{\tilde{\pi}}}$ (c.f \eqref{v_tilde_definition}) and using Jensen's inequality and the stationarity property we have that,
\begin{align}\nonumber
    \mathbb{E}\left[v^{\boldsymbol{\hat{\pi}}}(z_0)\right]-\mathbb{E}\left[v^{\boldsymbol{\tilde{\pi}}}(z_0)\right]&=\mathbb{E}\left[Pv^{\boldsymbol{\hat{\pi}}}(z_0)\right]-\mathbb{E}\left[Pv^{\boldsymbol{\tilde{\pi}}}(z_0)\right]
    \\&\leq \left|\mathbb{E}\left[Pv^{\boldsymbol{\hat{\pi}}}(z_0)\right]-\mathbb{E}\left[Pv^{\boldsymbol{\tilde{\pi}}}(z_0)\right]\right|\nonumber
    \\&\leq \left\|Pv^{\boldsymbol{\hat{\pi}}}-Pv^{\boldsymbol{\tilde{\pi}}}\right\|. \label{v_approx_intermediate_bound_P}
\end{align}
Now recall that $\tilde{Q}:=\Theta+\gamma Pv^{\boldsymbol{\tilde{\pi}}}$ and $Q^\star:=\Theta+\gamma Pv^{\boldsymbol{\pi^\star}}$,  using these expressions in \eqref{v_approx_intermediate_bound_P} we find that 
\begin{align*}
    \mathbb{E}\left[v^{\boldsymbol{\hat{\pi}}}(z_0)\right]-\mathbb{E}\left[v^{\boldsymbol{\tilde{\pi}}}(z_0)\right]&\leq \frac{1}{\gamma}\left\|\tilde{Q}-Q^\star\right\|. \label{v_approx_q_approx_bound}
\end{align*}
Moreover, by the triangle inequality and using the fact that $\mathfrak{F}(\Phi r^\star)=\tilde{\mathfrak{F}}(\Phi r^\star)$ and that $\mathfrak{F}Q^\star=Q^\star$ and $\mathfrak{F}\tilde{Q}=\tilde{Q}$ (c.f. \eqref{F_tilde_fixed_point}) we have that
\begin{align*}
\left\|\tilde{Q}-Q^\star\right\|&\leq \left\|\tilde{Q}-\mathfrak{F}(\Phi r^\star)\right\|+\left\|Q^\star-\tilde{\mathfrak{F}}(\Phi r^\star)\right\|    
\\&\leq \gamma\left\|\tilde{Q}-\Phi r^\star\right\|+\gamma\left\|Q^\star-\Phi r^\star\right\| 
\\&\leq 2\gamma\left\|\tilde{Q}-\Phi r^\star\right\|+\gamma\left\|Q^\star-\tilde{Q}\right\|, 
\end{align*}
which gives the following bound:
\begin{align*}
\left\|\tilde{Q}-Q^\star\right\|&\leq 2\left(1-\gamma\right)^{-1}\left\|\tilde{Q}-\Phi r^\star\right\|, 
\end{align*}
from which, using Lemma \ref{projection_F_contraction_lemma}, we deduce that $
    \left\|\tilde{Q}-Q^\star\right\|\leq 2\left[(1-\gamma)\sqrt{(1-\gamma^2)}\right]^{-1}\left\|\tilde{Q}-\Phi r^\star\right\|$,
after which by \eqref{v_approx_q_approx_bound}, we finally obtain
\begin{align*}
        \mathbb{E}\left[v^{\boldsymbol{\hat{\pi}}}(z_0)\right]-\mathbb{E}\left[v^{\boldsymbol{\tilde{\pi}}}(z_0)\right]\leq  2\left[(1-\gamma)\sqrt{(1-\gamma^2)}\right]^{-1}\left\|\tilde{Q}-\Phi r^\star\right\|,
\end{align*}
as required.
\end{proof}

Let us rewrite the update in the following way:
\begin{align*}
    r_{t+1}=r_t+\gamma_t\Xi(w_t,r_t),
\end{align*}
where the function $\Xi:\mathbb{R}^{2d}\times \mathbb{R}^p\to\mathbb{R}^p$ is given by:
\begin{align*}
\Xi(w,r):=\phi(z)\left(\Theta(z)+\gamma\max\left\{(\Phi r) (z'),\mathcal{M}(\Phi r) (z')\right\}-(\Phi r)(z)\right),
\end{align*}
for any $w\equiv (z,z')\in\left(\mathbb{N}\times\mathcal{S}\right)^2$ where $z=(t,s)\in\mathbb{N}\times\mathcal{S}$ and $z'=(t,s')\in\mathbb{N}\times\mathcal{S}$  and for any $r\in\mathbb{R}^p$. Let us also define the function $\boldsymbol{\Xi}:\mathbb{R}^p\to\mathbb{R}^p$ by the following:
\begin{align*}
    \boldsymbol{\Xi}(r):=\mathbb{E}_{w_0\sim (\mathbb{P},\mathbb{P})}\left[\Xi(w_0,r)\right]; w_0:=(z_0,z_1).
\end{align*}
\begin{lemma}\label{iteratation_property_lemma}
The following statements hold for all $z\in \{0,1\}\times \mathcal{S}$:
\begin{itemize}
    \item[i)] $
(r-r^\star)\boldsymbol{\Xi}_k(r)<0,\qquad \forall r\neq r^\star,    
$
\item[ii)] $
\boldsymbol{\Xi}_k(r^\star)=0$.
\end{itemize}
\end{lemma}
\begin{proof}
To prove the statement, we first note that each component of $\boldsymbol{\Xi}_k(r)$ admits a representation as an inner product, indeed: 
\begin{align*}
\boldsymbol{\Xi}_k(r)&=\mathbb{E}\left[\phi_k(z_0)(\Theta(z_0)+\gamma\max\left\{\Phi r(z_1),\mathcal{M}\Phi(z_1)\right\}-(\Phi r)(z_0)\right] 
\\&=\mathbb{E}\left[\phi_k(z_0)(\Theta(z_0)+\gamma\mathbb{E}\left[\max\left\{\Phi r(z_1),\mathcal{M}\Phi(z_1)\right\}|z_0\right]-(\Phi r)(z_0)\right]
\\&=\mathbb{E}\left[\phi_k(z_0)(\Theta(z_0)+\gamma P\max\left\{\left(\Phi r,\mathcal{M}\Phi\right)\right\}(z_0)-(\Phi r)(z_0)\right]
\\&=\left\langle\phi_k,\mathfrak{F}\Phi r-\Phi r\right\rangle,
\end{align*}
using the iterated law of expectations and the definitions of $P$ and $\mathfrak{F}$.

We now are in position to prove i). Indeed, we now observe the following:
\begin{align*}
\left(r-r^\star\right)\boldsymbol{\Xi}_k(r)&=\sum_{l=1}\left(r(l)-r^\star(l)\right)\left\langle\phi_l,\mathfrak{F}\Phi r -\Phi r\right\rangle
\\&=\left\langle\Phi r -\Phi r^\star, \mathfrak{F}\Phi r -\Phi r\right\rangle
\\&=\left\langle\Phi r -\Phi r^\star, (\boldsymbol{1}-\Pi)\mathfrak{F}\Phi r+\Pi \mathfrak{F}\Phi r -\Phi r\right\rangle
\\&=\left\langle\Phi r -\Phi r^\star, \Pi \mathfrak{F}\Phi r -\Phi r\right\rangle,
\end{align*}
where in the last step we used the orthogonality of $(\boldsymbol{1}-\Pi)$. We now recall that $\Pi \mathfrak{F}\Phi r^\star=\Phi r^\star$ since $\Phi r^\star$ is a fixed point of $\Pi \mathfrak{F}$. Additionally, using Lemma \ref{projection_F_contraction_lemma} we observe that $\|\Pi \mathfrak{F}\Phi r -\Phi r^\star\| \leq \gamma \|\Phi r -\Phi r^\star\|$. With this we now find that
\begin{align*}
&\left\langle\Phi r -\Phi r^\star, \Pi \mathfrak{F}\Phi r -\Phi r\right\rangle    
\\&=\left\langle\Phi r -\Phi r^\star, (\Pi \mathfrak{F}\Phi r -\Phi r^\star)+ \Phi r^\star -\Phi r\right\rangle
\\&\leq\left\|\Phi r -\Phi r^\star\right\|\left\|\Pi \mathfrak{F}\Phi r -\Phi r^\star\right\|- \left\|\Phi r^\star -\Phi r\right\|^2
\\&\leq(\gamma -1)\left\|\Phi r^\star -\Phi r\right\|^2,
\end{align*}
which is negative since $\gamma<1$ which completes the proof of part i).

The proof of part ii) is straightforward since we readily observe that
\begin{align*}
    \boldsymbol{\Xi}_k(r^\star)= \left\langle\phi_l, \mathfrak{F}\Phi r^\star-\Phi r\right\rangle= \left\langle\phi_l, \Pi \mathfrak{F}\Phi r^\star-\Phi r\right\rangle=0,
\end{align*}
as required and from which we deduce the result.
\end{proof}
To prove the theorem, we make use of a special case of the following result:

\begin{theorem}[Th. 17, p. 239 in \cite{benveniste2012adaptive}] \label{theorem:stoch.approx.}
Consider a stochastic process $r_t:\mathbb{R}\times\{\infty\}\times\Omega\to\mathbb{R}^k$ which takes an initial value $r_0$ and evolves according to the following:
\begin{align}
    r_{t+1}=r_t+\alpha \Xi(s_t,r_t),
\end{align}
for some function $s:\mathbb{R}^{2d}\times\mathbb{R}^k\to\mathbb{R}^k$ and where the following statements hold:
\begin{enumerate}
    \item $\{s_t|t=0,1,\ldots\}$ is a stationary, ergodic Markov process taking values in $\mathbb{R}^{2d}$
    \item For any positive scalar $q$, there exists a scalar $\mu_q$ such that $\mathbb{E}\left[1+\|s_t\|^q|s\equiv s_0\right]\leq \mu_q\left(1+\|s\|^q\right)$
    \item The step size sequence satisfies the Robbins-Monro conditions, that is $\sum_{t=0}^\infty\alpha_t=\infty$ and $\sum_{t=0}^\infty\alpha^2_t<\infty$
    \item There exists scalars $c$ and $q$ such that $    \|\Xi(w,r)\|
        \leq c\left(1+\|w\|^q\right)(1+\|r\|)$
    \item There exists scalars $c$ and $q$ such that $
        \sum_{t=0}^\infty\left\|\mathbb{E}\left[\Xi(w_t,r)|z_0\equiv z\right]-\mathbb{E}\left[\Xi(w_0,r)\right]\right\|
        \leq c\left(1+\|w\|^q\right)(1+\|r\|)$
    \item There exists a scalar $c>0$ such that $
        \left\|\mathbb{E}[\Xi(w_0,r)]-\mathbb{E}[\Xi(w_0,\bar{r})]\right\|\leq c\|r-\bar{r}\| $
    \item There exists scalars $c>0$ and $q>0$ such that $
        \sum_{t=0}^\infty\left\|\mathbb{E}\left[\Xi(w_t,r)|w_0\equiv w\right]-\mathbb{E}\left[\Xi(w_0,\bar{r})\right]\right\|
        \leq c\|r-\bar{r}\|\left(1+\|w\|^q\right) $
    \item There exists some $r^\star\in\mathbb{R}^k$ such that $\boldsymbol{\Xi}(r)(r-r^\star)<0$ for all $r \neq r^\star$ and $\bar{s}(r^\star)=0$. 
\end{enumerate}
Then $r_t$ converges to $r^\star$ almost surely.
\end{theorem}

In order to apply the Theorem \ref{theorem:stoch.approx.}, we show that conditions 1 - 7 are satisfied.

\begin{proof}
Conditions 1-2 are true by assumption while condition 3 can be made true by choice of the learning rates. Therefore it remains to verify conditions 4-7 are met.   

To prove 4, we observe that
\begin{align*}
\left\|\Xi(w,r)\right\|
&=\left\|\phi(z)\left(\Theta(z)+\gamma\max\left\{(\Phi r) (z'),\mathcal{M}\Phi (z')\right\}-(\Phi r)(z)\right)\right\|
\\&\leq\left\|\phi(z)\right\|\left\|\Theta(z)+\gamma\left(\left\|\phi(z')\right\|\|r\|+\mathcal{M}\Phi (z')\right)\right\|+\left\|\phi(z)\right\|\|r\|
\\&\leq\left\|\phi(z)\right\|\left(\|\Theta(z)\|+\gamma\|\mathcal{M}\Phi (z')\|\right)+\left\|\phi(z)\right\|\left(\gamma\left\|\phi(z')\right\|+\left\|\phi(z)\right\|\right)\|r\|.
\end{align*}
Now using the definition of $\mathcal{M}$, we readily observe that $\|\mathcal{M}\Phi (z')\|\leq \| \Theta\|+\gamma\|\mathcal{P}^\pi_{s's_t}\Phi\|\leq \| \Theta\|+\gamma\|\Phi\|$ using the non-expansiveness of $P$.

Hence, we lastly deduce that
\begin{align*}
\left\|\Xi(w,r)\right\|
&\leq\left\|\phi(z)\right\|\left(\|\Theta(z)\|+\gamma\|\mathcal{M}\Phi (z')\|\right)+\left\|\phi(z)\right\|\left(\gamma\left\|\phi(z')\right\|+\left\|\phi(z)\right\|\right)\|r\|
\\&\leq\left\|\phi(z)\right\|\left(\|\Theta(z)\|+\gamma\| \Theta\|+\gamma\|\psi\|\right)+\left\|\phi(z)\right\|\left(\gamma\left\|\phi(z')\right\|+\left\|\phi(z)\right\|\right)\|r\|,
\end{align*}
we then easily deduce the result using the boundedness of $\phi,\Theta$ and $\psi$.

Now we observe the following Lipschitz condition on $\Xi$:
\begin{align*}
&\left\|\Xi(w,r)-\Xi(w,\bar{r})\right\|
\\&=\left\|\phi(z)\left(\gamma\max\left\{(\Phi r)(z'),\mathcal{M}\Phi(z')\right\}-\gamma\max\left\{(\Phi \bar{r})(z'),\mathcal{M}\Phi(z')\right\}\right)-\left((\Phi r)(z)-\Phi\bar{r}(z)\right)\right\|
\\&\leq\gamma\left\|\phi(z)\right\|\left\|\max\left\{\phi'(z') r,\mathcal{M}\Phi'(z')\right\}-\max\left\{(\phi'(z') \bar{r}),\mathcal{M}\Phi'(z')\right\}\right\|+\left\|\phi(z)\right\|\left\|\phi'(z) r-\phi(z)\bar{r}\right\|
\\&\leq\gamma\left\|\phi(z)\right\|\left\|\phi'(z') r-\phi'(z') \bar{r}\right\|+\left\|\phi(z)\right\|\left\|\phi'(z) r-\phi'(z)\bar{r}\right\|
\\&\leq \left\|\phi(z)\right\|\left(\left\|\phi(z)\right\|+ \gamma\left\|\phi(z)\right\|\left\|\phi'(z') -\phi'(z') \right\|\right)\left\|r-\bar{r}\right\|
\\&\leq c\left\|r-\bar{r}\right\|,
\end{align*}
using Cauchy-Schwarz inequality and  that for any scalars $a,b,c$ we have that $
    \left|\max\{a,b\}-\max\{b,c\}\right|\leq \left|a-c\right|$.
    
Using Assumptions 3 and 4, we therefore deduce that
\begin{align}
\sum_{t=0}^\infty\left\|\mathbb{E}\left[\Xi(w,r)-\Xi(w,\bar{r})|w_0=w\right]-\mathbb{E}\left[\Xi(w_0,r)-\Xi(w_0,\bar{r})\right\|\right]\leq c\left\|r-\bar{r}\right\|(1+\left\|w\right\|^l).
\end{align}

Part 2 is assured by Lemma \ref{projection_F_contraction_lemma} while Part 4 is assured by Lemma \ref{value_difference_Q_difference} and lastly Part 8 is assured by Lemma \ref{iteratation_property_lemma}.
\end{proof}


\clearpage


% \begin{center}
% \begin{tabularx}{\textwidth} { 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X | }
%  \hline
%  \textbf{Baseline} & \textbf{Paper} & \textbf{Link} & \textbf{Continuous/Discrete?} \\\hline\hline
%   MAPPO   & - & - &- \\
%   LIIR   & 
% LIIR & https://github.com/yalidu/liir &- \\
%   MAVEN   & - &  https://github.com/AnujMahajanOxf/MAVEN &- \\
% Multi-explore   & - & https://github.com/shariqiqbal2810/Multi-Explore &- \\    
% ICQL   & https://arxiv.org/pdf/1906.02138.pdf &  &- \\    
%     QTRAN   & - & https://github.com/Sonkyunghwan/QTRAN.
%  &- \\
%     QMIX   & https://github.com/oxwhirl/wqmix & - &- \\    
%     indep. q learners   & https://github.com/oxwhirl/pymarl & - &- \\
%     indep. actor-critic learners   & - & - &- \\
%   \\ [1ex] 
%  \hline
% \end{tabularx}
% \end{center}





% \begin{figure}[h!]
% % \begin{table}[htb!]
% 	%\setlength{\extrarowheight}{2pt}
% \hspace{-40 mm}	\subfloat[$s(1)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(1)}$  & $a^2_{s(1)}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(1)}$ & $2$ & $0$ \\\cline{2-4}
% 		& $b^2_{s(1)}$ & $0$ & $1$ \\\cline{2-4}
% 	\end{tabular}}
% 		\subfloat[$s(2)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(2)}$  & $a^2_{s(2)}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(2)}$ & $-1$ & $0$ \\\cline{2-4}
% 		& $b^2_{s(2)}$ & $0$ & $3$ \\\cline{2-4}
% 	\end{tabular}}
% 			\subfloat[$s(2')$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(2')}$  & $a^2_{s(2')}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(2')}$ & $-1$ & $0$ \\\cline{2-4}
% 		& $b^2_{s(2')}$ & $0$ & $3$ \\\cline{2-4}
% 	\end{tabular}}
% 		\subfloat[$s(3)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(3)}$  & $a^2_{s(3)}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(3)}$ & $5$ & $2$ \\\cline{2-4}
% 		& $b^2_{s(3)}$ & $2$ & $2$ \\\cline{2-4}
% 	\end{tabular}}
% 	\subfloat[$s(3)$]{	\begin{tabular}{*{4}{c|}}
% 		\multicolumn{2}{c}{} 
% % 		& \multicolumn{2}{c}{agent $Y$}
% 		\\\cline{3-4}
% 		\multicolumn{1}{c}{} &  & $a^1_{s(3')}$  & $a^2_{s(3')}$ 
% 		\\\cline{2-4}
% % 		\multirow{2}*{agent $X$} 
% 		& $b^1_{s(3')}$ & $-100$ & $-50$ \\\cline{2-4}
% 		& $b^2_{s(3')}$ & $-50$ & $-50$ \\\cline{2-4}
% 	\end{tabular}}
% 	    \caption{\textbf{Example: Failure of CT-DE methods} Three agent Dec-POMPD. agent 3 has an action set $\{x,y\}$ and randomises so that $x$ is played with probability $p$ and $y$ is played with probability $1-p$. All agents observe the current payoff matrix (but not the state), call this observation $z(s)$. Notice that since the agent 3 action is hidden variable; $z(s(2))=z(s(2'))$ and $z(s(3))=z(s(3'))$ so that these states are indistinguishable (they are all in the information set at that round). We have the following (incomplete) transition laws: $P(s(2)|\cdot,a_3=x,\cdot)=1, P(s(2')|\cdot,a_3=y,\cdot)=1$ and $P(s(3)|s(2),(a^2_{s(2)},b^2_{s(2)}))=1, P(s(3')|s(2'),,(a^2_{s(2')},b^2_{s(2')})))=1$. The dec-POMDP solution is a Bayes Nash equilibrium, that is the agents maximise using the distribution induced by $p$. A centralised critic observes both all actions and the state.
% 	   % The stable points are $(a^1_{s(1)},b^1_{s(1)}),(a^2_{s(1)},b^2_{s(1)})$, $(a^2_{s(2)},b^1_{s(2)}),(a^1_{s(2)},b^2_{s(2)})$ and $(a^1_{s(2)},b^1_{s(2)}),(a^1_{s(2)},b^2_{s(2)})$. Fully decentralised methods in general converge to either stable point.}
% 	   }
% 	    %\label{fig:classic_games}
% 	    \label{fig:classic_games_potential}
% 	    \vspace{-10pt}
% \end{figure}

% \begin{itemize}
%     \item \textbf{Intuitive simple scenarios covering each challenge:} See Sec. 6.1 - 6.3 in Learning to Shape Rewards using a Game of Switching Controls (us, 2021). 

% % \begin{itemize}
% %         \item MARL coordination in outcomes (social welfare)
% %         \item MARL coordinated exploration
% %         \item Others?
% %     \end{itemize} 
% \end{itemize}
% \begin{enumerate}
% %     \item \textbf{Stochastic Matrix games} (show CT-DE failure). See Fig. 1.
    
% % \item  \textbf{Batch of maze environments} 
    
% %     \textbf{1} Test joint coordination - stochastic coordination game  (show convergence to socially optimal outcome). 
    
% %     \textit{\textbf{Outputs:} Show convergence point and performance curves.}\\ 
    
%     \textbf{2}. Coordinated exploration (show optimally coordinated exploration)
    
%     \textit{\textbf{Outputs:} heatmap of exploration.}\\
    
%     \textbf{3}. Show sparse reward setting (convergence to optimal joint policy) 
    
%     \textit{\textbf{Outputs:} Performance curves.}\\

% % \textbf{4}. Non monotonic setting SGs.
%     \item  \textbf{Suite of StarCraft experiments}: Sparse reward environment, coordination experiment,.... \textbf{Outputs:} Performance curves.
%     \item  \textbf{Multi-agent Mujoco.} \textbf{Outputs:} Performance curves.
%     \item \textbf{Merge env.? }(CT-DE failure) \textbf{Outputs:} Collision rate.
%     \item \textbf{ABLATION STUDIES:}
%     \begin{enumerate}
%         \item \textbf{1:Adaptivity}. when MARL agents use different types of policies (e.g. more stochastic) the framework adapts. \textbf{Outputs:} performance curves 
%     \item \textbf{Switching control ablation}. As in our single-agent submission--- show this removing the switching control reduces performance. \\\textbf{Outputs:} performance curves
%     \item \textbf{Functional form of shaping reward ablation} --- show this doesn't matter. \textbf{Outputs:} performance curves
%         \item \textbf{Choice of (MA)RL policy - plug \& play} --- show choice of MARL algorithm doesn't matter. \\\textbf{Outputs:} performance curves
% \end{enumerate}
%     \end{enumerate}
% % \section{Appendix}
% \clearpage
% \addcontentsline{toc}{section}{Additional Experiments} 
% \part{{\Large{Additional Experiments}}}

% \parttoc


% \newpage

% \subsubsection{Experiment 2: Optimal Joint Policies}\label{exp_2:coordination}
% \begin{center}
% \begin{tabular}{|c|c|c|} 
%   \hline
%   \textbf{Agent 2} & Top & Bottom \\ 
%   \hline
%   Top & $r/2$ & $-r$ \\ 
%   \hline
%   Bottom & $r$ & $R$ \\ 
%   \hline
% \end{tabular}
% \end{center}
% \begin{wrapfigure}{R}{0.6\textwidth}
%     \centering
%     % \vspace{-4mm}
%     \begin{tabular}{|c|c|c|} 
%   \hline
%   \textbf{Agent 1/Agent 2} & Top & Bottom \\ 
%   \hline
%   Top & $r/2,r/2$ & $r.-r$ \\ 
%   \hline
%   Bottom & $-r,r$ & $R,R$ \\ 
%   \hline
% \end{tabular}
%     % \includegraphics[scale=0.22]{Figures/coordinated.jpg}
%     \caption{TBA.}
%     \label{Figure:LearningCurves}
%     \vspace{-5mm}
% \end{wrapfigure}
% 
% 
% \subsubsection{Experiment 4: Subgoal discovery}
% \begin{wrapfigure}{R}{0.6\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[scale=0.5]{Figures/LBF_aac.png}
%     \caption{TBA.}
%     \label{Figure:LearningCurves}
% \end{wrapfigure}
%  \newline
% \textbf{Experiment 4: Subgoal discovery. }In this experiment, we tested our claim that LIGS can promote learning subgoals in MAS. For this, we use a modified version of the Level-based Foraging environment with two agents and a single reward (apple); all of them spawned at random positions. Each agent has a level $a_i$. Initially, the apple level is set to $K\!>\!a_0\!+\!a_1$ -- even if all agents enact the {\fontfamily{cmss}\selectfont collect} action simultaneously, they will not be able to obtain it. To solve the problem, the agents need to \textit{jointly} discover and perform a subgoal task which consists of finding and \textit{jointly} pressing a button which is randomly placed in the grid. Once pressed by all agents, the button sets the apple level to $K\!=\!a_0\!+\!a_1$ which then allows the game to be solved. \textcolor{red}{PLOT DESCRIPTION}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% SMAC %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}