\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.65in]{geometry}


\usepackage{authblk}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{pifont}
\usepackage{multicol}
\usepackage{siunitx}
\usepackage{textcomp}
\usepackage{bbm}
\usepackage{algorithm2e}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage[]{algorithm2e}
\usepackage{algorithmic}
\usepackage{tcolorbox}
\usepackage{tikz,pgfplots,tikz-3dplot}
\usetikzlibrary{arrows,shapes,positioning,calc,intersections,through}\tdplotsetmaincoords{55}{110}   
\pgfplotsset{compat=newest} 
\pgfplotsset{plot coordinates/math parser=false} 
% \usepackage{IEEEtrantools}
% \usepackage[cal=boondoxo]{mathalfa}
\allowdisplaybreaks
\newtheorem{experiment}{Experiment}
\newtheorem{proof}{Proof}
% \newtheorem*{remark-non}{Remark}
% \newenvironment{proof}{
%   \renewcommand{\proofname}{Sketch}\proof}{\endproof}
% \newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
  \renewcommand\customgenericname{#2}%
  \renewcommand\theinnercustomgeneric{##1}%
  \innercustomgeneric
  }
  {\endinnercustomgeneric}
}
\newcustomtheorem{customtheorem}{Theorem}
\newcustomtheorem{customlemma}{Lemma}
\newcustomtheorem{customproposition}{Proposition}
\newcommand{\boldpi}{\boldsymbol{\pi}}
\newcommand{\mc}{\mathcal}
\newcommand{\N}{\mc{N}}
\newcommand{\G}{\mc{G}}
\newcommand{\T}{\top}
\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\orange}{\textcolor{orange}}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=blue,
linktocpage}
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

\usepackage{graphicx}
\usepackage{microtype}
\usepackage{pgf,tikz}
\usepackage{setspace}
\usetikzlibrary{arrows}
\usepackage{tikz-network}
\usepackage{multirow,array}
\usepackage{subfig}
\usepackage[]{algorithm2e}
\usepackage{algorithmic}




\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
%\newtheorem*{proof}{Proof}
%%\newtheorem{theorem}[Theorem]
%%\newtheorem{observation}[Theorem]{Observation}
%\newtheorem*{ndefinition}{Definition}[section]
%\newtheorem*{nlemma}{Lemma}[section]
%\newtheorem*{ncorollary}{Corollary}[section]
%\theoremstyle{empty}
\newtheorem{refexample}{Example}[section]
\newtheorem{refproof}{Proof}[section]
\newtheorem{refcorollary}{Corollary}[section]
\usepackage{tcolorbox}
\usepackage{tikz,pgfplots,tikz-3dplot}
\usetikzlibrary{arrows,shapes,positioning,calc,intersections,through}\tdplotsetmaincoords{55}{110}   
\pgfplotsset{compat=newest} 
\pgfplotsset{plot coordinates/math parser=false} 
% \usepackage{IEEEtrantools}
% \usepackage[cal=boondoxo]{mathalfa}
\allowdisplaybreaks
% \newtheorem*{remark-non}{Remark}
\newtheorem{assumption}{Assumption}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{SPGs_arXiv.bib}




\title{Movers and Shapers: Learning and Solving Multi-Agent Systems with a Reward-Shaping Coordinating Device}
% \author{The Authors}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{David Mguni\footnote{davidmguni@hotmail.com}\footnote{Huawei R\&D UK} 
% \vspace{20 mm}
%   David Mguni\footnote{Corresponding author} \hspace{50 mm}Yaodong Yang\\\vspace{-3 mm}
%   \hspace{-3 mm}Huawei R\&D UK  \hspace{40 mm}Huawei R\&D\\
%   \hspace{20 mm}\texttt{davidmguni@hotmail.com}\hspace{22 mm} \texttt{yaodong.yang@huawei.com} 
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
% }

\begin{document}
\date{}
\maketitle
\begin{abstract}
Joint exploration is central to efficient learning within multi-agent systems. This affects not only how quickly agents can collectively determine the sequence of actions that constitutes their optimal policies but also their ability to coordinate their actions for efficient outcomes. In this paper, we introduce a new method in which a coordinating agent shapes the rewards of all agents. Our automated reward-shaping (RS) framework in which the shaping-reward function is constructed in a novel stochastic game between a coordinator and the set of agents. The coordinator learns both which states to add shaping rewards and their optimal magnitudes while the other agents learn their optimal policy for the task using the shaped rewards. We prove theoretically that our framework, which easily adopts existing RL algorithms, learns to construct a shaping-reward function that is tailored to the task and ensures convergence to higher performing policies for the given task. 
\end{abstract}
\clearpage
\tableofcontents
\clearpage
\section{Learning in Multi-Agent Systems (Team Games) with a Central Reward-Shaping Coordinator}
We consider reward shaping with switching controls in multi-agent team game settings. The central coordinator is an additional player that shapes the rewards of all agents. The central coordinator's payoff includes the team reward and an exploration bonus which takes as its input both the state and joint action of the agents.

With this we consider the following controller objective: 
\begin{align}
v^{\pi^0,\pi^i,\pi^{-i}}_i(s)=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t\left\{R(s_t,\boldsymbol{a}_t)+F(a^0_t,a^0_{t-1})\right\}\Big|s=s_0\right], \qquad \forall i \in\{1,\ldots N\}\nonumber
% \label{RL_controller_objective}
\end{align}

\begin{align*}\nonumber
 v^{\pi^0,\pi^i,\pi^{-i}}_0(s_0,I_0)  = \mathbb{E}_{\pi^0,\pi^i,\pi^{-i}}\left[ \sum_{t=0}^\infty \gamma^t\left(R(s_t,\boldsymbol{a}_t)+F(a^0_t,a^0_{t-1}) +\sum_{k\geq 1}^\infty c(I_t,I_{t-1})\delta^t_{\tau_{2k-1}}
+L(s_t,\boldsymbol{a}_t)\right)\right]&
\\ -\mathbb{E}_{\pi'}\left[\sum_{t=0}^\infty \gamma^tR(s_t,\boldsymbol{a}_t)\right]&.
% \label{P2_obj}
\end{align*}
\subsection{Outputs}
\begin{itemize}
    \item \textbf{Description of scenarios/applications of the framework:} similar to what we did with the single agent reward shaping environment. 
    
        \textbf{So far}
    \begin{itemize}
        \item MARL coordination in outcomes (social welfare)
        \item MARL coordinated exploration
        \item Others?
    \end{itemize} 
\end{itemize}
\begin{itemize}
    \item \textbf{Experiment 1:}  Batch of maze environments testing: \textbf{1} joint coordination (to achieve socially optimal outcome). \textbf{2}. joint exploration. \textbf{3}. trap experiment for exploration-based heuristic (as in our single agent reward-shaping paper). \textbf{4} safe goal and optimal goal experiment
    \item \textbf{Experiment 2:} Large scale multi-agent team game which requires coordination e.g. StarCraft
    \item \textbf{Experiment 3:} Complex multi-agent environment with sparse rewards e.g. Google football
    \item \textbf{Ablation study 1:} \textit{Adaptivity}. when MARL agents use different types of policies (e.g. more stochastic) the framework adapts 
    \item \textbf{Ablation study 2:} \textit{Switching control ablation}. As in our single-agent submission
\end{itemize}

\section{Computing Actual Saddle Point Equilibria in Zero-Sum Games Continuous Actions}
We consider reward-shaping in a one-shot zero-sum game with continuous actions. Gradient-based solvers offer the potential for quick and efficient computation, however they are known to often converge to other stationary points that do not correspond to equilibria. We include our reward-shaping methodology. The learning method uses a variant of stochastic fictitious play \url{https://www.sciencedirect.com/science/article/pii/S0022053114000623} 
(see \url{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364177} for the algorithm).
Note that now the policy has the joint action as an input instead of the state.) which now includes a shaping-reward agent. The goal of the shaping reward agent is to induce convergence to a saddle point equilibrium. Similar to before, inducing exploration is a key mechanism for the shaping reward agent.

\section{Equilibrium Selection in Coordination Games}
Equilibrium selection is a fundamental problem within game theory. In refers to the fact that typically, games have more than one Nash equilibrium however some of these Nash equilibria are preferred since they correspond to strategies likely to be realised in real-world settings or yield higher system efficiency. Nevertheless, game-theoretic algorithms that compute Nash equilibria do not exhibit such preferences. We employ our reward shaping methodology to induce convergence to certain equilibria. We tackle discrete games both in the normal form setting (coordination games) and stochastic game setting (stochastic team games and more generally, stochastic potential games).



\printbibliography
\end{document}
