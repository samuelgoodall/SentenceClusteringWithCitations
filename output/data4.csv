Foldername,sentenceID,sentence,citations,citation_titles,citation_authors,PaperID,ParagraphID,Bibliography used
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2011.04942,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,"As usual in admissible set theory (see \cite{Bar}), we write xso that $L_{\alpha}",['Bar'],[''],[''],urn:uuid:99a399dc-5eb1-3d0a-a951-caec3124f4a8,urn:uuid:79341262-de3b-32e4-bdcb-e922cf197d97,usable_dataset/2011.04942/20210528_Decision_times_amended_6_August.bbl
usable_dataset/2003.04738,urn:uuid:fc888183-2309-3255-8344-8171600c9261,"  A priori, this is just a set, but if  is sufficiently small (i.e., ‚Äúneat‚Äù in the sense of <cit.>), _K(G,X) can be canonically written as a finite disjoint union of hermitian symmetric domains.[If  fails to be sufficiently small, one might very reasonably argue that our definition of the Shimura variety of level  really is the definition of the coarse Shimura variety and that one should be working with stacks instead.]","['borelarith', 'pink']","['', '']","['', '']",urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:1705b94b-fa7a-3104-82f2-8a06f014ca45,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:b42a1849-1b71-32e5-b15b-753d03685b5b,  More detailed explanations may be found in <cit.>.,['milne-isv'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f94ba135-a328-3552-b444-f5e9d917eca5,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3d063580-44cb-390f-8e03-efcce177af66,"   For a (connected) reductive group  over , we denote by ‚Ñ¨(G,K)‚Ñ¨^red(G,K)GK <cit.>.",['bt-ii'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:eaadd445-cda2-3a89-a892-9f1d9b385fad,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:4406164c-3519-3e32-9be9-48adff923c0d,"kisin-pappas (originally in <cit.>), the points of ‚Ñ¨((V),K)(‚Ñí,c), i.e.,  
  * ‚àÖ‚Ñíùí™VŒõ‚àà‚ÑíœñŒõ‚àà‚Ñí), 
  * c‚Ñí‚Üí(œñ^nŒõ)=c(Œõ)+n.",['zbMATH03900941'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:564da421-ceab-34e2-83ec-4645d062be2a,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,Note that this is (a variant of) a well-known theorem by van Dantzig if N={1} <cit.>.,['vandantzig'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,By Kisin-Pappas <cit.> we do actually have such a diagram in our situation.,['kisin-pappas'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,"In <cit.>, Pappas and Zhu give a construction of the local model in quite a general context, in particular with no assumptions going beyond our running assumptions <ref>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3c89a2b4-14a6-3525-a3e1-5d2e67b3f895,"   The local model M^loc_G,Œº,KX_Œº‚äÜ_G√ó__p E__K,_p‚äó__pùí™_E__K,_p:=__K,__p^1‚äó_^1__p, u‚Ü¶ p_p is a base change of the global affine Gra√ümannian as defined in <cit.>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f389843d-eccf-3817-8603-fb5d31cbf9fb,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2005.00288,urn:uuid:d957f66d-cf9c-37e6-8ead-25f51713a1ff," SNNs have been a topic of active research with advances in architecture designs <cit.>, training methodologies <cit.>, <cit.> and practical deployments <cit.>, <cit.>.","['tavanaei2019deep', 'neil2016learning', 'wu2019direct', 'ankit2017resparc', 'lin2018mapping']","['', '', '', '', '']","['', '', '', '', '']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:b7a5b751-65ce-37eb-a075-f075c3c822c7,They have been successfully applied to diverse pattern recognition tasks from object recognition <cit.> to EEG classification <cit.>.,"['cao2015spiking', 'antelis2020spiking']","['', '']","['', '']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:034d2c0d-10d4-3a1e-816b-236143cfd382,Recently there have been works that show SNNs achieve comparable performances as the deep neural network architectures like VGG16 and Res-Nets for image classification tasks <cit.>.,['sengupta2019going'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0fa1c87d-23b3-34b7-991a-d40e47e39a80,"<cit.> presented a first spike-based object detection model, called Spiking-YOLO.",['kim2019spiking'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9756de44-a8b7-3a08-8b65-37e172cb4b61,"<cit.> proposed a modified back-propagation based approach where the membrane potentials are treated as differentiable signals, and discontinuities at spike times are considered as noise.",['lee2016training'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:84985dd6-b91f-330f-a15d-00ce001420f5,There is also a training methodology that considers the relative timing between the pre-synaptic and post-synaptic spikes to influence the training of synaptic weights via an asymmetric learning window <cit.>.,['kempter1999hebbian'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0c16d61c-2c02-365c-91fe-df833b0ea158,"<cit.> proposed an SNN training algorithm with continuous integration, which can handle input spikes with temporal information.",['yin2017algorithm'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:91138c59-f4b5-3ab7-be5e-37664638216c,"There are also Hebbian learning <cit.> based approaches that enable local training, unlike back-propagation.",['gupta2009hebbian'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:5205803d-112c-34e7-a006-a3216aeed6f8,<cit.> and was later applied and popularized for deep neural networks by Hinton et al.,['bucilua2006model'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:787f49fe-9c3f-3c15-b4c9-f5feea1a04d0,<cit.>.,['hinton2015distilling'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:f217319d-57b6-3530-865e-967c58fadfa1,<cit.> use the final softmax outputs of a larger ANN to learn a smaller model by minimizing the squared difference between their activations.,['bucilua2006model'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:1b2dabd8-d546-370c-8941-822e59e1f11c,The method in <cit.> is a more general one where the output logits are scaled by a temperature parameter to obtain soft thresholds and then used in the loss.,['hinton2015distilling'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0ea8f2a3-fb4a-3f94-9f22-724ec6fd233d,KD has been an active area of research with major applications in model compression <cit.>.,['cheng2017survey'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:dea92d49-d199-39aa-a46b-eec35af1a5f3,"There have also been works that demonstrate improvement in the performance of a model when using KD based learning, e.g., in object detection <cit.>.",['chen2017learning'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:10296ff6-80be-35ba-81ad-c8329d0f618c,"In addition to this, KD has also been used to improve the performance of various pattern recognition tasks when used as a training methodology for language understanding in <cit.> and sequence models in <cit.>.","['liu2019improving', 'huang2018knowledge']","['', '']","['', '']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:e8216b29-634d-3eb1-bb2a-1ca966cd409b,"<cit.> proposed that instead of minimizing cross-entropy with observed data, it is better to minimize cross-entropy with teacher's probability distribution.",['kim2016sequence'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9fc3c33d-9550-3a02-9155-67fdc63db541,<cit.> presents improvements in knowledge distillation by using an intermediate network they call teacher assistant.,['mirzadeh2019improved'],[''],[''],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2003.04738,urn:uuid:fc888183-2309-3255-8344-8171600c9261,"  A priori, this is just a set, but if  is sufficiently small (i.e., ‚Äúneat‚Äù in the sense of <cit.>), _K(G,X) can be canonically written as a finite disjoint union of hermitian symmetric domains.[If  fails to be sufficiently small, one might very reasonably argue that our definition of the Shimura variety of level  really is the definition of the coarse Shimura variety and that one should be working with stacks instead.]","['borelarith', 'pink']","['', '']","['', '']",urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:1705b94b-fa7a-3104-82f2-8a06f014ca45,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:b42a1849-1b71-32e5-b15b-753d03685b5b,  More detailed explanations may be found in <cit.>.,['milne-isv'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f94ba135-a328-3552-b444-f5e9d917eca5,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3d063580-44cb-390f-8e03-efcce177af66,"   For a (connected) reductive group  over , we denote by ‚Ñ¨(G,K)‚Ñ¨^red(G,K)GK <cit.>.",['bt-ii'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:eaadd445-cda2-3a89-a892-9f1d9b385fad,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:4406164c-3519-3e32-9be9-48adff923c0d,"kisin-pappas (originally in <cit.>), the points of ‚Ñ¨((V),K)(‚Ñí,c), i.e.,  
  * ‚àÖ‚Ñíùí™VŒõ‚àà‚ÑíœñŒõ‚àà‚Ñí), 
  * c‚Ñí‚Üí(œñ^nŒõ)=c(Œõ)+n.",['zbMATH03900941'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:564da421-ceab-34e2-83ec-4645d062be2a,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,Note that this is (a variant of) a well-known theorem by van Dantzig if N={1} <cit.>.,['vandantzig'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,By Kisin-Pappas <cit.> we do actually have such a diagram in our situation.,['kisin-pappas'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,"In <cit.>, Pappas and Zhu give a construction of the local model in quite a general context, in particular with no assumptions going beyond our running assumptions <ref>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3c89a2b4-14a6-3525-a3e1-5d2e67b3f895,"   The local model M^loc_G,Œº,KX_Œº‚äÜ_G√ó__p E__K,_p‚äó__pùí™_E__K,_p:=__K,__p^1‚äó_^1__p, u‚Ü¶ p_p is a base change of the global affine Gra√ümannian as defined in <cit.>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f389843d-eccf-3817-8603-fb5d31cbf9fb,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2005.00288,urn:uuid:d957f66d-cf9c-37e6-8ead-25f51713a1ff," SNNs have been a topic of active research with advances in architecture designs <cit.>, training methodologies <cit.>, <cit.> and practical deployments <cit.>, <cit.>.","['tavanaei2019deep', 'neil2016learning', 'wu2019direct', 'ankit2017resparc', 'lin2018mapping']","[""``Deep learning in spiking neural networks,''"", ""``Learning to be efficient: Algorithms for training low-latency, low-compute deep spiking neural networks,''"", ""``Direct training for spiking neural networks: Faster, larger, better,''"", ""``Resparc: A reconfigurable and energy-efficient architecture with memristive crossbars for deep spiking neural networks,''"", ""``Mapping spiking neural networks onto a manycore neuromorphic architecture,''""]","['Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh', 'Daniel Neil and Michael Pfeiffer and Shih-Chii Liu', 'Yujie Wu and Lei Deng and Guoqi Li and Jun Zhu and Yuan Xie and Luping Shi', 'Aayush Ankit and Abhronil Sengupta and Priyadarshini Panda and Kaushik Roy', 'Chit-Kwan Lin and Andreas Wild and Gautham N Chinya and Tsung-Han Lin and Mike Davies and  Hong Wang']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:b7a5b751-65ce-37eb-a075-f075c3c822c7,They have been successfully applied to diverse pattern recognition tasks from object recognition <cit.> to EEG classification <cit.>.,"['cao2015spiking', 'antelis2020spiking']","[""``Spiking deep convolutional neural networks for energy-efficient object recognition,''"", ""``Spiking neural networks applied to the classification of motor tasks in eeg signals,''""]","['Yongqiang Cao and Yang Chen and Deepak Khosla', 'and et al']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:034d2c0d-10d4-3a1e-816b-236143cfd382,Recently there have been works that show SNNs achieve comparable performances as the deep neural network architectures like VGG16 and Res-Nets for image classification tasks <cit.>.,['sengupta2019going'],"[""``Going deeper in spiking neural networks: Vgg and residual architectures,''""]",['Abhronil Sengupta and Yuting Ye and Robert Wang and Chiao Liu and Kaushik Roy'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0fa1c87d-23b3-34b7-991a-d40e47e39a80,"<cit.> presented a first spike-based object detection model, called Spiking-YOLO.",['kim2019spiking'],"[""``Spiking-yolo: Spiking neural network for real-time object detection,''""]",['Seijoon Kim and Seongsik Park and Byunggook Na and Sungroh Yoon'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9756de44-a8b7-3a08-8b65-37e172cb4b61,"<cit.> proposed a modified back-propagation based approach where the membrane potentials are treated as differentiable signals, and discontinuities at spike times are considered as noise.",['lee2016training'],"[""``Training deep spiking neural networks using backpropagation,''""]",['Jun Haeng Lee and Tobi Delbruck and Michael Pfeiffer'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:84985dd6-b91f-330f-a15d-00ce001420f5,There is also a training methodology that considers the relative timing between the pre-synaptic and post-synaptic spikes to influence the training of synaptic weights via an asymmetric learning window <cit.>.,['kempter1999hebbian'],"[""``Hebbian learning and spiking neurons,''""]",['Richard Kempter and Wulfram Gerstner and J Leo Van Hemmen'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0c16d61c-2c02-365c-91fe-df833b0ea158,"<cit.> proposed an SNN training algorithm with continuous integration, which can handle input spikes with temporal information.",['yin2017algorithm'],"[""``Algorithm and hardware design of discrete-time spiking neural networks based on back propagation with binary activations,''""]",['Shihui Yin and Shreyas K Venkataramanaiah and Gregory K Chen and Ram Krishnamurthy'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:91138c59-f4b5-3ab7-be5e-37664638216c,"There are also Hebbian learning <cit.> based approaches that enable local training, unlike back-propagation.",['gupta2009hebbian'],"[""``Hebbian learning with winner take all for spiking neural networks,''""]",['Ankur Gupta and Lyle N Long'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:5205803d-112c-34e7-a006-a3216aeed6f8,<cit.> and was later applied and popularized for deep neural networks by Hinton et al.,['bucilua2006model'],"[""``Model compression,''""]",['Cristian Bucilu«é and Rich Caruana and Alexandru Niculescu-Mizil'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:787f49fe-9c3f-3c15-b4c9-f5feea1a04d0,<cit.>.,['hinton2015distilling'],"[""``Distilling the knowledge in a neural network,''""]",['Geoffrey Hinton and Oriol Vinyals and Jeff Dean'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:f217319d-57b6-3530-865e-967c58fadfa1,<cit.> use the final softmax outputs of a larger ANN to learn a smaller model by minimizing the squared difference between their activations.,['bucilua2006model'],"[""``Model compression,''""]",['Cristian Bucilu«é and Rich Caruana and Alexandru Niculescu-Mizil'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:1b2dabd8-d546-370c-8941-822e59e1f11c,The method in <cit.> is a more general one where the output logits are scaled by a temperature parameter to obtain soft thresholds and then used in the loss.,['hinton2015distilling'],"[""``Distilling the knowledge in a neural network,''""]",['Geoffrey Hinton and Oriol Vinyals and Jeff Dean'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0ea8f2a3-fb4a-3f94-9f22-724ec6fd233d,KD has been an active area of research with major applications in model compression <cit.>.,['cheng2017survey'],"[""``A survey of model compression and acceleration for deep neural networks,''""]",['Yu Cheng and Duo Wang and Pan Zhou and Tao Zhang'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:dea92d49-d199-39aa-a46b-eec35af1a5f3,"There have also been works that demonstrate improvement in the performance of a model when using KD based learning, e.g., in object detection <cit.>.",['chen2017learning'],"[""``Learning efficient object detection models with knowledge distillation,''""]",['Guobin Chen and Wongun Choi and Xiang Yu and Tony Han and Manmohan Chandraker'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:10296ff6-80be-35ba-81ad-c8329d0f618c,"In addition to this, KD has also been used to improve the performance of various pattern recognition tasks when used as a training methodology for language understanding in <cit.> and sequence models in <cit.>.","['liu2019improving', 'huang2018knowledge']","[""``Improving multi-task deep neural networks via knowledge distillation for natural language understanding,''"", ""``Knowledge distillation for sequence model.,''""]","['Xiaodong Liu and Pengcheng He and Weizhu Chen and Jianfeng Gao', 'Mingkun Huang and Yongbin You and Zhehuai Chen and Yanmin Qian and Kai Yu']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:e8216b29-634d-3eb1-bb2a-1ca966cd409b,"<cit.> proposed that instead of minimizing cross-entropy with observed data, it is better to minimize cross-entropy with teacher's probability distribution.",['kim2016sequence'],"[""``Sequence-level knowledge distillation,''""]",['Yoon Kim and Alexander M Rush'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9fc3c33d-9550-3a02-9155-67fdc63db541,<cit.> presents improvements in knowledge distillation by using an intermediate network they call teacher assistant.,['mirzadeh2019improved'],"[""``Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher,''""]",['Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Hassan Ghasemzadeh'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2003.04738,urn:uuid:fc888183-2309-3255-8344-8171600c9261,"  A priori, this is just a set, but if  is sufficiently small (i.e., ‚Äúneat‚Äù in the sense of <cit.>), _K(G,X) can be canonically written as a finite disjoint union of hermitian symmetric domains.[If  fails to be sufficiently small, one might very reasonably argue that our definition of the Shimura variety of level  really is the definition of the coarse Shimura variety and that one should be working with stacks instead.]","['borelarith', 'pink']","['', '']","['', '']",urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:1705b94b-fa7a-3104-82f2-8a06f014ca45,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:b42a1849-1b71-32e5-b15b-753d03685b5b,  More detailed explanations may be found in <cit.>.,['milne-isv'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f94ba135-a328-3552-b444-f5e9d917eca5,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3d063580-44cb-390f-8e03-efcce177af66,"   For a (connected) reductive group  over , we denote by ‚Ñ¨(G,K)‚Ñ¨^red(G,K)GK <cit.>.",['bt-ii'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:eaadd445-cda2-3a89-a892-9f1d9b385fad,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:4406164c-3519-3e32-9be9-48adff923c0d,"kisin-pappas (originally in <cit.>), the points of ‚Ñ¨((V),K)(‚Ñí,c), i.e.,  
  * ‚àÖ‚Ñíùí™VŒõ‚àà‚ÑíœñŒõ‚àà‚Ñí), 
  * c‚Ñí‚Üí(œñ^nŒõ)=c(Œõ)+n.",['zbMATH03900941'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:564da421-ceab-34e2-83ec-4645d062be2a,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,Note that this is (a variant of) a well-known theorem by van Dantzig if N={1} <cit.>.,['vandantzig'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,By Kisin-Pappas <cit.> we do actually have such a diagram in our situation.,['kisin-pappas'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,"In <cit.>, Pappas and Zhu give a construction of the local model in quite a general context, in particular with no assumptions going beyond our running assumptions <ref>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3c89a2b4-14a6-3525-a3e1-5d2e67b3f895,"   The local model M^loc_G,Œº,KX_Œº‚äÜ_G√ó__p E__K,_p‚äó__pùí™_E__K,_p:=__K,__p^1‚äó_^1__p, u‚Ü¶ p_p is a base change of the global affine Gra√ümannian as defined in <cit.>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f389843d-eccf-3817-8603-fb5d31cbf9fb,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2005.00288,urn:uuid:d957f66d-cf9c-37e6-8ead-25f51713a1ff," SNNs have been a topic of active research with advances in architecture designs <cit.>, training methodologies <cit.>, <cit.> and practical deployments <cit.>, <cit.>.","['tavanaei2019deep', 'neil2016learning', 'wu2019direct', 'ankit2017resparc', 'lin2018mapping']","[""``Deep learning in spiking neural networks,''"", ""``Learning to be efficient: Algorithms for training low-latency, low-compute deep spiking neural networks,''"", ""``Direct training for spiking neural networks: Faster, larger, better,''"", ""``Resparc: A reconfigurable and energy-efficient architecture with memristive crossbars for deep spiking neural networks,''"", ""``Mapping spiking neural networks onto a manycore neuromorphic architecture,''""]","['Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh', 'Daniel Neil and Michael Pfeiffer and Shih-Chii Liu', 'Yujie Wu and Lei Deng and Guoqi Li and Jun Zhu and Yuan Xie and Luping Shi', 'Aayush Ankit and Abhronil Sengupta and Priyadarshini Panda and Kaushik Roy', 'Chit-Kwan Lin and Andreas Wild and Gautham N Chinya and Tsung-Han Lin and Mike Davies and  Hong Wang']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:b7a5b751-65ce-37eb-a075-f075c3c822c7,They have been successfully applied to diverse pattern recognition tasks from object recognition <cit.> to EEG classification <cit.>.,"['cao2015spiking', 'antelis2020spiking']","[""``Spiking deep convolutional neural networks for energy-efficient object recognition,''"", ""``Spiking neural networks applied to the classification of motor tasks in eeg signals,''""]","['Yongqiang Cao and Yang Chen and Deepak Khosla', 'and et al']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:034d2c0d-10d4-3a1e-816b-236143cfd382,Recently there have been works that show SNNs achieve comparable performances as the deep neural network architectures like VGG16 and Res-Nets for image classification tasks <cit.>.,['sengupta2019going'],"[""``Going deeper in spiking neural networks: Vgg and residual architectures,''""]",['Abhronil Sengupta and Yuting Ye and Robert Wang and Chiao Liu and Kaushik Roy'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0fa1c87d-23b3-34b7-991a-d40e47e39a80,"<cit.> presented a first spike-based object detection model, called Spiking-YOLO.",['kim2019spiking'],"[""``Spiking-yolo: Spiking neural network for real-time object detection,''""]",['Seijoon Kim and Seongsik Park and Byunggook Na and Sungroh Yoon'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9756de44-a8b7-3a08-8b65-37e172cb4b61,"<cit.> proposed a modified back-propagation based approach where the membrane potentials are treated as differentiable signals, and discontinuities at spike times are considered as noise.",['lee2016training'],"[""``Training deep spiking neural networks using backpropagation,''""]",['Jun Haeng Lee and Tobi Delbruck and Michael Pfeiffer'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:84985dd6-b91f-330f-a15d-00ce001420f5,There is also a training methodology that considers the relative timing between the pre-synaptic and post-synaptic spikes to influence the training of synaptic weights via an asymmetric learning window <cit.>.,['kempter1999hebbian'],"[""``Hebbian learning and spiking neurons,''""]",['Richard Kempter and Wulfram Gerstner and J Leo Van Hemmen'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0c16d61c-2c02-365c-91fe-df833b0ea158,"<cit.> proposed an SNN training algorithm with continuous integration, which can handle input spikes with temporal information.",['yin2017algorithm'],"[""``Algorithm and hardware design of discrete-time spiking neural networks based on back propagation with binary activations,''""]",['Shihui Yin and Shreyas K Venkataramanaiah and Gregory K Chen and Ram Krishnamurthy'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:91138c59-f4b5-3ab7-be5e-37664638216c,"There are also Hebbian learning <cit.> based approaches that enable local training, unlike back-propagation.",['gupta2009hebbian'],"[""``Hebbian learning with winner take all for spiking neural networks,''""]",['Ankur Gupta and Lyle N Long'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:5205803d-112c-34e7-a006-a3216aeed6f8,<cit.> and was later applied and popularized for deep neural networks by Hinton et al.,['bucilua2006model'],"[""``Model compression,''""]",['Cristian Bucilu«é and Rich Caruana and Alexandru Niculescu-Mizil'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:787f49fe-9c3f-3c15-b4c9-f5feea1a04d0,<cit.>.,['hinton2015distilling'],"[""``Distilling the knowledge in a neural network,''""]",['Geoffrey Hinton and Oriol Vinyals and Jeff Dean'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:f217319d-57b6-3530-865e-967c58fadfa1,<cit.> use the final softmax outputs of a larger ANN to learn a smaller model by minimizing the squared difference between their activations.,['bucilua2006model'],"[""``Model compression,''""]",['Cristian Bucilu«é and Rich Caruana and Alexandru Niculescu-Mizil'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:1b2dabd8-d546-370c-8941-822e59e1f11c,The method in <cit.> is a more general one where the output logits are scaled by a temperature parameter to obtain soft thresholds and then used in the loss.,['hinton2015distilling'],"[""``Distilling the knowledge in a neural network,''""]",['Geoffrey Hinton and Oriol Vinyals and Jeff Dean'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0ea8f2a3-fb4a-3f94-9f22-724ec6fd233d,KD has been an active area of research with major applications in model compression <cit.>.,['cheng2017survey'],"[""``A survey of model compression and acceleration for deep neural networks,''""]",['Yu Cheng and Duo Wang and Pan Zhou and Tao Zhang'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:dea92d49-d199-39aa-a46b-eec35af1a5f3,"There have also been works that demonstrate improvement in the performance of a model when using KD based learning, e.g., in object detection <cit.>.",['chen2017learning'],"[""``Learning efficient object detection models with knowledge distillation,''""]",['Guobin Chen and Wongun Choi and Xiang Yu and Tony Han and Manmohan Chandraker'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:10296ff6-80be-35ba-81ad-c8329d0f618c,"In addition to this, KD has also been used to improve the performance of various pattern recognition tasks when used as a training methodology for language understanding in <cit.> and sequence models in <cit.>.","['liu2019improving', 'huang2018knowledge']","[""``Improving multi-task deep neural networks via knowledge distillation for natural language understanding,''"", ""``Knowledge distillation for sequence model.,''""]","['Xiaodong Liu and Pengcheng He and Weizhu Chen and Jianfeng Gao', 'Mingkun Huang and Yongbin You and Zhehuai Chen and Yanmin Qian and Kai Yu']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:e8216b29-634d-3eb1-bb2a-1ca966cd409b,"<cit.> proposed that instead of minimizing cross-entropy with observed data, it is better to minimize cross-entropy with teacher's probability distribution.",['kim2016sequence'],"[""``Sequence-level knowledge distillation,''""]",['Yoon Kim and Alexander M Rush'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9fc3c33d-9550-3a02-9155-67fdc63db541,<cit.> presents improvements in knowledge distillation by using an intermediate network they call teacher assistant.,['mirzadeh2019improved'],"[""``Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher,''""]",['Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Hassan Ghasemzadeh'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2003.04738,urn:uuid:fc888183-2309-3255-8344-8171600c9261,"  A priori, this is just a set, but if  is sufficiently small (i.e., ‚Äúneat‚Äù in the sense of <cit.>), _K(G,X) can be canonically written as a finite disjoint union of hermitian symmetric domains.[If  fails to be sufficiently small, one might very reasonably argue that our definition of the Shimura variety of level  really is the definition of the coarse Shimura variety and that one should be working with stacks instead.]","['borelarith', 'pink']","['', '']","['', '']",urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:1705b94b-fa7a-3104-82f2-8a06f014ca45,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:b42a1849-1b71-32e5-b15b-753d03685b5b,  More detailed explanations may be found in <cit.>.,['milne-isv'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f94ba135-a328-3552-b444-f5e9d917eca5,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3d063580-44cb-390f-8e03-efcce177af66,"   For a (connected) reductive group  over , we denote by ‚Ñ¨(G,K)‚Ñ¨^red(G,K)GK <cit.>.",['bt-ii'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:eaadd445-cda2-3a89-a892-9f1d9b385fad,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:4406164c-3519-3e32-9be9-48adff923c0d,"kisin-pappas (originally in <cit.>), the points of ‚Ñ¨((V),K)(‚Ñí,c), i.e.,  
  * ‚àÖ‚Ñíùí™VŒõ‚àà‚ÑíœñŒõ‚àà‚Ñí), 
  * c‚Ñí‚Üí(œñ^nŒõ)=c(Œõ)+n.",['zbMATH03900941'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:564da421-ceab-34e2-83ec-4645d062be2a,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,Note that this is (a variant of) a well-known theorem by van Dantzig if N={1} <cit.>.,['vandantzig'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:73589932-9a50-345f-bc62-a4338bc5ed37,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,By Kisin-Pappas <cit.> we do actually have such a diagram in our situation.,['kisin-pappas'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:3f923ab0-3b92-30ab-bfdc-43f2f53011bc,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,"In <cit.>, Pappas and Zhu give a construction of the local model in quite a general context, in particular with no assumptions going beyond our running assumptions <ref>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:9f7484af-3691-3743-b4d1-99dc8fb80387,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2003.04738,urn:uuid:3c89a2b4-14a6-3525-a3e1-5d2e67b3f895,"   The local model M^loc_G,Œº,KX_Œº‚äÜ_G√ó__p E__K,_p‚äó__pùí™_E__K,_p:=__K,__p^1‚äó_^1__p, u‚Ü¶ p_p is a base change of the global affine Gra√ümannian as defined in <cit.>.",['pappas-zhu'],[''],[''],urn:uuid:50797d20-0f82-3a0a-9748-7087df31a609,urn:uuid:f389843d-eccf-3817-8603-fb5d31cbf9fb,usable_dataset/2003.04738/article-ekor.bbl
usable_dataset/2005.00288,urn:uuid:d957f66d-cf9c-37e6-8ead-25f51713a1ff," SNNs have been a topic of active research with advances in architecture designs <cit.>, training methodologies <cit.>, <cit.> and practical deployments <cit.>, <cit.>.","['tavanaei2019deep', 'neil2016learning', 'wu2019direct', 'ankit2017resparc', 'lin2018mapping']","[""``Deep learning in spiking neural networks,''"", ""``Learning to be efficient: Algorithms for training low-latency, low-compute deep spiking neural networks,''"", ""``Direct training for spiking neural networks: Faster, larger, better,''"", ""``Resparc: A reconfigurable and energy-efficient architecture with memristive crossbars for deep spiking neural networks,''"", ""``Mapping spiking neural networks onto a manycore neuromorphic architecture,''""]","['Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh', 'Daniel Neil and Michael Pfeiffer and Shih-Chii Liu', 'Yujie Wu and Lei Deng and Guoqi Li and Jun Zhu and Yuan Xie and Luping Shi', 'Aayush Ankit and Abhronil Sengupta and Priyadarshini Panda and Kaushik Roy', 'Chit-Kwan Lin and Andreas Wild and Gautham N Chinya and Tsung-Han Lin and Mike Davies and  Hong Wang']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:b7a5b751-65ce-37eb-a075-f075c3c822c7,They have been successfully applied to diverse pattern recognition tasks from object recognition <cit.> to EEG classification <cit.>.,"['cao2015spiking', 'antelis2020spiking']","[""``Spiking deep convolutional neural networks for energy-efficient object recognition,''"", ""``Spiking neural networks applied to the classification of motor tasks in eeg signals,''""]","['Yongqiang Cao and Yang Chen and Deepak Khosla', 'and et al']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:034d2c0d-10d4-3a1e-816b-236143cfd382,Recently there have been works that show SNNs achieve comparable performances as the deep neural network architectures like VGG16 and Res-Nets for image classification tasks <cit.>.,['sengupta2019going'],"[""``Going deeper in spiking neural networks: Vgg and residual architectures,''""]",['Abhronil Sengupta and Yuting Ye and Robert Wang and Chiao Liu and Kaushik Roy'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0fa1c87d-23b3-34b7-991a-d40e47e39a80,"<cit.> presented a first spike-based object detection model, called Spiking-YOLO.",['kim2019spiking'],"[""``Spiking-yolo: Spiking neural network for real-time object detection,''""]",['Seijoon Kim and Seongsik Park and Byunggook Na and Sungroh Yoon'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:5b6a72cc-4f83-3dae-b751-226b005d1494,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9756de44-a8b7-3a08-8b65-37e172cb4b61,"<cit.> proposed a modified back-propagation based approach where the membrane potentials are treated as differentiable signals, and discontinuities at spike times are considered as noise.",['lee2016training'],"[""``Training deep spiking neural networks using backpropagation,''""]",['Jun Haeng Lee and Tobi Delbruck and Michael Pfeiffer'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:84985dd6-b91f-330f-a15d-00ce001420f5,There is also a training methodology that considers the relative timing between the pre-synaptic and post-synaptic spikes to influence the training of synaptic weights via an asymmetric learning window <cit.>.,['kempter1999hebbian'],"[""``Hebbian learning and spiking neurons,''""]",['Richard Kempter and Wulfram Gerstner and J Leo Van Hemmen'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0c16d61c-2c02-365c-91fe-df833b0ea158,"<cit.> proposed an SNN training algorithm with continuous integration, which can handle input spikes with temporal information.",['yin2017algorithm'],"[""``Algorithm and hardware design of discrete-time spiking neural networks based on back propagation with binary activations,''""]",['Shihui Yin and Shreyas K Venkataramanaiah and Gregory K Chen and Ram Krishnamurthy'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:91138c59-f4b5-3ab7-be5e-37664638216c,"There are also Hebbian learning <cit.> based approaches that enable local training, unlike back-propagation.",['gupta2009hebbian'],"[""``Hebbian learning with winner take all for spiking neural networks,''""]",['Ankur Gupta and Lyle N Long'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:f4e19cd9-91c9-372d-b48e-7b21d43dcdaf,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:5205803d-112c-34e7-a006-a3216aeed6f8,<cit.> and was later applied and popularized for deep neural networks by Hinton et al.,['bucilua2006model'],"[""``Model compression,''""]",['Cristian Bucilu«é and Rich Caruana and Alexandru Niculescu-Mizil'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:787f49fe-9c3f-3c15-b4c9-f5feea1a04d0,<cit.>.,['hinton2015distilling'],"[""``Distilling the knowledge in a neural network,''""]",['Geoffrey Hinton and Oriol Vinyals and Jeff Dean'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:f217319d-57b6-3530-865e-967c58fadfa1,<cit.> use the final softmax outputs of a larger ANN to learn a smaller model by minimizing the squared difference between their activations.,['bucilua2006model'],"[""``Model compression,''""]",['Cristian Bucilu«é and Rich Caruana and Alexandru Niculescu-Mizil'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:1b2dabd8-d546-370c-8941-822e59e1f11c,The method in <cit.> is a more general one where the output logits are scaled by a temperature parameter to obtain soft thresholds and then used in the loss.,['hinton2015distilling'],"[""``Distilling the knowledge in a neural network,''""]",['Geoffrey Hinton and Oriol Vinyals and Jeff Dean'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:0ea8f2a3-fb4a-3f94-9f22-724ec6fd233d,KD has been an active area of research with major applications in model compression <cit.>.,['cheng2017survey'],"[""``A survey of model compression and acceleration for deep neural networks,''""]",['Yu Cheng and Duo Wang and Pan Zhou and Tao Zhang'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:dea92d49-d199-39aa-a46b-eec35af1a5f3,"There have also been works that demonstrate improvement in the performance of a model when using KD based learning, e.g., in object detection <cit.>.",['chen2017learning'],"[""``Learning efficient object detection models with knowledge distillation,''""]",['Guobin Chen and Wongun Choi and Xiang Yu and Tony Han and Manmohan Chandraker'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:10296ff6-80be-35ba-81ad-c8329d0f618c,"In addition to this, KD has also been used to improve the performance of various pattern recognition tasks when used as a training methodology for language understanding in <cit.> and sequence models in <cit.>.","['liu2019improving', 'huang2018knowledge']","[""``Improving multi-task deep neural networks via knowledge distillation for natural language understanding,''"", ""``Knowledge distillation for sequence model.,''""]","['Xiaodong Liu and Pengcheng He and Weizhu Chen and Jianfeng Gao', 'Mingkun Huang and Yongbin You and Zhehuai Chen and Yanmin Qian and Kai Yu']",urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:9ef4ef6f-b960-3bf2-9c94-1e3ff89c93b5,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:e8216b29-634d-3eb1-bb2a-1ca966cd409b,"<cit.> proposed that instead of minimizing cross-entropy with observed data, it is better to minimize cross-entropy with teacher's probability distribution.",['kim2016sequence'],"[""``Sequence-level knowledge distillation,''""]",['Yoon Kim and Alexander M Rush'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00288,urn:uuid:9fc3c33d-9550-3a02-9155-67fdc63db541,<cit.> presents improvements in knowledge distillation by using an intermediate network they call teacher assistant.,['mirzadeh2019improved'],"[""``Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher,''""]",['Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Hassan Ghasemzadeh'],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4,usable_dataset/2005.00288/draft.bbl
usable_dataset/2005.00702,urn:uuid:da8bff96-e743-32e2-9b81-b44865db7f1f,"Among manual methods,  tries to achieve obfuscation by instructing humans to either imitate the writing style of some author or just try to hide their own writing style.",['brennan2012adversarial'],['{Adversarial stylometry: Circumventing authorship recognition to preserve privacy and anonymity}'],"[[Person('Brennan, Michael'), Person('Afroz, Sadia'), Person('Greenstadt, Rachel')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:4f26c58a-31db-3ea3-ab80-7d2d24d2e548,"Among semi-automated methods,  presents a tool called Anonymouth which analyzes the input document using an authorship attribution method and then suggest text-based changes.",['mcdonald2012use'],['{Use fewer instances of the letter ‚Äúi‚Äù: Toward writing style anonymization}'],"[[Person('McDonald, Andrew WE'), Person('Afroz, Sadia'), Person('Caliskan, Aylin'), Person('Stolerman, Ariel'), Person('Greenstadt, Rachel')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:ead09e05-f280-36e9-911c-5efac10ebd0b,"In order to achieve obfuscation,  uses rule-based text transformations to push the style of input document towards the average style of corpus.",['karadzhov2017case'],['{The case for being average: A mediocrity approach to style masking and author obfuscation}'],"[[Person('Karadzhov, Georgi'), Person('Mihaylova, Tsvetomila'), Person('Kiprov, Yasen'), Person('Georgiev, Georgi'), Person('Koychev, Ivan'), Person('Nakov, Preslav')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:89eb9aeb-1b8b-3310-a841-20303526b881,"In search-based methods,  uses genetic algorithms (GA) along with an authorship attribution method to identify words in a document, changing which will have the maximum adverse effect on authorship attribution.",['mahmood2019girl'],['{A Girl Has No Name: Automated Authorship Obfuscation using Mutant-X}'],"[[Person('Mahmood, Asad'), Person('Ahmad, Faizan'), Person('Shafiq, Zubair'), Person('Srinivasan, Padmini'), Person('Zaffar, Fareed')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:1f1eaf56-745e-3038-a984-f05dc1d132d8,tries to achieve obfuscation by increasing the stylistic distance between input document and author's existing text.,['bevendorff2019heuristic'],['{Heuristic Authorship Obfuscation}'],"[[Person('Bevendorff, Janek'), Person('Potthast, Martin'), Person('Hagen, Matthias'), Person('Stein, Benno')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:5b606dae-c2d4-3376-b819-b471132e4f40,"Apart from these, there are also a couple of model-based approaches  which train a deep learning model to generate obfuscated documents.","['shetty2018a4nt', 'emmery2018style']","['{A4NT: author attribute anonymity by adversarial training of neural machine translation}', '{Style Obfuscation by Invariance}']","[[Person('Shetty, Rakshith'), Person('Schiele, Bernt'), Person('Fritz, Mario')], [Person('Emmery, Chris'), Person('Manjavacas, Enrique'), Person('Chrupa{\\l}a, Grzegorz')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:9b16a5a7-faa6-3bcd-869b-36a6be1793bd,uses stylometric features to perform obfuscation detection on manually obfuscated documents.,['juola2012detecting'],['Detecting stylistic deception'],"[[Person('Juola, Patrick')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:c5a9adc0-4c22-3ed3-a7d2-e821485d91c6,also performs manual obfuscation detection using stylistic features.,['afroz2012detecting'],"['Detecting hoaxes, frauds, and deception in writing style online']","[[Person('Afroz, Sadia'), Person('Brennan, Michael'), Person('Greenstadt, Rachel')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:de6718d1-c3f8-35de-85c4-46484c74b942,uses stylometric analysis to detect whether a document was generated using text spinners or not.,['shahid2017accurate'],['Accurate detection of automatically spun content via stylometric analysis'],"[[Person('Shahid, Usman'), Person('Farooqi, Shehroze'), Person('Ahmad, Raza'), Person('Shafiq, Zubair'), Person('Srinivasan, Padmini'), Person('Zaffar, Fareed')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:cc8a971a-d0fe-37a5-8d8d-4b2bc22cfb2d,helps humans in detecting whether a given piece of text is written by a human or a particular language model called GPT-2.,['gehrmann2019gltr'],['{GLTR}: Statistical Detection and Visualization of Generated Text'],"[[Person('Gehrmann, Sebastian'), Person('Strobelt, Hendrik'), Person('Rush, Alexander')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:8a3bf656-2e8d-393a-8bb7-a4cfb57aa694,"They achieve this by extracting probability distribution of words given their context, using a pre-trained GPT-2  and then providing this information to humans.",['radford2019language'],['Language models are unsupervised multitask learners'],"[[Person('Radford, Alec'), Person('Wu, Jeffrey'), Person('Child, Rewon'), Person('Luan, David'), Person('Amodei, Dario'), Person('Sutskever, Ilya')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:0f645d4d-05a9-3e3a-9b4c-669e89e98049,performs neural fake news detection by identifying if the given news article is written by a human or their fake news generation model called GROVER.,['zellers2019defending'],['Defending Against Neural Fake News'],"[[Person('Zellers, Rowan'), Person('Holtzman, Ari'), Person('Rashkin, Hannah'), Person('Bisk, Yonatan'), Person('Farhadi, Ali'), Person('Roesner, Franziska'), Person('Choi, Yejin')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.00702,urn:uuid:2fc0b160-bd83-329b-99a9-b5c9e70dcefe,also tries to distinguish between human and language model generated text.,['bakhtin2019real'],['{Real or Fake? Learning to Discriminate Machine from Human Generated Text}'],"[[Person('Bakhtin, Anton'), Person('Gross, Sam'), Person('Ott, Myle'), Person('Deng, Yuntian'), Person(""Ranzato, Marc'Aurelio""), Person('Szlam, Arthur')]]",urn:uuid:69e67307-62d3-35b0-8264-3fea8ae73c5c,urn:uuid:312a08f8-28be-3c89-b6a9-a4b367daa46e,usable_dataset/2005.00702/Asad.bib
usable_dataset/2005.14137,urn:uuid:03c0889a-57e8-3e1f-b88d-b882ebd55025,Boundary Attack¬† is one of the first work that uses final decisions of a classifier to perform blackbox attacks.,['brendel2017decision'],['Decision-based adversarial attacks: Reliable attacks against black-box machine learning models'],"[[Person('Brendel, Wieland'), Person('Rauber, Jonas'), Person('Bethge, Matthias')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:9eb3c66f-54a4-3ccc-abf2-6d1972433ae3,usable_dataset/2005.14137/bibliography.bib
usable_dataset/2005.14137,urn:uuid:009dc301-b5db-31df-bfac-7f5f8c460385,"propose to choose the random perturbation in each step more wisely instead of Gaussian perturbation, using Perlin noise, alpha distribution and DCT respectively.","['brunner2019guessing', 'srinivasan2019black', 'guo2018low']","['Guessing smart: Biased sampling for efficient black-box adversarial attacks', 'Black-Box Decision based Adversarial Attack with Symmetric alpha-stable Distribution', 'Low frequency adversarial perturbation']","[[Person('Brunner, Thomas'), Person('Diehl, Frederik'), Person('Le, Michael Truong'), Person('Knoll, Alois')], [Person('Srinivasan, Vignesh'), Person('Kuruoglu, Ercan E'), Person('M{\\""u}ller, Klaus-Robert'), Person('Samek, Wojciech'), Person('Nakajima, Shinichi')], [Person('Guo, Chuan'), Person('Frank, Jared S'), Person('Weinberger, Kilian Q')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:c6374c6e-9e5a-3d91-b618-494d4ce1ee24,usable_dataset/2005.14137/bibliography.bib
usable_dataset/2005.14137,urn:uuid:eedf3c20-4c55-389d-9349-f153dac044d6,propose a similar idea - approximating the gradient around the boundary using Monte Carlo algorithm.,"['ilyas2018black', 'khalid2019red', 'liu2019geometry', 'chen2019hopskipjumpattack']","['Black-box adversarial attacks with limited queries and information', 'RED-Attack: Resource Efficient Decision based Attack for Machine Learning', 'A geometry-inspired decision-based attack', 'Hopskipjumpattack: A query-efficient decision-based attack']","[[Person('Ilyas, Andrew'), Person('Engstrom, Logan'), Person('Athalye, Anish'), Person('Lin, Jessy')], [Person('Khalid, Faiq'), Person('Ali, Hassan'), Person('Hanif, Muhammad Abdullah'), Person('Rehman, Semeen'), Person('Ahmed, Rehan'), Person('Shafique, Muhammad')], [Person('Liu, Yujia'), Person('Moosavi-Dezfooli, Seyed-Mohsen'), Person('Frossard, Pascal')], [Person('Chen, Jianbo'), Person('Jordan, Michael I'), Person('Wainwright, Martin J')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:c6374c6e-9e5a-3d91-b618-494d4ce1ee24,usable_dataset/2005.14137/bibliography.bib
usable_dataset/2005.14137,urn:uuid:f3ea0930-85a7-362b-ba72-4c0a8efaae27,"proposes to transform the boundary-based output into a continuous metric, so that the score-based attack techniques can be adopted.",['cheng2018query'],['Query-efficient hard-label black-box attack: An optimization-based approach'],"[[Person('Cheng, Minhao'), Person('Le, Thong'), Person('Chen, Pin-Yu'), Person('Yi, Jinfeng'), Person('Zhang, Huan'), Person('Hsieh, Cho-Jui')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:f968bdb5-887d-306c-a8b1-06871c26f3ba,usable_dataset/2005.14137/bibliography.bib
usable_dataset/2005.14137,urn:uuid:16476a3b-cec8-3f6e-9db1-1cfa511d75ce,adopts evolution algorithm to achieve the decision-based attack against face recognition system.,['dong2019efficient'],['Efficient Decision-based Black-box Adversarial Attacks on Face Recognition'],"[[Person('Dong, Yinpeng'), Person('Su, Hang'), Person('Wu, Baoyuan'), Person('Li, Zhifeng'), Person('Liu, Wei'), Person('Zhang, Tong'), Person('Zhu, Jun')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:f968bdb5-887d-306c-a8b1-06871c26f3ba,usable_dataset/2005.14137/bibliography.bib
usable_dataset/2005.14137,urn:uuid:c49a8bb2-997f-38a2-a767-9f5e9591a743,"In¬†, the authors draw intuition from JPEG codec¬† image compression techniques and propose to use discrete cosine transform (DCT) for generating low frequency adversarial perturbations to assist score-based adversarial attack.","['guo2018low', 'wallace1992jpeg']","['Low frequency adversarial perturbation', 'The JPEG still picture compression standard']","[[Person('Guo, Chuan'), Person('Frank, Jared S'), Person('Weinberger, Kilian Q')], [Person('Wallace, Gregory K')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:f74375d4-a75e-3e50-a6ac-53705ee085c6,usable_dataset/2005.14137/bibliography.bib
usable_dataset/2005.14137,urn:uuid:6beb2f91-f461-3c24-b27f-f4094d615c9c,AutoZoom¬† trains an auto-encoder offline with natural images and uses the decoder network as a dimension reduction tool.,['tu2019autozoom'],['Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks'],"[[Person('Tu, Chun-Chen'), Person('Ting, Paishun'), Person('Chen, Pin-Yu'), Person('Liu, Sijia'), Person('Zhang, Huan'), Person('Yi, Jinfeng'), Person('Hsieh, Cho-Jui'), Person('Cheng, Shin-Ming')]]",urn:uuid:9a7fe2d7-759f-3f82-ad21-e86c3c63214b,urn:uuid:f74375d4-a75e-3e50-a6ac-53705ee085c6,usable_dataset/2005.14137/bibliography.bib
