\section{Experiments}\label{sec:experiments}

\noindent\textbf{Overview.} In this section, we present our experimental results. First, in Section \ref{subsec:bigbenchmarks}, we use \tool to securely compute inference on the ImageNet dataset using the following \tensorflow programs: \resnet\footnote{\url{https://github.com/tensorflow/models/tree/master/official/r1/resnet}} and \densenet\footnote{\url{https://github.com/pudae/tensorflow-densenet}}. 
%The computation is performed as a three-party secure computation protocol, where the model and query is secret shared amongst the three parties.
%We stress that no prior work has run MPC on networks of this scale. 
We also show that the performance of semi-honest and malicious protocols generated by \tool scale linearly with the depth of DNNs.
 Second, in Section \ref{sec:prior-comparison}, we show that \cryptflow\ outperforms prior works on secure inference of DNNs. 
%These experiments are on smaller DNNs that perform prediction over the MNIST and CIFAR datasets as prior work could only handle such small benchmarks.
 Next, we evaluate each component of \tool in more detail. 
In Section \ref{sec:athosexperiments}, we show that the fixed-point code generated by Athos matches the accuracy of floating-point  \resnet and \densenet.  We show in Section \ref{subsec:porthosexperiments} how the optimizations in Porthos help it outperform prior works in terms of communication complexity and overall execution time. In Section \ref{subsec:aramisexperiments}, we show the overhead of obtaining malicious secure MPC (over semi-honest security) using Aramis for GMW~\cite{gmw} and Porthos.  
We show Aramis-based malicious secure inference outperforms pure crypto-based  malicious secure protocols by huge margins in Section~\ref{sec:concurrent-comparison}.
Finally, in section \ref{subsec:realworldimpact}, we discuss two case-studies of running \cryptflow\ on DNNs for healthcare.
 We begin by providing details of the systems used to run our experiments.
\\\\
\noindent\textbf{System Details.} All our large benchmark experiments are in a LAN setting on 3.7GHz machines, each with 4 cores and with 16 GB of RAM running Linux Ubuntu 16.04. The measured bandwidth between each of the machines was at most 377 MBps and the latency was sub-millisecond. 
Since we wanted to use the same machines to benchmark both our semi-honest as well as our malicious secure protocols, we were constrained to use machines that had Intel SGX enabled on them - this led to machines that had considerably lower bandwidth between them (377 MBps) than those normally used by prior works in the area (e.g. \cite{aby3, quantizednn} used networks with bandwidth of 1.5 GBps). For Aramis, we used Intel SGX SDK version 2.4.
The compilation time of \tool is around 5 sec for \resnet, 35 sec for \densenet and 2 minutes for {{\textsc{ResNet200}}\xspace}.
% remains under 40 seconds for our benchmarks.
% \nc{A line here about the code being available...}


%Through this evaluation, we want to demonstrate the following:
%\begin{itemize}
%\item Section 6.1 shows that we can run state of the art neural networks securely using cryptflow.
%We show imagenet scale predictions using some of the most famous DNNs (ResNet50, VGG, MobileNets).
%No prior published work has run MPC on networks of this scale.
%\item Section 6.2 shows that cryptflow is far superior to other alternatives performance wise. 
%These experiments are on smaller MNIST/CIFAR DNNs as prior work has handled only those.
%Maybe we also show that cryptflow is much easier to use than other frameworks.
%\item Section 6.3 shows that Athos' scale exploration is critical for obtaining a good fixed-point network.
%We also show a sanity check that fixed-point runs much faster than floating-point in MPC.
%\item Section 6.4 uses microbenchmarks to evaluate Porthos against state-of-the-art 2PC and 3PC crypto protocols.
%\item Section 6.5 shows overhead of malicious security using SGX. 
%\end{itemize}
\subsection{Secure Inference on ImageNet}\label{subsec:bigbenchmarks}
%In this section, we show the power of \cryptflow, by demonstrating secure inference of \resnet and \densenet over the ImageNet dataset with over 1000 classes. 
We briefly describe our  benchmarks and then present performance results.
\begin{enumerate}
\item \resnet~\cite{resnet} is a network that follows the residual neural network architecture. 
The residual nodes employ ``skip connections'' or short cuts between layers.
% in order to avoid the problem of vanishing gradients while training.
 It consists of 53 convolution layers with filters of size up to $7\times 7$, and 1 fully connected layer of size $2048\times 1001$. The activation function between most layers is batch normalization (Appendix~\ref{appendix:batchnorm}) followed by ReLU. After the first convolutional layer, the activation function also includes a MaxPool.

\item \densenet~\cite{densenet} is a form of residual neural network that employs several parallel skips. It consists of 121 convolutional layers with filters of size up to $7\times 7$. The activation function between these layers is usually batch normalization, followed by ReLU. Some layers also use MaxPool or AvgPool. 

%\item \squeezenet~\cite{squeezenet} is a notoriously hard to train network and no prior work has considered evaluating this architecture on ImageNet with %fixed-point arithmetic.
%It consists of 26 convolutional layers with filters of size up to $3\times 3$. The activation function between these layers is usually ReLU and MaxPool. 
\end{enumerate}~\\
\noindent\textbf{Performance.} Table \ref{tab:bigbenchmarks} shows performance of \cryptflow\ on these benchmarks.
% within a range of 25--36 seconds with semi-honest security and 75--112 seconds with malicious security.
 We measure communication as total communication between all $3$ parties - each party roughly communicates a third of this value. The communication in semi-honest secure and malicious secure inference is almost the same. Thus demonstrating that ImageNet scale inference can be performed in about 30 seconds with semi-honest security and in under two minutes with malicious security. The malicious protocol of \tool is about 3x slower than the semi-honest version.



%\nc{We probably need to say something about QuantizedNN and differences}
\begin{table}
  \centering
%  \resizebox{0.7\columnwidth}{!}{

      \begin{tabular}{|c|c|c|c|c|}
    \hline
    Benchmark & Semi-Honest (s) &  Malicious (s) & Comm. (GB)  \\
    \hline
    $\resnet$ & $25.9$ & 75.4 &$6.9$\\ \hline
    $\densenet$ & $36.0$ & 112.9 &$10.5$\\ 	\hline
%    $\squeezenet$ & $11.3$ & 29.0 &$2.6$\\ 	\hline

\end{tabular}
%}
 \caption{\cryptflow: ImageNet scale benchmarks.}
\label{tab:bigbenchmarks}
%\tableup
	%\vspace{-0.5cm}
\end{table}~\\
\noindent\textbf{Scalability.} We show that the running time of \tool-based protocols increases linearly with the depth of DNNs. We compile \textsc{ResNet-$n$} (where $n$, the approximate number of convolutional layers, varies from 18 to 200) with \tool and evaluate with both semi-honest (Porthos) and malicious secure protocols (Aramis) in Figure \ref{fig:scalingResnet}. Our largest benchmark here is \textsc{ResNet-$200$}, the deepest version of \textsc{ResNet} on the ImageNet dataset \cite{he2016identity}, which has 65 million parameters. Other \textsc{ResNet}-$n$ benchmarks have between 11 to 60 million parameters \footnote{Specifically, 11, 22, 25, 44 and 60 million parameters for \textsc{ResNet}-$n$ for $n=$ 18, 34, 50, 101, and 152 respectively.}. We observe that the communication and runtime increase linearly with depth. Even with increasing depth, the overhead of malicious security (over semi-honest security) remains constant at about 3X. 
\begin{figure}
  \includegraphics[width=\linewidth]{scale-plot-resnet.pdf}
	\caption{Scalability of {\cryptflow} on {\sc ResNet}-$n$.}
  \label{fig:scalingResnet}
\end{figure}

\subsection{Comparison with prior work}
\label{sec:prior-comparison}
In this section, we show that \tool outperforms prior works on secure inference of DNNs
 on the benchmarks they consider, i.e., tiny 2--4 layer DNNs over the MNIST and CIFAR-10 datasets. We stress that these benchmarks are very small compared to the ImageNet scale DNNs discussed above. % and hence may not be an accurate reflection of the power of \cryptflow. 
In order to provide a fair comparison, for these experiments, we use a network with similar bandwidth as prior works (1.5 GBps) and machines with similar compute (2.7 GHz). 
%For protocols whose code is publicly available (e.g. SecureNN~\cite{securenncode}), we ran the protocol on our benchmark machines and the times reported are those; for others, the times are from the respective papers that use similar machines. 

Table~\ref{tab:porthosvspriormnist} shows that Porthos outperforms prior (ABY$^3$ \cite{aby3}, Chameleon\footnote{Chameleon is a 2PC protocol in the online phase but requires a trusted third party in the offline phase. We report overall time here.}~\cite{chameleon}, and SecureNN \cite{securenn}) and concurrent (QuantizedNN~\cite{quantizednn}) semi-honest secure 3PC works on the MNIST dataset. 
It is well-known that 3PC-based techniques like Porthos are much faster than techniques based on 2PC and FHE. We relegate comparison between Porthos and 2PC/FHE
works to Appendix~\ref{unfaircomp}. We omit comparisons with~\cite{trident,astraccsw} as their published MSB protocol was incorrect~\cite{astraeprint}.
%
%Table \ref{tab:porthosvspriorcifar10} compares Porthos with prior 2PC work.  We omit some prior works (e.g., \cite{secureml,hycc,chameleon}, etc.) in these tables as they are slower than %Gazelle and do not provide additional insights. 
%We note that other than ABY$^3$ \cite{aby3}, QuantizedNN \cite{quantizednn}, and SecureNN \cite{securenn}, other works are for 2-party secure computation. 
%As can be seen from the tables, the 2PC systems are much slower than the 3PC systems and Porthos performs better than other 3PC systems. For instance, for a 4-layer CNN for MNIST %from MiniONN~\cite{minionn}, the best 2PC-backend Gazelle takes 810 ms, prior best 3PC work takes 47 ms, and Porthos takes 34 ms. 
%Further, some of these benchmarks employ much higher bandwidth than our system  -- e.g. ABY$^3$~\cite{aby3} and QuantizedNN~\cite{quantizednn} make use of a network with %1.5GBps bandwidth which is {\em more than 3 times faster than our network}; however the code of these works are not public and hence we could not reproduce their results on our %benchmark machines. Even then, our performance numbers either beat prior times or are only marginally slower that accounting for bandwidth difference would make our protocol faster. 

\begin{table}
  \centering
%  \resizebox{\columnwidth}{!}{

      \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Benchmark & \cite{aby3}  & \cite{quantizednn} & \cite{securenn} & \cite{chameleon} & Porthos \\
    \hline
	Logistic Regression & $4$ & $-$ & $-$ & -  & $2.7$\\
    \hline
	SecureML (3 layer) & $8$ & $20$ & $17$ & - & $8$\\
	\hline
	MiniONN (4 layer)  & -  & $80$ & $47$ & 2240 & $34$\\
	\hline
   LeNet (4 layer) & - & $120$ & $79$ & - & $58$ \\
	\hline
\end{tabular}
%}
 \caption{Comparison with 3PC on MNIST dataset with ABY$^3$ \cite{aby3}, QuantizedNN~\cite{quantizednn}, SecureNN \cite{securenn}, and Chameleon~\cite{chameleon}. All times in milliseconds.}
\label{tab:porthosvspriormnist}
%\tableup
	%\vspace{-0.5cm}
\end{table}



%To put Aramis in comparison with concurrent actively secure inference works, we compare Aramis with QuantizedNN \cite{quantizednn} in table \ref{tab:aramisvsspdz} on the 4 networks described in SecureNN \cite{securenn}. We used MP-SPDZ framework \cite{mpspdz} to run QuantizedNN on these 4 benchmarks with malicious security in an honest majority setting. MP-SPDZ repository already has these 4 benchmarks written for QuantizedNN which we used for this comparison. However, it only implements 8-bit quantization with networks stripped out of all ReLU layers, while for Aramis we ran full networks with 64-bit quantization. In doing so we are being unfair to Aramis and the execution time of MP-SPDZ with ReLUs would be much worse
%than what is shown here.

\begin{comment}
\begin{table}
  \centering
  \resizebox{0.7\columnwidth}{!}{

      \begin{tabular}{|l|c|c|c|c|}
    \hline
    Benchmark & Aramis Gains over Mal. QuantizedNN\\
    \hline
	SecureNN N/W A & $9$x \\
	\hline
	SecureNN N/W B & $32.7$x \\
	\hline
	SecureNN N/W C & $34.1$x \\
	\hline
	SecureNN N/W D & $7.9$x \\
	\hline

\end{tabular}
}
 \caption{MNIST dataset -- Aramis vs Malicious QuantizedNN}
\label{tab:aramisvsspdz}
%\tableup
	%\vspace{-0.5cm}
\end{table}
\end{comment}




%these benchmarks are at a regime where computation is not an issue (as the matrix multiplications performed by parties are small) and are only bandwidth constrained. This is in contrast to larger benchmarks where both compute and communication are overheads. 
\subsection{Athos experiments}
\label{sec:athosexperiments}
\noindent\textbf{Accuracy of Float-to-Fixed.} 
We show that Athos generated fixed-point code matches the accuracy of floating-code on \resnet and \densenet in Table~\ref{tab:fixed-accuracy}.
The table also shows the precision or the scale that is selected by Athos (Section~\ref{subsec:athosquantizer}). We observe that different benchmarks require different precision to maximize the classification accuracy and that the technique of ``sweeping'' through various precision levels is effective.
We show how accuracy varies with precision in Appendix~\ref{appendix:accuracies}.
Evaluating accuracy also helps validate the correctness of our compilation~\cite{frigate}. 

\begin{table}
\centering
% \resizebox{0.7\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Benchmark & Float & Fixed & Float & Fixed & Scale \\
          & Top 1 & Top 1 & Top 5 & Top 5 & \\
\hline
$\resnet$     & 76.47 & 76.45 & 93.21 & 93.23 & 12 \\ \hline
$\densenet$   & 74.25 & 74.33 & 91.88 & 91.90 & 11 \\ \hline
%$\squeezenet$ & 55.86 & 55.92 & 79.18 & 79.24 & 10 \\ \hline
\end{tabular}
%}
\caption{Accuracy of fixed-point vs floating-point.}
\label{tab:fixed-accuracy}
%\tableup
%\vspace{-0.8cm}
\end{table}~\\
\noindent\textbf{Modularity.} 
Since \tool is modular, we can compile it to various \mpc backends. To demonstrate this ability, we also add a 2PC semi-honest secure protocol ABY~\cite{aby} to \tool. 
The performance with this backend is in Table \ref{tab:2pcnumbers}.
We ran logistic regression (LR) as well as a small LeNet network~\cite{lenet} which comprises of 2 convolutional layers (with maximum filter size of $5\times 5$) and 2 fully connected layers, with ReLU and MaxPool as the activation functions.
% We see that performance of 2PC is much worse than 3PC Porthos.. 
 This evaluation shows that \cryptflow\ can be easily used for a variety of backends. 
 % - however the current state-of-the-art performance of 2PC makes it difficult to execute the large DNNs described in Section \ref{subsec:bigbenchmarks}. 
%One could potentially implement the functions in Table~\ref{tab:smf} with
% state-of-the-art 2PC backends such as Delphi~\cite{delphi}. The code of Delphi is not publicly available and hence we could not do the same.

\begin{table}
  \centering
%  \resizebox{0.7\columnwidth}{!}{

      \begin{tabular}{|c|c|c|}
    \hline
    Benchmark & \cryptflow\ (s) & Communication (MB) \\
    \hline
	$\mathsf{Logistic Regression}$ & $0.227$ & $25.5$\\
	\hline
    $\mathsf{LeNet}$ $\mathsf{Small}$ & $47.4$ & $2939$ \\ 	\hline

\end{tabular}
%}
 \caption{\cryptflow\ compilation to 2PC on MNIST.}
\label{tab:2pcnumbers}
%\tableup
%	\vspace{-0.8cm}
\end{table}

%A few other interesting observations follow. After a certain threshold in scaling factor, accuracies drop dramatically. This is expected as beyond this level of precision, we lose information %due to the overflow of underlying values when performing computation such as matrix multiplication. Interestingly, accuracy doesn't always improve with larger bits of precision even until %this threshold - e.g., the best Top 1 accuracy on the \resnet\ network is obtained at 12 bits of precision.

\subsection{Porthos experiments}\label{subsec:porthosexperiments}
Since Porthos builds on SecureNN, we compare them in mode detail.
As described earlier, Porthos improves over the communication complexity of SecureNN~\cite{securenn} both for convolutional layers as well as for non-linear activation functions.
We have already compared SecureNN and Porthos on benchmarks considered in SecureNN in Table~\ref{tab:porthosvspriormnist}.
Additionally, we also compare Porthos and SecureNN on ImageNet scale benchmarks in Table \ref{tab:porthosvssecurenn}. For this purpose, we add the code of SecureNN available at~\cite{securenncode} as another backend to \tool. These results show that Porthos improves upon the communication of SecureNN by a factor of roughly 1.2X--1.5X and the runtime by a factor of roughly 1.4X--1.5X.
% on these benchmarks.

\begin{table}
  \centering
  \resizebox{\columnwidth}{!}{

      \begin{tabular}{|c|c|c|c|c|}
    \hline
    Benchmark & SecureNN & Porthos& SecureNN & Porthos \\
     & (s)  & (s) & Comm. (GB) & Comm. (GB) \\
	\hline
	$\resnet$ & $38.36$ & $25.87$& $8.54$& $6.87$ \\
	\hline
    $\densenet$ & $53.99$ & $36.00$& $13.53$ & $10.54$ \\ 	\hline
  %  $\squeezenet$ & $16.55$ & $11.28$& $3.88$ & $2.63$ \\ 	\hline

\end{tabular}
}
 \caption{Porthos vs SecureNN.}
\label{tab:porthosvssecurenn}
%	\vspace{-0.5cm}
%\tableup
\end{table}

\subsection{Aramis experiments}\label{subsec:aramisexperiments}
We applied Aramis to both the 2-party GMW protocol~\cite{gmw} (using the codebase~\cite{gmwcode}, based on~\cite{gmwpaper}) as well as Porthos. 
The results for different functions using the GMW protocol are presented in Table \ref{tab:gmwport}. $\mathsf{IP}_n$ denotes the inner product of two $n$-element vectors over $\bbF_2$, $\mathsf{Add}_{32}$ and $\mathsf{Mult}_{32}$ denote addition and multiplication over $32$ bits respectively, and $\mathsf{Millionaire}_{32}$ denotes the millionaires problem that compares two $32-$bit integers $x$ and $y$ and outputs a single bit denoting whether $x>y$.  The overheads of Aramis-based malicious security, are within $54\%$ of the semi-honest protocol. Table~\ref{tab:bigbenchmarks} and Figure~\ref{fig:scalingResnet} evaluate Aramis with Porthos.
%Aramis in this table denotes the semi-honest GMW protocol ported into Intel SGX to provide malicious security. Evaluation of Aramis applied to Porthos 

% As can be seen, all overheads in this case, are within $54\%$ of the semi-honest protocol. The evaluation of Aramis on

%Since these benchmarks are small, all of the code and data fit inside the SGX enclave without any requirement for paging and hence overheads are also minimal. For ImageNet scale %benchmarks, the overheads are higher (Table~\ref{tab:bigbenchmarks}).
%Since these benchmarks are much larger, we have to deal with well-known paging issues that occur when using Intel SGX with large data~\cite{intelsgxperf}; here we incur overheads of %about 3X over semi-honest protocols. 

\subsubsection{Comparison with crypto-only malicious \mpc}
\label{sec:concurrent-comparison}
We demonstrate that Aramis based malicious secure protocols are better suited for large scale inference tasks compared to pure cryptographic solutions. 
We compare the performance of Porthos compiled with Aramis and the concurrent work of QuantizedNN~\cite{quantizednn} that uses the MP-SPDZ~\cite{mpspdz} framework to also provide a malicious secure variant of their protocol. Both these approaches provide security for the same  setting of 3PC with 1 corruption. On the four MNIST inference benchmarks A/B/C/D in the MP-SPDZ repository, Aramis is 10X/46X/44X/15X faster.
% Furthermore, in these performance measurements, we are being unfair to Aramis: MP-SPDZ strips out all the ReLUs from its performance estimates and its %performance would be much worse in their presence. However, the Aramis time measurements include the time to compute ReLUs, which can be 80\% of the %total execution time. If one were to evaluate ReLUs with MP-SPDZ then the speedups of Aramis would be even higher.
% \nc{Should we clarify somewhere that earlier mentions of QuantizedNN were for semihonest security and here alone we are referring to their malicious protocol? To prevent against a reviewer thinking that all are malicious protocols?}

\begin{table}
  \centering
 % \resizebox{0.7\columnwidth}{!}{

      \begin{tabular}{|c|c|c|c|}
    \hline
    Benchmark & GMW (ms) & Aramis (ms) & Overhead \\
    \hline
	$\mathsf{IP}_{10,000}$ & $464$ & $638$ & $1.37$x\\
	\hline
    $\mathsf{IP}_{100,000}$ & $2145$ & $3318$ & $1.54$x \\ 	\hline
    $\mathsf{Add}_{32}$ & $279$ & $351$ & $1.25$x \\ 	\hline
    $\mathsf{Mult}_{32}$ & $354$ & $461$ & $1.30$x \\ 	\hline
    $\mathsf{Millionaire}_{32}$ & $292$ & $374$ & $1.28$x \\ 
	\hline

\end{tabular}
%}
 \caption{Semi-honest GMW vs Malicious Aramis.}
\label{tab:gmwport}
	%\vspace{-0.5cm}

%\tableup
\end{table}

%Table \ref{tab:porthosport} shows the overheads of Aramis over Porthos for the benchmarks in Section \ref{subsec:bigbenchmarks}. 
%Figure~\ref{fig:aramisreluplot} shows how overhead of Aramis over Porthos varies with memory usage. In this landscape, the plot also shows where the benchmarks in Section \ref{subsec:bigbenchmarks} lie.

\begin{comment}
\begin{table}
  \centering
	\resizebox{0.7\columnwidth}{!}{

      \begin{tabular}{|l|c|c|c|l|}
    \hline
    Benchmark & Porthos (s) & Aramis (s) & Overhead \\
    \hline
	$\resnet$ & $25.87$ & $75.4$ & $2.9$x\\
	\hline
    $\densenet$ & $36.00$ & $112.9$ & $3.1$x \\ 	\hline
    $\squeezenet$ & $11.28$ & $29.01$ & $2.5$x \\ 	\hline
\end{tabular}
}
 \caption{Semi-honest Porthos vs Malicious Aramis.}
\label{tab:porthosport}
	%\vspace{-0.5cm}
%\tableup
\end{table}
\end{comment}

%\mayank{Adding new SGX plots here. NOTE: the overheads of SqueezeNet, ResNet and DenseNet will change when Athos is updated. Take new readings before submission. NOT FINAL.}
%We also benchmark the performance of a single ReLU layer and plot its trend with increasing ReLU complexity interpreted as memory usage in our plot. The cryptographic protocol that %securely realizes ReLU incurs an 8x increase in the memory footprint of a ReLU operation which is of concern in Aramis given the fact that the usable EPC page size is below 100MB. Figure %\ref{fig:aramisreluplot} shows that overhead of Aramis over Porthos scales well even when we run ReLUs securely requiring upto 2GB of memory. In this memory usage landscape, the plot %also shows where the benchmarks in Section \ref{subsec:bigbenchmarks} lie.
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{plot-sgx-relu.pdf}
%  \caption{Aramis overheads over Porthos vs Memory usage}
%  \label{fig:aramisreluplot}
%\end{figure}


\subsection{Real world impact}\label{subsec:realworldimpact}
%Our evaluation thus far has been based on ML models and datasets in image recognition. In this section, 
We discuss our experience with using {\cryptflow} to compile and run DNNs used in healthcare. These DNNs are available as pre-trained Keras models. We converted them into \tensorflow using~\cite{kttf} and compiled the automatically generated \tensorflow code with \tool.

\paragraph{Chest X-Ray} In \cite{chestxray2018}, the authors train a {\densenet} to predict lung diseases from chest X-ray images. They use the publicly available NIH dataset of chest X-ray images and end up achieving an average AUROC score of 0.845 across 14 possible disease labels.
% We took their publicly available pretrained keras model, converted it to tensorflow~\cite{kttf}, and compiled the \tensorflow code with {\cryptflow}.
During secure inference, we observed no loss in accuracy and the runtime is similar to the runtime of \densenet for ImageNet.
%Both the latency and accuracy of this inference is similar to the one we report for {\densenet} and . We also end up achieving a similar AUROC score.
%\nishant{Todo on me to find this AUROC score for compiled code and double check that the performance is indeed identical to \densenet}

\paragraph{Diabetic Retinopathy CNN} Diabetic Retinopathy (DR), one of the major causes of blindness, is a medical condition that leads to damage of retina due to diabetes \cite{janakirammsv2017}. In recent times, major tech companies have taken an interest in using DNNs for diagnosing DR from retinal images \cite{janakirammsv2017,googleDRPaper}. 
%We used one such DNN pretrained in keras, converted the same to \tensorflow~\cite{kttf}, and then ran {\cryptflow} on it to run it securely using Porthos as our backend. 
Predicting whether a retina image has DR or not can be done securely in about 30 seconds with \tool.
%\nishant{Add more on this: performance, accuracy metric? but we only had 100 images, however google paper has auroc score.}
