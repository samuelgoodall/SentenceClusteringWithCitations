urn:uuid:762ef344-d10a-3044-99ab-1be8fc7d7ab4,"On similar lines, the work by Kim et al. \cite{kim2016sequence} proposed that instead of minimizing cross-entropy with observed data, it is better to minimize cross-entropy with teacher's probability distribution.",['kim2016sequence'],"[""\\bibitem{kim2016sequence}\nYoon Kim and Alexander~M Rush,\n\\newblock ``Sequence-level knowledge distillation,''\n\\newblock {\\em arXiv preprint arXiv:1606.07947}, 2016.""]",[],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4
urn:uuid:9fc3c33d-9550-3a02-9155-67fdc63db541,\cite{mirzadeh2019improved} presents improvements in knowledge distillation by using an intermediate network they call teacher assistant.,['mirzadeh2019improved'],"[""\\bibitem{mirzadeh2019improved}\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, and Hassan Ghasemzadeh,\n\\newblock ``Improved knowledge distillation via teacher assistant: Bridging the\n  gap between student and teacher,''\n\\newblock {\\em arXiv preprint arXiv:1902.03393}, 2019.""]",[],urn:uuid:aefa99a6-a09a-32e9-a6c9-02c81cd9444b,urn:uuid:3f556df2-184d-35e3-b034-feef8e45fad4
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307," Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling \cite{dong2014learning}, {i.e.} starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff \cite{goodman2005introduction}.","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in \cite{yang2014single}.",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge, \cite{dong2014learning} is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"\cite{johnson2016perceptual,ledig2017photo} are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss\cite{johnson2016perceptual} and Generative Adversarial Networks(GAN)\cite{goodfellow2014generative}.","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was \cite{rivenson2017deep}, where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671," Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm \cite{gerchberg1972practical}, digital holography (DH) \cite{goodman1967digital}, phase shifting interferometry \cite{creath1985phase}, and Transport of Intensity Equation (TIE) \cite{teague1983deterministic}.","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al \cite{sinha2017lensless} using an end-to-end residual convolutional neural network\cite{he2016deep}.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al \cite{rivenson2018phase} proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al \cite{goy2018low} further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"Perceptual Quality Enhancement  Prior to \cite{li2018spectral}, it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth\cite{gupta2011modified,wang2004image,wang2003multiscale,bruna2015super,mathieu2015deep,dosovitskiy2016generating}.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al\cite{mahendran2015understanding}, to feature visualization by Simonyan et al\cite{simonyan2013deep} and Yosinski et al\cite{yosinski2015understanding}, and to texture synthesis and style transfer by Gatys et al\cite{gatys2015texture}.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250, Johnson et al\cite{johnson2016perceptual} is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al\cite{ledig2017photo} proposed a SR generative adversarial neural network\cite{goodfellow2014generative}(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks\cite{simonyan2014very} and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"Dual frequency band reconstruction In 2008, HiLo microscopy \cite{lim2008wide} was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN)\cite{Pan_2018_CVPR} structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
usable_dataset/1707.00945,urn:uuid:c3c10d3e-bc0c-3094-b2db-d2c70ec28e0f,"A look back at (old) SPARK's history and its success, as well as an initial picture of SPARK 2014 is given by Chapman and Schanda in <cit.>.",['Chapman2014'],[''],[''],urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:831164a3-3a1e-35cb-a5cb-b4494a8c8cba," A small case study with SPARK 2014 is presented in <cit.>, but at that point multi-threading (Ravenscar) was not yet supported, and floating point numbers have been skipped in the proof.",['Trojanek2014'],[''],[''],urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:00f0c7fb-df05-3cb2-ab54-23df44e61105,"in <cit.>, with whom we share the opinion of minor usability issues, and that some small amount of developer training is required.",['Dross2014'],[''],[''],urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:79c48323-6a4e-3f18-b370-6992872624f2," Finally, SPARK 2014 with Ravenscar has recently been announced to be used in the Lunar IceCube <cit.> satellite, a successor of the successful CubeSat project that was implemented in SPARK 2005.",['Brandon2016'],[''],[''],urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:c3c10d3e-bc0c-3094-b2db-d2c70ec28e0f,"A look back at (old) SPARK's history and its success, as well as an initial picture of SPARK 2014 is given by Chapman and Schanda in <cit.>.",['Chapman2014'],['Are we there yet? 20 Years of industrial theorem proving with SPARK'],"['Chapman, R. and Schanda, F.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:831164a3-3a1e-35cb-a5cb-b4494a8c8cba," A small case study with SPARK 2014 is presented in <cit.>, but at that point multi-threading (Ravenscar) was not yet supported, and floating point numbers have been skipped in the proof.",['Trojanek2014'],['mobile robot navigation algorithms: A case study in SPARK'],"['Trojanek, P. and Eder, K.: {Verification and testing of']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:00f0c7fb-df05-3cb2-ab54-23df44e61105,"in <cit.>, with whom we share the opinion of minor usability issues, and that some small amount of developer training is required.",['Dross2014'],['Three Case Studies for SPARK 2014'],"['D. and Moy,  Y.: {Rail and Space, Security:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:79c48323-6a4e-3f18-b370-6992872624f2," Finally, SPARK 2014 with Ravenscar has recently been announced to be used in the Lunar IceCube <cit.> satellite, a successor of the successful CubeSat project that was implemented in SPARK 2005.",['Brandon2016'],['The Use of SPARK in a Complex Spacecraft'],"['Brandon, C. and Chapin, P.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:c3c10d3e-bc0c-3094-b2db-d2c70ec28e0f,"A look back at (old) SPARK's history and its success, as well as an initial picture of SPARK 2014 is given by Chapman and Schanda in <cit.>.",['Chapman2014'],['Are we there yet? 20 Years of industrial theorem proving with SPARK'],"['Chapman, R. and Schanda, F.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:831164a3-3a1e-35cb-a5cb-b4494a8c8cba," A small case study with SPARK 2014 is presented in <cit.>, but at that point multi-threading (Ravenscar) was not yet supported, and floating point numbers have been skipped in the proof.",['Trojanek2014'],['mobile robot navigation algorithms: A case study in SPARK'],"['Trojanek, P. and Eder, K.: {Verification and testing of']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:00f0c7fb-df05-3cb2-ab54-23df44e61105,"in <cit.>, with whom we share the opinion of minor usability issues, and that some small amount of developer training is required.",['Dross2014'],['Three Case Studies for SPARK 2014'],"['D. and Moy,  Y.: {Rail and Space, Security:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:79c48323-6a4e-3f18-b370-6992872624f2," Finally, SPARK 2014 with Ravenscar has recently been announced to be used in the Lunar IceCube <cit.> satellite, a successor of the successful CubeSat project that was implemented in SPARK 2005.",['Brandon2016'],['The Use of SPARK in a Complex Spacecraft'],"['Brandon, C. and Chapin, P.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:c3c10d3e-bc0c-3094-b2db-d2c70ec28e0f,"A look back at (old) SPARK's history and its success, as well as an initial picture of SPARK 2014 is given by Chapman and Schanda in <cit.>.",['Chapman2014'],['Are we there yet? 20 Years of industrial theorem proving with SPARK'],"['Chapman, R. and Schanda, F.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:831164a3-3a1e-35cb-a5cb-b4494a8c8cba," A small case study with SPARK 2014 is presented in <cit.>, but at that point multi-threading (Ravenscar) was not yet supported, and floating point numbers have been skipped in the proof.",['Trojanek2014'],['mobile robot navigation algorithms: A case study in SPARK'],"['Trojanek, P. and Eder, K.: {Verification and testing of']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:00f0c7fb-df05-3cb2-ab54-23df44e61105,"in <cit.>, with whom we share the opinion of minor usability issues, and that some small amount of developer training is required.",['Dross2014'],['Three Case Studies for SPARK 2014'],"['D. and Moy,  Y.: {Rail and Space, Security:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:79c48323-6a4e-3f18-b370-6992872624f2," Finally, SPARK 2014 with Ravenscar has recently been announced to be used in the Lunar IceCube <cit.> satellite, a successor of the successful CubeSat project that was implemented in SPARK 2005.",['Brandon2016'],['The Use of SPARK in a Complex Spacecraft'],"['Brandon, C. and Chapin, P.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:c3c10d3e-bc0c-3094-b2db-d2c70ec28e0f,"A look back at (old) SPARK's history and its success, as well as an initial picture of SPARK 2014 is given by Chapman and Schanda in .",['Chapman2014'],['Are we there yet? 20 Years of industrial theorem proving with SPARK'],"['Chapman, R. and Schanda, F.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:831164a3-3a1e-35cb-a5cb-b4494a8c8cba,"A small case study with SPARK 2014 is presented in , but at that point multi-threading (Ravenscar) was not yet supported, and floating point numbers have been skipped in the proof.",['Trojanek2014'],['mobile robot navigation algorithms: A case study in SPARK'],"['Trojanek, P. and Eder, K.: {Verification and testing of']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:445ff467-2110-373b-b1a4-92ad79b81290,"Larger case studies are summarized by Dross et al. in , with whom we share the opinion of minor usability issues, and that some small amount of developer training is required.",['Dross2014'],['Three Case Studies for SPARK 2014'],"['D. and Moy,  Y.: {Rail and Space, Security:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1707.00945,urn:uuid:79c48323-6a4e-3f18-b370-6992872624f2,"Finally, SPARK 2014 with Ravenscar has recently been announced to be used in the Lunar IceCube  satellite, a successor of the successful CubeSat project that was implemented in SPARK 2005.",['Brandon2016'],['The Use of SPARK in a Complex Spacecraft'],"['Brandon, C. and Chapin, P.:']",urn:uuid:368bf77b-8c3d-3f99-9043-6b30eac8354c,urn:uuid:84c9f5c5-d932-3159-bea3-b48be3c292a9,usable_dataset/1707.00945/paper.bbl
usable_dataset/1811.07945,urn:uuid:462386dc-8cd2-3f1c-8836-c1d195e7d307,"* Image Super-resolution (SR) The term super-resolution (SR) can have different interpretations in different research communities: it can mean upsampling , i.e. starting from the raw image  captured on a limited number of pixels and arriving at an estimate , with a larger number of pixels within the same spatial support; it can also be interpreted as deblurring, where the blur kernel is a low-pass filter with transfer function equal to zero for spatial frequencies above cutoff .","['dong2014learning', 'goodman2005introduction']","['Learning a deep convolutional network for image super-resolution', '']","['C. Dong and C. C. Loy and K. He and X. Tang', 'J. W. Goodman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:4cdbf73e-a9c9-3726-9a6c-8c775ffb960c,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:0cd0644d-a8c4-3648-8030-cc4f2d5e8607,"An extensive review on non-machine learning based methods of single-image SR (under the upsampling interpretation), is given in .",['yang2014single'],['Single-image super-resolution: A benchmark'],['C.-Y. Yang and C. Ma and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:08d0a675-deb2-3ca0-9b31-151661e51e1f,"To our knowledge,  is the first effort to use DNN in this context.",['dong2014learning'],['Learning a deep convolutional network for image super-resolution'],['C. Dong and C. C. Loy and K. He and X. Tang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:85ee77d4-d9c6-3662-bbf6-73b55237486c,"are among the important follow-ups, where visually more appealing reconstructions are made possible with the use of perceptual loss and Generative Adversarial Networks(GAN).","['johnson2016perceptual', 'goodfellow2014generative', 'johnson2016perceptual', 'ledig2017photo']","['Perceptual losses for real-time style transfer and super-resolution', 'Generative adversarial nets', 'Perceptual losses for real-time style transfer and super-resolution', 'Photo-realistic single image super-resolution using a generative adversarial network']","['J. Johnson and A. Alahi and L. Fei-Fei', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'J. Johnson and A. Alahi and L. Fei-Fei', 'and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:dbd3385d-708b-3060-8c94-01301fd654e2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8f760664-ad72-38e8-8ed8-add6fad49ebb,"To our knowledge, the only prior work using DNNs on deblurring with a hard-cutoff in frequency imposed by the kernel was , where the low-resolution image and high-resolution image within a training pair were captured using two lenses with different numerical apertures (NAs).",['rivenson2017deep'],['Deep learning microscopy'],['and H. G{'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:5b50d685-9c36-3604-a346-8e2b107f51f2,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:cb1c2012-165c-3087-b2de-67d3dfc05671,"Traditional approaches for QPR include the Gerchberg-Saxton (GS) algorithm , digital holography (DH) , phase shifting interferometry , and Transport of Intensity Equation (TIE) .","['gerchberg1972practical', 'goodman1967digital', 'creath1985phase', 'teague1983deterministic']","['A practical algorithm for the determination of phase from image and diffraction plane pictures', 'Digital image formation from electronically detected holograms', 'Phase-shifting speckle interferometry', 'Deterministic phase retrieval: a green’s function solution']","['R. W. Gerchberg', 'J. W. Goodman and R. Lawrence', 'K. Creath', 'M. R. Teague']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:0f84d140-34d5-3bef-834c-f44053a5f817,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f896c97b-3532-3b93-aece-571650f25c1e,Deep learning was introduced for QPR by Sinha et al  using an end-to-end residual convolutional neural network.,"['sinha2017lensless', 'he2016deep']","['Lensless computational imaging through deep learning', 'Deep residual learning for image recognition']","['A. Sinha and J. Lee and S. Li and G. Barbastathis', 'K. He and X. Zhang and S. Ren and J. Sun']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:63f2abda-08fc-3ea9-8f6d-689e9316529d,"Subsequently, Rivenson et al  proposed a QPR in a DH implementation with a DNN including an approximant: first  an approximate phase reconstruction was obtained using an optical back-propagation method; that reconstruction was then fed into the neural network for further improvement.",['rivenson2018phase'],['Phase recovery and holographic image reconstruction using deep learning in neural networks'],['and D. Teng and A. Ozcan'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:8e071a7f-5ce5-3844-8697-823c801faf5f,Goy et al  further showed that the GS algorithm is a good approximant for the DNN to perform QPR under conditions of low photon flux.,['goy2018low'],['Low photon count phase retrieval using deep learning'],['A. Goy and K. Arthur and S. Li and G. Barbastathis'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:c377542f-a007-3bcf-8572-80f15a7635fb,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ddf0d98b-ed7f-3c0b-a4e0-ea701b2b4536,"* Perceptual Quality Enhancement  Prior to , it was realized that low-level pixel-wise losses generally encourage finding pixel-wise averages of plausible solutions which tend to be over-smooth.","['li2018spectral', 'gupta2011modified', 'wang2004image', 'wang2003multiscale', 'bruna2015super', 'mathieu2015deep', 'dosovitskiy2016generating']","['Spectral pre-modulation of training examples enhances the spatial resolution of the phase extraction neural network (phenn)', 'A modified psnr metric based on hvs for quality assessment of color images', 'Image quality assessment: from error visibility to structural similarity', 'Multiscale structural similarity for image quality assessment', 'Super-resolution with deep convolutional sufficient statistics', 'Deep multi-scale video prediction beyond mean square error', 'Generating images with perceptual similarity metrics based on deep networks']","['S. Li and G. Barbastathis', 'P. Gupta and P. Srivastava and S. Bhardwaj and V. Bhateja', 'Z. Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli', 'Z. Wang and E. P. Simoncelli and A. C. Bovik', 'J. Bruna and P. Sprechmann and Y. LeCun', 'M. Mathieu and C. Couprie and Y. LeCun', 'A. Dosovitskiy and T. Brox']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:6ecc8058-3b62-3670-a77a-b6cb1d28a9b1,"This strategy was applied to feature inversion by Mahendran et al, to feature visualization by Simonyan et al and Yosinski et al, and to texture synthesis and style transfer by Gatys et al.","['mahendran2015understanding', 'simonyan2013deep', 'yosinski2015understanding', 'gatys2015texture']","['Understanding deep image representations by inverting them', 'Deep inside convolutional networks: Visualising image classification models and saliency maps', 'Understanding neural networks through deep visualization', 'Texture synthesis using convolutional neural networks']","['A. Mahendran and A. Vedaldi', 'K. Simonyan and A. Vedaldi and A. Zisserman', 'J. Yosinski and J. Clune and A. Nguyen and T. Fuchs and H. Lipson', 'L. Gatys and A. S. Ecker and M. Bethge']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:ac136ef5-09b3-31d6-8023-8ab81a308250,Johnson et al is the first work that combines the benefits of the fast rendering of output by a CNN and perceptual loss to train feed-forward convolutional neural networks for style transfer and SR.,['johnson2016perceptual'],['Perceptual losses for real-time style transfer and super-resolution'],['J. Johnson and A. Alahi and L. Fei-Fei'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:f0982557-97af-3d08-8c7d-ff491c2a87f5,"Subsequently, Ledig et al proposed a SR generative adversarial neural network(SRGAN), where the perceptual loss based on high-level feature maps of the VGG-networks and a discriminator, jointly encourage reconstructions to be perceptually indistinguishable from the high resolution images.","['ledig2017photo', 'goodfellow2014generative', 'simonyan2014very']","['Photo-realistic single image super-resolution using a generative adversarial network', 'Generative adversarial nets', 'Very deep convolutional networks for large-scale image recognition']","['and J. Caballero and A. Cunningham and A. Acosta and  A. P. Aitken and A. Tejani and J. Totz and Z. Wang and et al', 'I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and  A. Courville and Y. Bengio', 'K. Simonyan and A. Zisserman']",urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:1c77e1be-893e-3226-a1c5-eebde04854db,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:014b8dae-f66c-3d4d-b0c1-f94ed09b1f4c,"* Dual frequency band reconstruction In 2008, HiLo microscopy  was proposed by Lim et al to enhance the resolution of optical imaging system.",['lim2008wide'],['Wide-field fluorescence sectioning with hybrid speckle and uniform-illumination microscopy'],['D. Lim and K. K. Chu and J. Mertz'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:301b7313-f05f-3f1a-84a1-ed30c000c3d3,usable_dataset/1811.07945/main.bbl
usable_dataset/1811.07945,urn:uuid:b53e2166-2dff-3710-8fe3-360b59601f79,"In 2018, Pan et al proposed a dual convolutional neural network (DualCNN) structure consisting of two branches to solve low-level vision problems.",['Pan_2018_CVPR'],['Learning dual convolutional neural networks for low-level vision'],['J. Pan and S. Liu and D. Sun and J. Zhang and Y. Liu and J. Ren and Z. Li and J. Tang and H. Lu and Y.-W.  Tai and M.-H. Yang'],urn:uuid:88ab0b5e-e6d5-3c69-9a47-cfb5d0dcc3b4,urn:uuid:a877e6ac-a0f6-3724-981a-7cf04ddff9e0,usable_dataset/1811.07945/main.bbl
