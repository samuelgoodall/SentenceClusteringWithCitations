

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} %


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{pifont}

\usepackage{multirow}
\usepackage{verbatim}
\usepackage{color}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabulary,multirow,overpic,xcolor}

\usepackage[caption=false]{subfig}
\usepackage{pifont}%
\usepackage{epstopdf}
\epstopdfsetup{update} %

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{enumerate}

\usepackage{bm}
\usepackage{t1enc}
\usepackage{colortbl}
\usepackage{cite}
\usepackage{soul}


\newcommand{\knr}[1]{{\textcolor{blue}{#1}}}


\definecolor{pos}{RGB}{0,153,0}
\definecolor{neg}{RGB}{0,0,0}
\definecolor{smpos}{RGB}{0,0,0}
\definecolor{row}{RGB}{240,240,240}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\ve}[1]{\mathbf{#1}} %
\newcommand{\ma}[1]{\mathrm{#1}} %
\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Sec.~\ref{#1}}

\newcommand{\bd}[1]{\textbf{#1}}
\newcommand{\app}{\raise.17ex\hbox{$\scriptstyle\sim$}}
\def\x{$\times$}
\newcommand{\blocket}[4]{\multirow{3}{*}{\(\left[\begin{array}{c}\text{1$\times$1$^\text{2}$, #1}\\[-.1em] \text{$3$$\times$3$^\text{2}$, #2}\\[-.1em] \text{1$\times$1$^\text{2}$, #3}\end{array}\right]\)$\times$#4}
}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth\global\arrayrulewidth1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@} {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother

\addtolength{\floatsep}{-2mm}
\addtolength{\textfloatsep}{-1mm}
\addtolength{\intextsep}{-1mm}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}




\def\cvprPaperID{9849} %
\def\confYear{CVPR 2022}


\begin{document}

\title{SWAT: Spatial Structure Within and Among Tokens}


\author{Kumara Kahatapitiya and Michael S. Ryoo\\
Stony Brook University \\%, Stony Brook, NY 11794, USA\\
 {\tt\small \{kkahatapitiy,mryoo\}@cs.stonybrook.edu}%
}

\maketitle

\begin{abstract}
	
	Modeling visual data as tokens (i.e., image patches), and applying attention mechanisms or feed-forward networks on top of them has shown to be highly effective in recent years. The common pipeline in such approaches includes a tokenization method, followed by a set of layers/blocks for information mixing, both within tokens and among tokens. In common practice, image patches are flattened when converted into tokens, discarding the spatial structure within each patch. Next, a module such as multi-head self-attention captures the pairwise relations among the tokens and mixes them. In this paper, we argue that models can have significant gains when spatial structure is preserved in tokenization, and is explicitly used in the mixing stage. We propose two key contributions: (1) Structure-aware Tokenization and, (2) Structure-aware Mixing, both of which can be combined with existing models with minimal effort. We introduce a family of models (\textbf{SWAT}), showing improvements over the likes of DeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including ImageNet classification and ADE20K segmentation. Our code and models will be released online.
	
\end{abstract}


\section{Introduction}

Convolutional architectures (CNNs) \cite{he2016deep, tan2019efficientnet} have been dominant in computer vision for a while now. When they were first introduced for large-scale training in image domain, their benefits were quickly realized over Multi-layer Perceptrons (MLPs). In addition to efficient weight sharing, the inductive bias generated by exploring local structure in images was one of the key factors for its success \cite{lecun2015deep}. In language domain however, CNNs were less effective due to lack of such strong local structure. Consequently, attention mechanisms emerged dominant, exploring long-range relationships and modeling language as a sequence \cite{dauphin2017language}. 
More recently, attention models-- specifically Transformers \cite{vaswani2017attention}, have been extended to represent visual data \cite{dosovitskiy2020image}, with the key concept of tokenizing an input image to create a sequence (or a set), discarding their structure. Within a short period of time, token-based models (i.e., class of models such as ViTs \cite{dosovitskiy2020image} and MLP-Mixers \cite{tolstikhin2021mixer}) has outperformed CNN models on most visual tasks. However, we ask, could the spatial structure-- when preserved, benefit token-based models and further improve their performance? 

\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{improvement_swat.pdf}
	\caption{\textbf{Performance vs. Complexity} on ImageNet-1K \cite{deng2009imagenet}. We implement our proposed  (1) \textit{Structure-aware Tokenization}, and (2) \textit{Structure-aware Mixing} in common Transformer/Mixer architectures including DeiT \cite{touvron2021deit}, Swin \cite{liu2021swin}, MLP-Mixer \cite{tolstikhin2021mixer} and ResMLP \cite{touvron2021resmlp}. The resulting family of \textbf{SWAT} models consistently outperform their counterparts, with minimal increase in complexity. We consider the system-agnostic metric FLOPs as the complexity measure.}
	\label{fig:acc_vs_compute}
\end{figure}

Token-based models in computer vision are rapidly evolving. From Vision Transformers \cite{dosovitskiy2020image} to MLP-Mixers \cite{tolstikhin2021mixer} and hybrid-architectures \cite{peng2021conformer, wu2021cvt}, intriguing concepts are being introduced and tested on tasks including classification \cite{dosovitskiy2020image, touvron2021deit, liu2021swin}, detection \cite{zhu2020deformable, carion2020end, dai2021up} and segmentation \cite{xie2021segformer, duke2021sstvos}, to name a few. All such models can be framed with two main components: (1) \textit{Tokenization}, which converts image patches into tokens, and (2) \textit{Mixing} (attention-based as in Multi-head Self Attention (MHSA) or MLP-based), which shares information within and among tokens. In general, during tokenization, an image patch is directly mapped into a token, not preserving the spatial structure within a token.
After this mapping, most of the architectures focus on capturing pairwise relations or global patterns among the tokens, without attempting to capture local spatial structure within tokens.


Structure is an important cue in visual data. In images, 2D spatial structure preserves geometry and object-part relationships.
Simply put, structure gives meaning to visual data in human perspective. However, in machine perception, if a jumbled set of image patches are tokenized and processed through a token-based model, it can give the same classification performance (as it is a set operator), even though the input is really meaningless to a human. This can lead to adversarial attacks. However, structure-aware modeling would allow us to overcome such issues in token-based models. Not only the structure among tokens, but also the structure within tokens is equally-important which is discarded at tokenization. It is particularly beneficial to maintain the structure within tokens for fine-grained prediction tasks such as segmentation or detection.

In this paper, we propose to preserve and make use of the spatial structure both within and among tokens. To do this we focus on two components: (1) \textit{Structure-aware Tokenization} and (2) \textit{Structure-aware Mixing}\footnote{We commonly refer to both attention-based (MHSA) and MLP-based information sharing as \textit{Mixing} throughout this paper.}, both of which can be adopted in existing token-based architectures with minimal effort. Our Structure-aware Tokenization converts image patches to tokens, but preserves the spatial structure within a patch as channel segments of the token. The Structure-aware Mixing use the preserved structure to look beyond a single channel in token mixing and look beyond a single token in channel mixing, based on 2D convolutions. With these two contributions, we introduce a family of models: \textbf{SWAT}, and compare against common baselines such as DeiT \cite{touvron2021deit}, Swin Transformer \cite{liu2021swin}, MLP-Mixer \cite{tolstikhin2021mixer} and ResMLP \cite{touvron2021resmlp}. Our models show consistent improvement over baseline models on multiple benchmarks including ImageNet-1K \cite{deng2009imagenet} classification and ADE20K \cite{zhou2019ade20k} semantic segmentation. We further visualize fine-grained attention patterns captured by our models. Performance gains on ImageNet-1K classification against complexity (measured by system-agnostic metric, FLOPs) is shown in \fref{fig:acc_vs_compute}.


\section{Related Work}
\label{sec:related}

\paragraph{Token-based models:} Transformer architectures have been dominant in language modeling for quite some time \cite{vaswani2017attention, devlin2018bert}.
However, they are adopted to visual data more recently following the seminal work in ViT \cite{dosovitskiy2020image}. 
Even though similar attention mechanisms were previously seen in computer vision \cite{wang2018non, hu2018squeeze, zhao2020exploring}, they have been used in conjunction with CNNs. The true potential of attention was realized when used with tokenization to model visual data as a sequence of tokens. Since then, a variety of token-based modeling techniques have been introduced.
Methods such as MLP-Mixer \cite{tolstikhin2021mixer}, ResMLP \cite{touvron2021resmlp}, gMLP \cite{liu2021pay} and LIT \cite{pan2021less} use MLPs to perform token mixing. 
DeiT \cite{touvron2021deit} introduce the use of distillation in token-based models and train with an efficient recipe. Similarly, an extensive ablation on training settings is done in \cite{steiner2021train}. 
In \cite{caron2021emerging, chen2021empirical} token-based models are trained with self-supervision, showing interesting properties learned by attention. Similar properties are discussed in \cite{touvron2021going, naseer2021intriguing} as well.
Token learner \cite{ryoo2021tokenlearner} learns a small subset of tokens to reduce compute. More work explore a similar idea of efficient token selection, including Sparse MLP \cite{tang2021sparse}, PS-ViT \cite{yue2021vision}, timeSformer \cite{bertasius2021space} and Focal Transformer \cite{yang2021focal}. 
Swin Transformer \cite{liu2021swin} introduce attention within shifted-windows for better information sharing, and a progressive downsampling of tokens with patch merging. 
Related ideas of having multiple scales of tokens were shown in PiT \cite{heo2021rethinking}, PVT \cite{wang2021pyramid}, MViT \cite{fan2021multiscale}, CrossViT \cite{chen2021crossvit} and ConvMLP \cite{li2021convmlp}.
In another direction, efficient attention mechanisms have been explored, for example, in AFT \cite{zhai2021attention}, Performer \cite{choromanski2020rethinking}, Lambda Networks \cite{bello2021lambdanetworks} and LeViT \cite{graham2021levit}. 

\paragraph{Token adoptation in vision tasks:} At the moment, token-based models are widely applied in almost all domains in vision, including classification \cite{dosovitskiy2020image, touvron2021deit, liu2021swin}, object detection \cite{zhu2020deformable, carion2020end, dai2021up}, segmentation \cite{xie2021segformer, duke2021sstvos}, image generation \cite{liang2021swinir, cao2021video, esser2021taming, jiang2021transgan, deng2021stytr}, video understanding \cite{lee2020parameter, nagrani2021attention, fan2021multiscale, girdhar2021anticipative, bertasius2021space, akbari2021vatt, liu2021video, arnab2021vivit, zhang2021vidtr, liu2021end, chen2021mm, sharir2021image}, dense prediction \cite{yang2021transformer, ranftl2021vision}, point clouds processing \cite{zhao2021point, guo2021pct}, reinforcement learning \cite{chen2021decision, janner2021reinforcement} and tracking \cite{sun2020transtrack}.

\paragraph{Structure with token-based models:} Some prior work in token-based models has explored the use of structure, using hybrid architectures of convolutions. CNN stem is used before tokenization to extract local structure before token mixing in \cite{xiao2021early}. Bottleneck Transformers replace a few convolutional layers in a CNN with MHSA \cite{srinivas2021bottleneck} model pairwise attention. Convolutions have been used for inter-token relationship modeling in Conformer \cite{peng2021conformer} and CvT \cite{wu2021cvt}. Soft inductive biases introduced in ConViT \cite{d2021convit} try to preserve structure similar to convolutions. Transformer-in-Transformer \cite{han2021transformer} and Nested Transformers \cite{zhang2021aggregating} both have a separate token structure within tokens. This is similar to what we do, but such modeling makes the networks heavy. A structure-based grouping method is proposed in T2T-ViT \cite{yuan2021tokens}. CeiT \cite{yuan2021incorporating} is closely-related to our proposed channel mixing with token structure. Here, 2D depthwise convolution is inserted between MLPs in channel mixing. However, CeiT does not preserve the structure within tokens explicitly, nor make use of it.


\vspace{-2mm}


\section{Spatial Structure Within and Among Tokens}
\label{sec:method}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.85\textwidth]{tx_intro.pdf}
	\caption{\textbf{SWAT Overview:} We show the architecture of SWAT (bottom) and a baseline model (top) in this figure. We propose two main contributions: (1) Structure-aware Tokenization and, (2) Structure-aware Mixing, which can be applied to common Transformer or Mixer architectures with minimal effort. Structure-aware tokenization preserves the spatial structure within a token, as channel segments. Simply put, first, we tokenize with a patch size $(p/\alpha\times p/\alpha)$ instead of $(p\times p)$, resulting in $\times\alpha^2$ more intermediate tokens. Next, we restructure $\alpha\times\alpha$ neighboring tokens into one token (concatenating in channel dimension), which gives the same number of tokens (and the channel dimension) into  Mixing operations, ensuring no additional cost in the downstream. However, now, this newly-preserved structure within tokens can be explored for better downstream processing, which was very limited previously. More on how we use the structure in Mixing is shown in \fref{fig:swat_deit} and \fref{fig:swat_mixer}.}
	\label{fig:overview}
\end{figure*}


In SWAT family of models, we explore the benefits of preserving spatial structure not only among tokens, but within tokens as well. To do this with a general framework, we consider both Transformer and MLP-Mixer models as a unified model, which consists of two main components: (1) \textit{Tokenization}, for converting image patches into tokens, and, (2) \textit{Mixing}, for sharing information within and among tokens. Mixing can mean either the use of Multi-Layer Perceptron (MLP), Multi-Headed Self-Attention (MHSA), or both for information sharing. In this setting, we suggest improvements to both Tokenization and Mixing, to preserve and make use of spatial structure. Namely, we introduce \textit{Structure-aware Tokenization} and \textit{Structure-aware Mixing}, which we describe below in detail.

\subsection{Structure-aware Tokenization}

Here, we propose to preserve the spatial structure within tokens, without having any additional burden on downstream processing. The idea is to keep the spatial information inside the channels of a token. In general, image patches are converted into tokens by sliding a large convolutional kernel with a stride (say for instance, a $16\times16$ kernel with a stride of 16), which extracts a set of tokens. In such a method, all the spatial information within a patch is directly fused into the channels of the corresponding token, losing the explicit structure in the process. In our method, we replace this direct fusion, so that we retain structural information within tokens.

More concretely, let us consider an input image of size $H\times W\times 3$, and a baseline tokenizer which converts image patches into tokens by extracting non-overlapping patches of size $p\times p$. This is usually implemented as a convolutional layer with $C$ kernels of size of $(p\times p)$, applied at a stride of $p$. The output here will be an $H/p\times W/p$ 2D structure of tokens, which is reshaped to create a sequence of $HW/p^2$ tokens of embedding dimension $C$. Even though these tokens are processed downstream as a sequence, they can be reshaped back into the original 2D structure of $H/p\times W/p$ whenever necessary. It has been observed that the tokens preserve this structure (among tokens) through skip connections and positional encodings \cite{caron2021emerging, naseer2021intriguing}, 
even after a series of Mixing blocks. However, the structure within a $p\times p$ patch is irreversibly lost; although each token is a linear abstraction of $p\times p$ pixels, remapping the token back to its original $p\times p$ shape for a further spatial analysis in the subsequent layers/blocks is not directly feasible.

In contrast, the proposed tokenizer in SWAT retains the structure within a token. We do this by first having $C/\alpha^2$ convolutional kernels of size $(p/\alpha\times p/\alpha)$ (where $\alpha>1$) and applying it with a stride of $p/\alpha$. The resulting intermediate set of tokens will have a 2D structure of $\alpha H/p\times \alpha W/p$ and a dimension of $C/\alpha^2$. Next, such  $\alpha\times\alpha$ neighboring tokens are reshaped into a single token (concatenating in the channel dimension), creating the same number of tokens $HW/p^2$ as before, with the embedding dimension of $C$. By doing so, we have an $\alpha\times\alpha$ 2D structure within the channels of each token, which can be preserved throughout downstream processing, by the same principles: skip connections and (optional) positional embeddings. This process is further illustrated in \fref{fig:overview}. The SWAT tokenizer will have a fewer parameters, in fact,  $3Cp^2/\alpha^4$, compared to that of the baseline ($3Cp^2$), which can impair the learning capacity. In practice however, our tokenization mentioned above need not be a single layer, but rather can be a bottleneck structure of multiple layers (still having the same downsampling factor of $1/\alpha$ w.r.t. the baseline), which will enable the tokenizer to have an equivalent number of parameters as the baseline, while preserving the structure within tokens.

\subsection{Structure-aware Mixing}

Following the tokens generated by the SWAT tokenizer, which preserve the spatial structure both within and among tokens, we explore how to best make use of it. The idea is simple: when we have such a 2D structure, the elements, i.e., tokens or channels, will have the notion of their neighboring elements in 2D space, which can act as an inductive bias. We can benefit from this locality by using 2D convolutions, mixing information in a local region of elements, in addition to the usual global information sharing in Transformer/Mixer models. We present this idea in two parts: (1) \textit{Token Mixing with Channel Structure} and, (2) \textit{Channel Mixing with Token Structure}.

\subsubsection{Token Mixing with Channel Structure}

\begin{figure}[t]
	\centering
	\vspace{2mm}
	\includegraphics[width=1.\linewidth]{swat_tx.pdf}
	\caption{\textbf{SWAT Transformer Block:} We make use of 2D structure of channels (within tokens) in token mixing, and 2D structure of tokens in channel mixing. In token mixing, we insert a 2D convolution (applied on reshaped input) parallel to each projection MLP before and after attention (MHSA) layer. These explore the structure within tokens. In channel mixing, first, MLPs are replaced with pointwise convolutions (on transposed axes), which makes no change in processing. Next, we insert a 2D depthwise convolution to look into the structure among tokens. Some key tensor reshape operations are shown below each figure.}
	\label{fig:swat_deit}
\end{figure}

Token Mixing happens in different ways in Transformers \cite{dosovitskiy2020image, touvron2021deit} and Mixers \cite{tolstikhin2021mixer, touvron2021resmlp}. In Transformers, each token attends to every other token dynamically (i.e., attention weights are input dependent) in a MHSA layer, sandwiched between two MLP projection layers. Here, by definition, token mixing (i.e., information sharing among tokens) happens \textit{while also looking within each token}\footnote{We also call this as \textit{looking beyond a single channel}. Since information within a token is embedded in channel dimension, looking beyond a single channel means the same as looking within a token.}. To elaborate, before and after attention, each channel can share information with every other channel through MLPs, i.e., information is shared within a token. In contrast, in Mixers, information sharing between tokens is done with static pairwise-relations (i.e., learned weights not dependent on the input), \textit{while not looking within each token}. It means tokens are mixed channel-wise in the subsequent layers instead. In both cases, we want to to look at the structure within the tokens (i.e., spatial structure preserved as channels) in token mixing. To simply put, the difference is that in Transformers we are improving an operation which is already there (making use of structure within tokens), but in Mixers, we introduce a completely-new operation.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{swat_mlp.pdf}
	\vspace{1.5mm}
	\caption{\textbf{SWAT Mixer Block:} In both token mixing and channel mixing, we first replace the MLPs with pointwise convolutions (on transposed axes, eg: an MLP on $B\times N\times C$ equals to a pointwise convolution on $B\times C\times N$ in PyTorch-like channel-first implementations of Conv). This makes no change to how an input is processed. However, now we can easily explore the 2D structure of channels (within tokens) in token mixing and, structure of tokens when channel mixing, by inserting a 2D depthwise convolution. We also show some key tensor reshape operations below each figure.}
	\label{fig:swat_mixer}
\end{figure}

\paragraph{Transformers:} We insert a 2D convolution layer in parallel to projection MLPs before and after MHSA (see \fref{fig:swat_deit} bottom-left) to explore the Channel structure, i.e., structure within tokens. After SWAT tokenizer, channel dimension $C$ has an internal structure of $c\times h\times w$ (as shown in \fref{fig:overview}, with usual notation), which we use to reshape the input as,
\begin{equation*}
	B\times N\times C \rightarrow B\times N\times (chw) \rightarrow (BN)\times c\times h\times w.
\end{equation*}
\noindent Here, $B$ represent batch, $N$, number of tokens and $C$, embedding dimension. When a 2D convolution\footnote{Note that here we consider a channel-first implementation of convolution, as in PyTorch. For instance, 2D convolution input has a shape of $B\times C\times H\times W$, with usual notation.} is applied on this tensor, it can mix channel information as the projection MLP, but also considering the inductive bias of local structure.


\paragraph{Mixers:} In Mixers, we first replace the MLPs in token mixing with pointwise $1\times1$ convolutions (applied on transposed axes). We do not change the underlying operation yet, but just apply a different layer (MLP $\rightarrow$ $1\times1$ Conv). For instance, applying an MLP on a tensor of shape $B\times C\times N$ is the same as applying a pointwise convolution on  a tensor $B\times N\times C$ (again, we consider channel-first implementation of Conv and channel-last implementation of MLP, as in PyTorch). We do this because it is convenient to consider the 2D structure in channels. Next, we insert a 2D depthwise convolution in-between the two pointwise convolutions, applied on a reshaped input as,
\begin{equation*}
	B\times N\times C \rightarrow B\times N\times (chw) \rightarrow (Bc)\times N\times h\times w.
\end{equation*}
\noindent Altogether, this block now performs token mixing, while looking beyond a single channel. See \fref{fig:swat_mixer} bottom-left.

\subsubsection{Channel Mixing with Token Structure}

Channel Mixing operation is the same for both Transformers and Mixers. In a baseline, two MLPs are applied on an input tensor of shape $B\times N\times C$, which mix intra-token information. In SWAT, since we want to do this \textit{while also looking among tokens}\footnote{We also call this as \textit{looking beyond a single token}.}, we replace the two MLPs with the same sandwich layers of a 2D depthwise convolution in-between two pointwise convolutions. This is applied on an input reshaped as,
\begin{equation*}
	B\times N\times C \rightarrow B\times (HW)\times C \rightarrow B\times C\times H\times W.
\end{equation*}
\noindent See \fref{fig:swat_deit} or \fref{fig:swat_mixer} bottom-right. Now, this block performs channel mixing, while looking beyond a single token.


\subsection{Design decisions}

\paragraph{Why parallel 2D Conv in Transformer token mixing?} We tried replacing the projections in Transformer token mixing with 2D convolutions directly, without having a parallel design. However, the number of parameters reduce considerably, impairing the training capacity. Thus, we have a parallel design (with negligible parameter increase by adding the Conv layer), and sum the outputs, propagating the structure with the new branch. Here, outputs of the branches should be scaled by $\times0.5$ to avoid training instability caused by MHSA operation.  

\paragraph{Progressively downsampling architectures:} It is rather straightforward to implement SWAT tokenization with uniform resolution structures such as ViT \cite{dosovitskiy2020image}, DeiT \cite{touvron2021deit} or Mixers \cite{tolstikhin2021mixer}. However, we also experiment with progressively downsampling architectures such as Swin \cite{liu2021swin}. Here, SWAT tokenization is applied at the input without any change in concept. However, we also focus on preserving the structure (both within and among tokens) when downsampling, and implement a new patch merging operation. In the original implementation, the authors perform 
$B \times H\times W\times C \rightarrow B \times H/2\times W/2\times 4C \rightarrow B \times H/2\times W/2\times 2C$ 
with reshaping and MLP mapping operations applied sequentially, which breaks the structure within tokens (no skip connections either to preserve structure). Therefore, instead, we perform $B \times H\times W\times C \rightarrow B \times H\times W\times (chw) \rightarrow B \times c\times (Hh)\times (Ww)$ (reshaping to the full 2D structure), apply a strided 2D convolution with a stride of 2 (again, we consider a channel-first implementation of Conv as in PyTorch), and reshape the structure within tokens back into the channels again, i.e., $B \times 2c\times (Hh/2)\times (Ww/2) \rightarrow B \times H/2\times W/2\times 2(chw)$.

\paragraph{About spatial structure within tokens:} We have to consider different settings for the structure within tokens, based on the embedding dimension and the patch size of the architecture. In models with a patch size of 16, (eg: DeiT Tiny and Small) we have a structure of $8\times8$, with patch size of 32 (eg: DeiT Base/32), a structure of $16\times16$, and with patch size of 4 (eg: Swin Tiny), a structure of $2\times2$. We scale the channel dimension of these intermediate tokens to fit the embedding dimension of the model.



\section{Experiments}
\label{sec:results}

In this section, we evaluate our family of models, SWAT on image classification and semantic segmentation. We use Imagenet-1K \cite{deng2009imagenet} and ADE20K \cite{zhou2019ade20k} as benchmarks to compare against common Transformer/Mixer architectures such as DeiT \cite{touvron2021deit}, Swin \cite{liu2021swin}, MLP-Mixer \cite{tolstikhin2021mixer} and ResMLP \cite{touvron2021resmlp}. In our ablations, we further evaluate the effect of preserving the spatial structure within and among tokens.


\subsection{ImageNet Classification}
\label{subsec:charades}

\paragraph{Dataset:} ImageNet-1K \cite{deng2009imagenet} is a commonly used classification benchmark, with 1.2M training images and 50K validation images, annotated with 1000 categories. For all our models, we report Top-1 (\%) accuracy on single-crop evaluation with complexity metrics such as parameters and FLOPs. %

\paragraph{Training:} We train all our models for 300 epochs on inputs of $224\times224$ using \texttt{timm} \cite{rw2019timm} library. Initial learning rate is set at $7.5\times10^{-4}$ for a batch size of 1024, and trained with AdamW \cite{kingma15adam} and Cosine decay schedule. Learning rate warm-up is performed for 10 epochs. We use a weight decay \cite{loshchilov2017decoupled} of 0.03 and gradient clipping of 1. Our Tiny models are trained with no regularization (stochastic depth \cite{huang2016deep} and dropout \cite{srivastava2014dropout}) and minimal augmentations, whereas Small and Base models use 0.1 regularization, MixUp \cite{zhang2017mixup}/CutMix \cite{yun2019cutmix} with 0.5 probability, RandAugment \cite{cubuk2020randaugment} and repeated augmentations \cite{hoffer2020augment}. We use the EMA \cite{polyak1992acceleration} weights to evaluate our models. Most of these settings are similar to DeiT \cite{touvron2021deit} training recipe. All models are trained with Mixed Precision.


\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{l|c|c|r|r|r}
			\multicolumn{1}{c|}{\multirow{2}{*}{Model}}  & Input & \multirow{2}{*}{Epochs} & Top-1 & Params. & FLOPs \\%& Throughput \\
			& size & & (\%) & (M) & (G) \\%& (im/s) \\
			\shline
			
			DeiT - Ti \cite{touvron2021deit} & 224 & 300 & 72.2 & 5.7 & 1.3 \\%& 2592  \\
			\rowcolor{row}SWAT$_\text{DeiT}$ - Ti & 224 & 300 & \textbf{75.7} & 5.8 & 1.4 \\%& 1361  \\ 
			\hline
			
			DeiT - S \cite{touvron2021deit} & 224 & 300 & 79.8 & 22.1 & 4.6 \\%& 955 \\
			\rowcolor{row}SWAT$_\text{DeiT}$ - S & 224 & 300 & \textbf{80.5} & 22.3 & 4.9 \\%& 628  \\ 
			\hline
			
			DeiT/32 - B \cite{touvron2021deit} & 224 & 300 & 75.5 & 88.2 & 4.3 \\%& 1269 \\
			\rowcolor{row}SWAT$_\text{DeiT}$/32 - B & 224 & 300 & \textbf{76.2} & 86.3 & 4.5 \\%&  935 \\
			\shline
			
			Mixer - Ti \cite{tolstikhin2021mixer} & 224 & 300 & 68.3 & 5.1 & 1.0 \\%& 3985 \\
			\rowcolor{row}SWAT$_\text{Mixer}$ - Ti & 224 & 300 & \textbf{72.3} & 5.1 & 1.0 \\%& 2569 \\ 
			\hline
			
			Mixer - S \cite{tolstikhin2021mixer} & 224 & $^{\dagger}$5 & 73.8 & 18.5  & 3.8 \\%& 1270 \\
			\rowcolor{row}SWAT$_\text{Mixer}$ - S & 224 & 300 & \textbf{77.9} & 18.6 & 3.8 \\%& 962 \\ 
			
			
			
	\end{tabular}}
	\vspace{2mm}
	\caption{\textbf{SWAT improves both Transformer and Mixer models.} We compare SWAT with DeiT \cite{touvron2021deit} and MLP-Mixer \cite{tolstikhin2021mixer} on ImageNet-1K. We report the performance in Tiny, Small and Base (w/ patch size of 32) configurations. SWAT models consistently outperform their counterparts in all these settings with minimal change in parameters or compute requirement. %
		Improved performance is in \textbf{bold}. $^\dagger$Pretrained on JFT300M. %
	}
	\vspace{-3mm}
	\label{tab:deit_mixer}
\end{table}


\paragraph{SWAT improves both Transformer and Mixer models:} In \tref{tab:deit_mixer}, we present the performance of SWAT with the two main types of token-based models: those using attention (MHSA) such as DeiT \cite{touvron2021deit}, and those using MLPs such as Mixer \cite{tolstikhin2021mixer}. In both model families, SWAT consistently outperforms the baselines, verifying that our Structure-aware Tokenization and Structure-aware Mixing can be applied in both cases.
We also consider multiple scales of models, specifically, Tiny, Small and Base (w/ patch size of 32) configurations, with varying range of parameters and compute. These are standard models reported in previous work. We implement our tokenizer and replace Transformer/Mixing blocks with ours in each configuration (eg: DeiT-Ti $\rightarrow$ SWAT$_\text{DeiT}$-Ti). In all configurations, SWAT models show consistent improvements. In SWAT$_\text{DeiT}$, Tiny version achieves the highest gain of $+3.5\%$, with $+0.7\%$ in Small, and $+0.57\%$ in Base/32. In SWAT$_\text{Mixer}$, both Tiny ($+4.0\%$) and Small ($+4.1\%$) versions show a considerable improvement over the baselines. SWAT models will have minimal (or no) increment in parameters or compute requirement. 


\paragraph{SWAT can be generalized:} In \tref{tab:midsize}, we implement SWAT with multiple common token-based models including DeiT \cite{touvron2021deit}, Swin \cite{liu2021swin}, Mixer \cite{tolstikhin2021mixer} and ResMLP \cite{touvron2021resmlp}, and report the performance in mid-sized (15-30M parameters) standard configurations. We use the same hyperparameter settings and training recipe as in DeiT \cite{touvron2021deit} for all our models (for consistency), and rerun some of the baseline models (with original code) for which the settings are different, making a fair comparison. We observe consistent gains in SWAT family of models, with $+0.7\%$ in SWAT$_\text{DeiT}$, $+1.2\%$ in SWAT$_\text{Swin}$, $+2.2\%$ in SWAT$_\text{Mixer}$ and $+2.8\%$ in SWAT$_\text{ResMLP}$. We see some differences in reported vs. our baselines, specifically, $-1.7\%$ in Swin, $+1.9\%$ in Mixer, $-5.4\%$ in ResMLP due to changes in hyperparameters, pretraining data and/or training schedules. In contrast however, all our reported settings are directly comparable. Moreover, SWAT have minimal change in parameters and compute in all evaluated baseline models.

We present a detailed analysis of model throughput (im/s) at inference, for SWAT models and their baselines in Appedix \ref{sec:appendix}. We consider FLOPs and parameters as our metrics of model complexity, as they are system-agnostic and can be reproduced.

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{l|c|c|r|r|r}
			\multicolumn{1}{c|}{\multirow{2}{*}{Model}}  & Input & \multirow{2}{*}{Epochs} & Top-1 & Params. & FLOPs \\%& Throughput \\
			& size & & (\%) & (M) & (G) \\%& (im/s) \\
			\shline
			DeiT \cite{touvron2021deit} & 224 & 300 & 79.8 & 22.1 & 4.6 \\%& 2592 \\
			\rowcolor{row}SWAT$_\text{DeiT}$& 224 & 300 & \textbf{80.5} & 22.3 & 4.9 \\%& 1361  \\ 
			\hline
			
			Swin \cite{liu2021swin} & 224 & 300 & \underline{81.3} & 28.3 & 4.5 \\%& $^*$755 \\
			Swin (our set.) & 224 & 300 & 79.6 & 28.3 & 4.5 \\%& 769 \\
			\rowcolor{row}SWAT$_\text{Swin}$ & 224 & 300 & \textbf{80.8} & 27.1 & 4.7 \\%& 432 \\
			\hline
			Mixer \cite{tolstikhin2021mixer} & 224 & 5$^{\dagger}$ & \underline{73.8} & 18.5 & 3.8 \\%& $^*$3994 \\
			Mixer (our set.) & 224 & 300 & 75.7  & 18.5 & 3.8 \\%& 3985 \\
			\rowcolor{row}SWAT$_\text{Mixer}$ & 224 & 300 & \textbf{77.9} & 18.6 & 3.8 \\%& 2569  \\ 
			\hline
			
			ResMLP \cite{touvron2021resmlp} & 224 & 400 & \underline{76.6} & 15.4 & 3.0 \\%& $^*$1415 \\
			ResMLP (our set.) & 224 & 300 & 71.2 & 15.4 & 3.0 \\%& 1557 \\
			\rowcolor{row}SWAT$_\text{ResMLP}$ & 224 & 300 & \textbf{74.0} & 15.6 & 3.1 \\%& 1107 \\
			
			
	\end{tabular}}
	\vspace{-1mm}
	\caption{\textbf{SWAT can be generalized.} We report experiments on ImageNet-1K with mid-sized models (15-30M params.) of DeiT \cite{touvron2021deit}, Swin \cite{liu2021swin}, MLP-Mixer \cite{tolstikhin2021mixer} and ResMLP \cite{touvron2021resmlp}.For fair comparison, we also implement baseline models in our setting. Performance improvement is in \textbf{bold}, and performance in different training settings (hyperparameters, data and/or training schedule) is \underline{underlined}.
		SWAT outperforms all the baseline models consistently with minimal change in parameters or compute requirement. All numbers reported in our settings are directly comparable. %
		$^\dagger$Pretrained on JFT300M. %
	}
	\vspace{-3mm}
	\label{tab:midsize}
\end{table}



\subsection{Ablations on ImageNet}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.\linewidth]{swat_vis.pdf}
	\caption{\textbf{Visualization of token attention} in DeiT-Ti \cite{touvron2021deit} and SWAT$_\text{DeiT}$-Ti. We use the code from DINO \cite{caron2021emerging} for visualization. However since we do not use class tokens as in DINO, we show the attention averaged across tokens. SWAT, as it preserves structure even within tokens, shows more contrastive and fine-grained attention maps compared to DeiT (even though we consider same number of tokens, i.e., resolution, in both). Note the better-visible boundaries and segments in SWAT, compared to smoothed-out variations in DeiT. Some cases where SWAT fails to capture fine details are also shown (to the right). Best viewed in color and zoomed-in.
	}
	\label{fig:attn_vis}
\end{figure*}

We present ablations on Tiny versions of SWAT$_\text{DeiT}$ and SWAT$_\text{Mixer}$ in \tref{tab:ablation_deit}. Specifically, we focus on Structure-aware Tokenization, looking beyond a single channel in token mixing (either considering the structure within tokens or not), and looking beyond a single token in channel mixing (again, with the structure among tokens or not). When looking beyond a single channel in token mixing, we use convolutions (2D depthwise) if we want to consider the structure within tokens, or use an MLP if not. Similarly, when looking beyond a single token, if we want to consider the structure among tokens, we use convolution, or otherwise, an MLP. 

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{l|l|l|l|r|r|r}
			\multicolumn{1}{c|}{\multirow{2}{*}{Model}}  & Struct. & \multicolumn{1}{c|}{Beyond} & Beyond & Top-1 & Params. &  FLOPs \\%& Throughput\\
			& tokens & a channel & \multicolumn{1}{c|}{a token} & (\%) & (M) & (G) \\%& (im/s) \\
			\shline
			\multirow{5}{*}{DeiT \cite{touvron2021deit}} & $\;$\xmark & $\;$\cmark & $\;$\xmark & 73.3 & 5.72 & 1.25 \\%& 2592 \\
			
			& $\;$\xmark & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 74.6  & 5.96 & 1.30 \\%&  1953 \\
			
			& $\;$\cmark & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 75.4  & 5.82  & 1.35 \\%& 1746 \\
			
			& $\;$\cmark$_\text{pos.}$ & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 75.1  & 5.82 & 1.35 \\%&  1685 \\
			
			& $\;$\cmark & $\;$\cmark$_\text{struct.}$ & $\;$\cmark$_\text{struct.}$ & 75.3  & 5.82  & 1.40 \\%& 1399 \\
			
			\rowcolor{row}SWAT$_\text{DeiT}$ & $\;$\cmark$_\text{pos.}$ & $\;$\cmark$_\text{struct.}$ & $\;$\cmark$_\text{struct.}$ & \textbf{75.7}  & 5.83 & 1.40 \\%& 1361 \\
			
			\hline
			\multirow{7}{*}{Mixer \cite{tolstikhin2021mixer}} & $\;$\xmark & $\;$\xmark & $\;$\xmark &  68.3 & 5.07  & 0.97 \\%& 3985\\
			
			& $\;$\cmark & $\;$\xmark & $\;$\xmark & 67.9  & 4.88  & 0.94 \\%& 3703 \\
			
			& $\;$\xmark & $\;$\cmark & $\;$\xmark & 68.9  & 5.08  & 0.97 \\%& 3702 \\
			
			& $\;$\xmark & $\;$\xmark & $\;$\cmark$_\text{struct.}$ & 70.8 & 5.28  & 1.01 \\%& 2879\\
			
			& $\;$\xmark & $\;$\cmark & $\;$\cmark & 70.9 & 5.91  & 1.35 \\%& 2634 \\
			
			& $\;$\cmark & $\;$\cmark & $\;$\cmark & 70.6 & 5.71  & 1.32 \\%& 2511 \\
			
			& $\;$\xmark & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 71.2  & 5.81 & 1.08 \\%&  2733 \\
			
			\rowcolor{row}SWAT$_\text{Mixer}$ & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & $\;$\cmark$_\text{struct.}$ & \textbf{72.3} & 5.10 & 0.99 \\%& 2569 \\
	\end{tabular}}
	\caption{\textbf{SWAT Ablations with DeiT-Ti \cite{touvron2021deit} and Mixer-Ti \cite{tolstikhin2021mixer}} on ImageNet-1K. We report the gains from (1) Structure-aware Tokenization (w/ or w/o positional encodings within a token when applicable), (2) looking beyond a single channel in token mixing (w/ or w/o structure within tokens), and (3) looking beyond a single token in channel mixing (w/ or w/o the structure among tokens). Structure-aware inputs and Structure-aware Mixing gives consistent improvements, as shown in \textbf{bold}. %
	}
	\vspace{-3mm}
	\label{tab:ablation_deit}
\end{table}

\paragraph{Structure-aware Tokenization:} We compare different settings with SWAT tokenizer. 
In Mixer \cite{tolstikhin2021mixer}, we see a performance drop when we introduce the Structure-aware Tokenization, but do not use the structure (within tokens) explicitly. The drop is $-0.4\%$ if not looking beyond a single channel at all, or $-0.3\%$ if looking beyond but without considering structure (as in MLPs). In DeiT \cite{touvron2021deit} however, we see a $+0.8\%$ boost with the proposed tokenizer, even if we do not use this structure in processing. In both cases, when we specifically use the structure within tokens given by the tokenization, we see consistent gains ($+1.1\%$ in DeiT and $+1.5\%$ in Mixer). We also consider absolute positional encodings within a token (i.e, channel positional encodings) in DeiT (in contrast to Mixers). Interestingly, this only helps when we use the structure in processing with $+0.4\%$, but $-0.3\%$ otherwise.

\paragraph{Channel Structure (within tokens):} In token mixing, we look beyond a single channel, and when doing so, we can either consider the channel structure (using 2D depthwise convolutions) or not (using MLPs). In Transformers (DeiT \cite{touvron2021deit}), this happens by default in projection MLPs before and after MHSA. When we consider the structure (/w channel positional encodings), we see a $+0.6\%$ improvement. In Mixers however, we newly-introduce the concept of looking beyond a single channel. Therefore, it can have three configurations: not looking beyond a single channel, looking beyond but without considering structure (using MLPs) or considering structure (using 2D depthwise convolutions). The results show that looking beyond a single channel helps ($+0.4\%$), and considering the structure gives a further boost of $+1.1\%$.

\paragraph{Token Structure (among tokens):} In channel mixing, we look beyond a single token. We can either consider the token structure (using 2D depthwise convolution) or not (having an MLP instead). In Mixer \cite{tolstikhin2021mixer}, looking beyond a single token helps with a $+2.0\%$ boost, and a further $+0.3\%$ improvement when considering the token structure. In DeiT \cite{touvron2021deit}, looking beyond a single token with structure gives a $+1.3\%$ improvement.



\begin{figure*}[t]
	\centering
	\includegraphics[width=1.\linewidth]{swat_seg.pdf}
	\caption{\textbf{Segmentation masks} generated with UperNet \cite{xiao2018unified} with Swin-Ti \cite{liu2021swin} (our setting) and SWAT$_\text{Swin}$-Ti as backbones. SWAT shows better segmentation results in comparison. Note the fine structures better captured by SWAT, thanks to preserved spatial information within tokens. Best viewed in color and zoomed-in. }
	\vspace{-4mm}
	\label{fig:seg}
\end{figure*}


\paragraph{Attention visualization:} In \fref{fig:attn_vis}, we visualize token attentions in Tiny configurations of DeiT \cite{touvron2021deit} and SWAT$_\text{DeiT}$. We use the code from DINO \cite{caron2021emerging} paper as a base. However, in our models, since we do not use a class token, we can not visualize the attention on it as in \cite{caron2021emerging}. Instead, we show the attention maps of the final layer of each model averaged across tokens. We consider larger image size ($1024\times1024$) compared to training ($224\times224$) to get higher resolution visualizations, use the same patch size of 16, and interpolate positional encodings accordingly. We can see clear differences between the attention in DeiT \cite{touvron2021deit} and SWAT$_\text{DeiT}$. In SWAT, we have more contrastive attention which resembles fine-grained structures (eg: boundaries in object segments), since we preserve such structure within tokens. In contrast, DeiT attention is smoothed-out and subtle. It is worth noting that we use the same resolution (i.e., same number of tokens) in both cases.

\vspace{-4mm}
\subsection{Semantic Segmentation}
\label{subsec:multithumos}
\vspace{-1mm}

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{l|c|r|r|r|r}
			\multicolumn{1}{c|}{\multirow{2}{*}{Method}}  & \multicolumn{1}{c|}{\multirow{2}{*}{Backbone}} & \multirow{2}{*}{mIoU} & Params. & FLOPs & \multirow{2}{*}{FPS} \\
			& & & (M) & (G) & \\
			\shline
			DANet \cite{fu2019dan} & Resnet-101 & 45.2 & 69 & 1119 & 15.2 \\
			DLab.v3+ \cite{chen2018encoder} & Resnet-101 & 44.1 & 63 & 1021 & 16.0 \\
			ACNet \cite{fu2019adaptive} & Resnet-101 & 45.9 & - & - & - \\
			DNL \cite{yin2020disentangled} & Resnet-101 & 46.0 & 69 & 1249 & 14.8 \\
			OCRNet \cite{yuan2020object} & Resnet-101 & 45.3 & 56 & 923 & 19.3 \\
			UperNet \cite{xiao2018unified} & Resnet-101 & 44.9 & 86 & 1029 & 20.1 \\
			\hline
			UperNet & DeiT-S \cite{touvron2021deit} & 44.0 & 52 & 1099 & 16.2 \\
			UperNet & Swin-Ti \cite{liu2021swin} & 46.1 & 60 & 945 & 18.5 \\ \hline
			UperNet & Swin-Ti (our set.) & 40.2 & 60 & 945 & 19.2 \\
			\rowcolor{row} UperNet & SWAT$_\text{Swin}$-Ti & 42.8 & 59 & 950 & 15.7 \\
			
	\end{tabular}}
	\caption{\textbf{SWAT for semantic segmentation} on ADE20K \cite{zhou2019ade20k} dataset. We report results in the same setting as Swin \cite{liu2021swin} using \texttt{mmsegmentation} \cite{mmseg2020} framework. For fair comparison, we implement the baseline model in our setting as well (with same hyperparameters and training schedule). FPS is measured on a single V100 GPU. SWAT outperforms Swin in comparable settings, but slightly slower due to extra convolutions.}
	\vspace{-4mm}
	\label{tab:seg}
\end{table}

\paragraph{Dataset:} ADE20K \cite{zhou2019ade20k} benchmark contains annotations for semantic segmentation across 150 categories. It comes with 25K annotated images in total, with 20K training, 2K validation and 3K testing. We report mIoU for our models in multi-scale testing (i.e., [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]$\times$ the training resolution) similar to previous work \cite{liu2021swin}, along with complexity metrics such as parameters, FLOPs (for input size of $512\times2048$ similar to \cite{liu2021swin}) and frame-rate.

\paragraph{Training:} We follow a similar training recipe to Swin \cite{liu2021swin}. Our backbones are pretrained on ImageNet-1K \cite{deng2009imagenet} for 300 epochs at $224\times224$, before re-training with a decoder for segmentation at $512\times512$. We use UperNet \cite{xiao2018unified} as our decoder within \texttt{mmsegmentation} \cite{mmseg2020}. Our models are trained with AdamW \cite{kingma15adam} with an initial learning rate of $6\times10^{-5}$, warmup of 1500 iterations and a training schedule of 160k iterations with a batch size of 2 images per GPU (on 8 V100s). We use random flipping, scaling and photometric distortions as augmentations, stochastic depth \cite{huang2016deep} of 0.2 and weight decay \cite{loshchilov2017decoupled} of 0.01 as regularizations.

\paragraph{Results:} In \tref{tab:seg}, we show the performance of SWAT$_\text{Swin}$ when used with UperNet \cite{xiao2018unified} for semantic segmentation on ADE20K, and compare with similar-sized baselines. Due to the differences that we observed in the reported and reproduced results of Swin backbone (due to hyperparameter changes) in \tref{tab:midsize}, we use the versions trained in our setting for fair comparison in downstream segmentation. SWAT gives better segmentation performance ($+2.6$ mIoU) over Swin \cite{liu2021swin}, when trained under the same settings. However, FPS of the SWAT$_\text{Swin}$ based model is slightly lower, due to extra convolutions introduced in SWIT. %
Segmentation masks generated by SWAT$_\text{Swin}$ and Swin \cite{liu2021swin} (our setting) backbones (in \fref{fig:seg}) qualitatively show this improvement. \vspace{-1mm}



\section{Conclusion}
\label{sec:conclusion}

\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\renewcommand{\thefigure}{A.\arabic{figure}}
\begin{table*}[ht]\centering
	\vspace{-4mm}
	\captionsetup[subfloat]{captionskip=2pt}
	\captionsetup[subffloat]{justification=centering}
	
	\subfloat[\textbf{With different SWAT models.} We consider DeiT \cite{touvron2021deit}, Swin \cite{liu2021swin}, Mixer \cite{tolstikhin2021mixer} and ResMLP \cite{touvron2021resmlp} as baselines. Parameters and FLOPs, which are system-agnostic, show consistent changes. However, throughput (on a single V100) show interesting variations: (1) smaller changes in larger models (i.e., DeiT/Swin), and (2) changes in Transformer models are larger compared to Mixer models.
	\label{tab:scale}]{
		\tablestyle{2pt}{1.05}
		\resizebox{0.26\linewidth}{!}{
			\begin{tabular}{l|r|r|r|r}
				\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & Top-1 & Params. & FLOPs & Throughput \\
				& (\%) & (M) & (G) & (im/s) \\
				\shline
				
				DeiT - Ti \cite{touvron2021deit} & 72.2 & 5.7 & 1.3 & 2391  \\
				\rowcolor{row}SWAT$_\text{DeiT}$ - Ti & \textbf{75.7} & 5.8 & 1.4 & 1330 \\ %
				\hline
				
				DeiT - S \cite{touvron2021deit} & 79.8 & 22.1 & 4.6 & 924 \\
				\rowcolor{row}SWAT$_\text{DeiT}$ - S & \textbf{80.5} & 22.3 & 4.9 & 645 \\ %
				\hline
				
				DeiT/32 - B \cite{touvron2021deit} & 75.5 & 88.2 & 4.3 & 1275 \\
				\rowcolor{row}SWAT$_\text{DeiT}$/32 - B & \textbf{76.2} & 86.3 & 4.5 &  923 \\ %
				\hline
				
				Mixer - Ti \cite{tolstikhin2021mixer} & 68.3 & 5.1 & 1.0 & 3701 \\
				\rowcolor{row}SWAT$_\text{Mixer}$ - Ti & \textbf{72.3} & 5.1 & 1.0 & 2759 \\  %
				\hline
				
				Mixer - S \cite{tolstikhin2021mixer} & 75.7  & 18.5 & 3.8 & 1235  \\
				\rowcolor{row}SWAT$_\text{Mixer}$ - S & \textbf{77.9} & 18.6 & 3.8 & 1022 \\ %
				
				\hline
				
				Swin - Ti \cite{liu2021swin} & 79.6 & 28.3 & 4.5 & 703  \\
				\rowcolor{row}SWAT$_\text{Swin}$ - Ti & \textbf{80.8} & 27.1 & 4.7 & 402 \\ %
				
				\hline
				
				ResMLP \cite{touvron2021resmlp} & 71.2 & 15.4 & 3.0 & 1562  \\
				\rowcolor{row}SWAT$_\text{ResMLP}$ & \textbf{74.0} & 15.6 & 3.1 & 1136 \\ %
				
				
				
		\end{tabular}}
	}\hspace{2mm}
	\subfloat[\textbf{With SWAT ablations.} We consider different settings leading up to SWAT models, and their complexity measured in parameters, FLOPs or throughput (on a single V100). Each SWAT model shows small changes in system-agnostic metrics compared to the corresponding baseline. Structure-aware tokenization shows inconsistent throughput changes ($-10\%$ to $+3\%$). Change due to positional encodings is small ($-2\%$). Structure-aware mixing with DeiT \cite{touvron2021deit} costs throughput approx. $-10\%$ in token mixing and $-20\%$ in channel mixing. In Mixer \cite{tolstikhin2021mixer}, the corresponding costs are approx. $-5\%$ and $-20\%$, respectively.
	\label{tab:ablation}]{
		\tablestyle{2pt}{1.05}
		\resizebox{0.34\linewidth}{!}{
			\begin{tabular}{l|l|l|l|r|r|r|r}
				\multicolumn{1}{c|}{\multirow{2}{*}{Model}}  & Struct. & \multicolumn{1}{c|}{Beyond} & Beyond & Top-1 & Params. &  FLOPs & Throughput\\
				& tokens & a channel & \multicolumn{1}{c|}{a token} & (\%) & (M) & (G) & (im/s) \\
				\shline
				\multirow{5}{*}{DeiT \cite{touvron2021deit}} & $\;$\xmark & $\;$\cmark & $\;$\xmark & 73.3 & 5.72 & 1.25 & 2391 \\
				
				& $\;$\xmark & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 74.6  & 5.96 & 1.30 & 1911 \\ %
				
				& $\;$\cmark & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 75.4  & 5.82  & 1.35 & 1686 \\ %
				
				& $\;$\cmark$_\text{pos.}$ & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 75.1  & 5.82 & 1.35 & 1652 \\ %
				
				& $\;$\cmark & $\;$\cmark$_\text{struct.}$ & $\;$\cmark$_\text{struct.}$ & 75.3  & 5.82  & 1.40 & 1372 \\ %
				
				\rowcolor{row}SWAT$_\text{DeiT}$ & $\;$\cmark$_\text{pos.}$ & $\;$\cmark$_\text{struct.}$ & $\;$\cmark$_\text{struct.}$ & \textbf{75.7}  & 5.83 & 1.40 & 1330 \\ %
				
				\hline
				\multirow{7}{*}{Mixer \cite{tolstikhin2021mixer}} & $\;$\xmark & $\;$\xmark & $\;$\xmark &  68.3 & 5.07  & 0.97 & 3701\\
				
				& $\;$\cmark & $\;$\xmark & $\;$\xmark & 67.9  & 4.88  & 0.94 & 3452 \\ %
				
				& $\;$\xmark & $\;$\cmark & $\;$\xmark & 68.9  & 5.08  & 0.97 & 3485 \\ %
				
				& $\;$\xmark & $\;$\xmark & $\;$\cmark$_\text{struct.}$ & 70.8 & 5.28  & 1.01 & 3053 \\ %
				
				& $\;$\xmark & $\;$\cmark & $\;$\cmark & 70.9 & 5.91  & 1.35 & 2367 \\ %
				
				& $\;$\cmark & $\;$\cmark & $\;$\cmark & 70.6 & 5.71  & 1.32 & 2487 \\ %
				
				& $\;$\xmark & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & 71.2  & 5.81 & 1.08 & 2906 \\ %
				
				\rowcolor{row}SWAT$_\text{Mixer}$ & $\;$\cmark & $\;$\cmark$_\text{struct.}$ & $\;$\cmark$_\text{struct.}$ & \textbf{72.3} & 5.10 & 0.99 & 2759 \\ %
				
				\multicolumn{8}{c}{} \\ %
		\end{tabular}}
	}\hspace{2mm}
	\subfloat[\textbf{With different SWAT$_\text{Mixer}$ configurations.} We consider different Mixer \cite{tolstikhin2021mixer} block configs, with the complexity measured in throughput (on a single V100/A100). Here, (P) represents pointwise and (D), depthwise convolutions. Throughputs depend on the hardware, and different cuda implementations. On A100s, MLP is significantly faster than Conv (P). Even though MLP, 1D Conv (P) and 2D Conv (P) perform the same operation, they have different throughputs. An additional MLP incurs higher cost compared to 2D Conv (D) on V100, which is not the case on A100.
	\label{tab:block}]{
		\tablestyle{2pt}{1.05}
		\resizebox{0.30\linewidth}{!}{
			\begin{tabular}{l|c|r|r}
				\multicolumn{1}{c|}{\multirow{2}{*}{Mixer Block configuration}} & Struct. & Throughput & Throughput \\
				& tokens & \textbf{V100} (im/s) & \textbf{A100} (im/s)\\
				\shline
				$2\times$ MLP & \xmark & 3701 & 9127 \\
				$2\times$ 1D Conv (P) & \xmark & 3441 & 6921 \\
				$2\times$ 2D Conv (P) & \xmark & 3716 & 7421 \\
				\hline
				$2\times$ 1D Conv (P) + $1\times$ MLP & \xmark & 2496 & 5480 \\
				$2\times$ 2D Conv (P) + $1\times$ MLP & \xmark & 2440 & 5290 \\
				$2\times$ 2D Conv (P) + $1\times$ 2D Conv (D) & \xmark & 2927 & 5538 \\
				\rowcolor{row}$2\times$ 2D Conv (P) + $1\times$ 2D Conv (D) & \cmark & 2759 & 5112 \\
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				\multicolumn{4}{c}{} \\ %
				
		\end{tabular}}
	}%
	
	
	\caption{\textbf{Throughput analysis of SWAT models with different settings.} Performance on ImageNet-1K and complexity measured in parameters, FLOPs or throughput. System-agnostic complexity measures such as parameters or FLOPs give consistent changes (and can be reproduced). However, throughput values vary based on the hardware and cuda implementations (even for the same operation, eg: MLP vs pointwise Conv). Therefore, we consider parameters and FLOPs as more-reliable complexity measures.}
	\label{tab:ablations}
	\vspace{-2mm}
\end{table*}

In this work, we present the merits of preserving spatial structure within and among Tokens, in common Transformer/Mixer architectures. Our two key contributions are: (1) Structure-aware Tokenization and (2) Structure-aware Mixing, which can be adopted with minimal effort to many different models. The resulting family of models, SWAT, outperforms previous work on multiple benchmarks with minimal change in compute requirement. We hope SWAT will open-up new ways of using spatial structure as an inductive bias in token-based models. %

\paragraph{Acknowledgements:} This work was supported by the National Science Foundation (IIS-2104404 and CNS-2104416). We thank the Robotics Lab at SBU for helpful discussions.


\newcount\cvprrulercount
\appendix


\section{Appendix: Throughput of SWAT models}
\label{sec:appendix}

In this paper, we consider FLOPs as the main complexity metric, since it is system-agnostic. Here, we report throughput numbers using PyTorch 1.7.1 on a single V100 GPU. These may change depending on the actual hardware and underlying cuda optimizations.

\paragraph{With model scale:} When we consider larger SWAT models, the change of throughput due to Structure-aware Tokenization and Mixing becomes small (see \tref{tab:scale}). In DeiT \cite{touvron2021deit}, we see a $-44\%$ change in Tiny, $-30\%$ in Small and $-28\%$ in Base/32 models. In Mixer \cite{tolstikhin2021mixer}, we see a $-25\%$ change in Tiny and $-17\%$ in Small models. However, none of the SWAT models have significant change in system-agnostic measures such as parameters or FLOPs.

\paragraph{With Transformers vs. Mixers:} The change in throughput with Transformer models such as DeiT-Ti ($-44\%$) \cite{touvron2021deit} or Swin-Ti \cite{liu2021swin} ($-43\%$) is higher compared to Mixer models such as Mixer-Ti \cite{tolstikhin2021mixer} ($-25\%$) or ResMLP-S12 \cite{touvron2021resmlp} ($-27\%$) (see \tref{tab:scale} and \tref{tab:ablation}). The difference between models is in Token Mixing. In Transformers, SWAT includes additional $2\times$ parallel 2D Conv blocks, whereas in Mixers, $1\times$ cascaded 2D depthwise Conv block. 2D Conv blocks in Transformers see small feature depths (eg: depth$=3$ in Tiny) and run in parallel to MLPs, which makes it hard to justify the extra throughput cost compared to cascaded 2D depthwise Conv in Mixers. This shows inconsistency in cuda optimizations for different operators.

\paragraph{With MLP vs. Conv:} Same operation as MLP can be performed using 1D or 2D pointwise convolutions. However, in terms of throughput, pointwise convolutions show differences compared to MLPs both in V100s ($-7\%$ in 1D and $0\%$ in 2D) and A100s ($-24\%$ in 1D and $-19\%$ in 2D). See \tref{tab:block}. This shows differences in cuda optimizations, even for essentially-the-same operation.

\paragraph{With V100 vs. A100:} We see striking differences in throughput variations on V100 compared to A100 (see \tref{tab:block}). On A100s, implementation of MLPs seems to be much faster compared to pointwise convolutions. This difference in throughput directly propagates to SWAT models.

Looking at the above inconsistencies in measuring complexity as throughput, we consider system-agnostic measures such as parameters or FLOPs to be more reliable and reproducible metrics to be evaluated in this paper.

{\small
\balance
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
